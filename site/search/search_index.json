{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This is the new documentation site for breadNET Other sites maintained Site name Link Main site breadnet.co.uk CV bradley.breadnet.co.uk Kubernetes Manifest documentation kubernetes.breadnet.co.uk How this site works Like mentioned above - This is the new documentation site, replacing bookstack.breadnet.co.uk (Which is now offline) I have decided to move towards an SCM based site. This means I write markdown files and then using mkdocs with the material theme the site gets built. How the site gets built I have a CD pipeline that builds the site and copies it to the webserver You can view the Source code for the pipeline","title":"Home"},{"location":"#other-sites-maintained","text":"Site name Link Main site breadnet.co.uk CV bradley.breadnet.co.uk Kubernetes Manifest documentation kubernetes.breadnet.co.uk","title":"Other sites maintained"},{"location":"#how-this-site-works","text":"Like mentioned above - This is the new documentation site, replacing bookstack.breadnet.co.uk (Which is now offline) I have decided to move towards an SCM based site. This means I write markdown files and then using mkdocs with the material theme the site gets built.","title":"How this site works"},{"location":"#how-the-site-gets-built","text":"I have a CD pipeline that builds the site and copies it to the webserver You can view the Source code for the pipeline","title":"How the site gets built"},{"location":"bookstack/","text":"Hello You have most likely followed a link from Bookstack, my old documentation site. This is the new site for all documentation. I have done my best to preserve the directory structure from Bookstack, but some pages have new homes. Redirects All the content pages are set up to redirect to their new homes. You can have a look at the redirects Here Chapters Bookstack had the below structure for pages . \u2514\u2500\u2500 bookstack \u2514\u2500\u2500 book \u2514\u2500\u2500 chapter \u2514\u2500\u2500 page An example of this would be for the page VPN Network routing Mikrotik It's URL is https://bookstack.breadnet.co.uk/books/kb-articles/page/vpn-network-routing-mikrotik And if we were to visualize the directory it would be: \u2514\u2500\u2500 bookstack \u2514\u2500\u2500 KB Artickes \u2514\u2500\u2500 Networking \u2514\u2500\u2500 VPN Network routing Mikrotik I am not redirecting from the section \"KB Articles\" - Only the pages as no one other than my self will be linking to chapters. Missing pages you and your team relied on If you or your team relied on a page, please drop me an email and I will crack it out the archive store for you!","title":"Bookstack"},{"location":"bookstack/#hello","text":"You have most likely followed a link from Bookstack, my old documentation site. This is the new site for all documentation. I have done my best to preserve the directory structure from Bookstack, but some pages have new homes.","title":"Hello"},{"location":"bookstack/#redirects","text":"All the content pages are set up to redirect to their new homes. You can have a look at the redirects Here","title":"Redirects"},{"location":"bookstack/#chapters","text":"Bookstack had the below structure for pages . \u2514\u2500\u2500 bookstack \u2514\u2500\u2500 book \u2514\u2500\u2500 chapter \u2514\u2500\u2500 page An example of this would be for the page VPN Network routing Mikrotik It's URL is https://bookstack.breadnet.co.uk/books/kb-articles/page/vpn-network-routing-mikrotik And if we were to visualize the directory it would be: \u2514\u2500\u2500 bookstack \u2514\u2500\u2500 KB Artickes \u2514\u2500\u2500 Networking \u2514\u2500\u2500 VPN Network routing Mikrotik I am not redirecting from the section \"KB Articles\" - Only the pages as no one other than my self will be linking to chapters.","title":"Chapters"},{"location":"bookstack/#missing-pages-you-and-your-team-relied-on","text":"If you or your team relied on a page, please drop me an email and I will crack it out the archive store for you!","title":"Missing pages you and your team relied on"},{"location":"not-found/","text":"Page not migrated Hello! I assume you're seeing this page because you have come from Bookstack via a link that no longer exists! Certain pages were not migrated as they were deemed low traffic. If that page is something that you relied on in for your documentation, please reach out to me and I will migrate it. Go home","title":"Page not migrated"},{"location":"not-found/#page-not-migrated","text":"Hello! I assume you're seeing this page because you have come from Bookstack via a link that no longer exists! Certain pages were not migrated as they were deemed low traffic. If that page is something that you relied on in for your documentation, please reach out to me and I will migrate it. Go home","title":"Page not migrated"},{"location":"automation/airflow/airflow-basics/","text":"Airflow basics Airflow exists for the sole purpose of removing the need to write a bash or python script, setup a server with a cron job and then run your ETL pipeline. With Airflow we're doing all this under one roof with monitoring and visualization built in. Airflow uses something called a DAG (Directed acyclic graph) Airflow is primerilly written with Python scrips that compose of the below imports from airflow import DAG from datetime import datetime We need to use datetime as airflow runs certain operations on a time schedule, an example is below with DAG(\"my_dag\", start_date=datetime(2022, 1, 1), schedule_interval=\"@daily\") as dag: What happens here is we: Define the dag with a name my_dag Set a start time that the Dag is effective from 2022, january the first Set a schuedle time Daily at midnight (see below table) Can also be a cron job We have a set few options of built in timings we can use, or a cron job. Presets Preset Meaning Cron None Don\u2019t schedule, use for exclusively \u201cexternally triggered\u201d DAGs N/a @once Schedule once and only once N/a @hourly Run once an hour at the beginning of the hour 0 * * * * @daily Run once a day at midnight 0 0 * * * @weekly Run once a week at midnight on Sunday morning 0 0 * * 0 @monthly Run once a month at midnight of the first day of the month 0 0 1 * * @yearly Run once a year at midnight of January 1 0 0 1 1 * Note: use schedule_interval=None opposed to directly quoting with \"none\" Another way to set out the config is like below: default_args = { 'owner': 'airflow', 'depends_on_past': False, 'start_date': datetime(2015, 12, 1), 'email': ['airflow@example.com'], 'email_on_failure': False, 'email_on_retry': False, 'retries': 1, 'retry_delay': timedelta(minutes=5), 'schedule_interval': '@hourly', } dag = DAG('tutorial', catchup=False, default_args=default_args) Each section is self explanatory (provided you can understand the general key value pari's name) Some are not, I will explain below depends_on_past depends_on_past (boolean) when set to True, keeps a task from getting triggered if the previous schedule for the task hasn\u2019t succeeded. retry_delay The delay after a failure before it should try the DAG again owner Who owns the file permissions and who the program runs under The catchup is quite advanced so we will deep dive it below Catchup We need to make some assumptions here Assume Start date of 2020-1-1 Start time of 6am Schedule interval of hourly The action will happen at 2020-1-1 7am Use the equation below Trigger Point \u2192 start_date + { schedule_interval } \u2192 till the end. In simple terms If you set the create date a year ago, and upload it will create many many dag instances, to prevent this we set catchup=False The below explains this in way too much detail, but it's good to have In the above image we are initial config where everything is fine and our DAG Run happened at 6. Then we paused the DAG. Here we see that since at the next schedule DAG run was paused hence start_date for the schedule is not available. At the next schedule the same happened (DAG run was not triggered). Now we enable or schedule the DAG run from console In the above diagram we see at the next schedule previously missed DAG Runs were triggered. Notice start_date is the next schedule (9). While the Execution dates are the actual ones, if you notice, start_date are same for the last three DAG runs. This denotes backfill. So first DAG run for execution date of 6 happened then for 7 and then for 8.","title":"Airflow basics"},{"location":"automation/airflow/airflow-basics/#airflow-basics","text":"Airflow exists for the sole purpose of removing the need to write a bash or python script, setup a server with a cron job and then run your ETL pipeline. With Airflow we're doing all this under one roof with monitoring and visualization built in. Airflow uses something called a DAG (Directed acyclic graph) Airflow is primerilly written with Python scrips that compose of the below imports from airflow import DAG from datetime import datetime We need to use datetime as airflow runs certain operations on a time schedule, an example is below with DAG(\"my_dag\", start_date=datetime(2022, 1, 1), schedule_interval=\"@daily\") as dag: What happens here is we: Define the dag with a name my_dag Set a start time that the Dag is effective from 2022, january the first Set a schuedle time Daily at midnight (see below table) Can also be a cron job We have a set few options of built in timings we can use, or a cron job.","title":"Airflow basics"},{"location":"automation/ansible/basics/","text":"Ansible Basics I always create a folder for new tasks, so installing Nginx Ansible | |-----Nginx | |---hosts | |---nginx.yaml etc Change the directory to the task at hand, so here it's installing Nginx Create a file called ansible.cfg and place the below in the file. Subsitute out the hosts for the names, and the server addresses for either their hostname, should you have dns set up, or their IP if you do not. [defaults] hostfile = hosts Next we will create the hosts file, this is what ansible will reference later when we ask it to point to either a server, or a set of servers [example1] <host1> <host2> <1.1.1.1> <1.0.0.1> [webservers] web.bread dbserver.bread example.bread [production:servers] #we can call a group by using this webservers example1 Ansible is a bitch and only uses ssh with keys. follow this guide here on how to move your key to the server you want to work on to ping a server and check that asnible can see it: ansible <host> -m ping Creating the ansible job is fun. Here we will install nginx Because it's yaml, use double space --- - hosts: nginx sudo: yes tasks: - name: install nginx apt: name=nginx update_cache=yes state=latest Running an ansible job ansible-playbook <host group> <task>.yaml <--ask-sudo-pass>","title":"Ansible basics"},{"location":"automation/ansible/basics/#ansible-basics","text":"I always create a folder for new tasks, so installing Nginx Ansible | |-----Nginx | |---hosts | |---nginx.yaml etc Change the directory to the task at hand, so here it's installing Nginx Create a file called ansible.cfg and place the below in the file. Subsitute out the hosts for the names, and the server addresses for either their hostname, should you have dns set up, or their IP if you do not. [defaults] hostfile = hosts Next we will create the hosts file, this is what ansible will reference later when we ask it to point to either a server, or a set of servers [example1] <host1> <host2> <1.1.1.1> <1.0.0.1> [webservers] web.bread dbserver.bread example.bread [production:servers] #we can call a group by using this webservers example1 Ansible is a bitch and only uses ssh with keys. follow this guide here on how to move your key to the server you want to work on to ping a server and check that asnible can see it: ansible <host> -m ping Creating the ansible job is fun. Here we will install nginx Because it's yaml, use double space --- - hosts: nginx sudo: yes tasks: - name: install nginx apt: name=nginx update_cache=yes state=latest Running an ansible job ansible-playbook <host group> <task>.yaml <--ask-sudo-pass>","title":"Ansible Basics"},{"location":"automation/ansible/python-install/","text":"Python install using Ansible For some reporting, Ansible needs python! Have your hosts file as usual. create a new task called python.yaml - name : dependency provisioning hosts : daily become : yes become_method : sudo gather_facts : false tasks : - name : install python2 raw : sudo apt-get -y install python-simplejson Run: ansible-playbook python.yaml --ask-sudo-pass","title":"Ansible install python"},{"location":"automation/ansible/python-install/#python-install-using-ansible","text":"For some reporting, Ansible needs python! Have your hosts file as usual. create a new task called python.yaml - name : dependency provisioning hosts : daily become : yes become_method : sudo gather_facts : false tasks : - name : install python2 raw : sudo apt-get -y install python-simplejson Run: ansible-playbook python.yaml --ask-sudo-pass","title":"Python install using Ansible"},{"location":"certifications/cka/cka-1/","text":"CKA - Page 1 Resources Code for exam reduction: DEVOPS15 Github for additional stuff: https://github.com/kodekloudhub/certified-kubernetes-administrator-course Cluster Architecture These nodes may be physical or virtual and host applications in the form of containers This is responsible for storing the values and managing pods, containers, and then the values are stored in a key value store. This is under the master called Kube-scheduler, which checks taints, placement and restarts All actions are done via the Kubernetes API which is exposed via the kube-apiserver Everything is in the form of containers, so all the nodes need to have docker installed or a similar runtime egine (containerd or rokt) The Kubeapi server pulls from the kube-apiserver to see what needs to be done. The pods and containers need to be able to communicate, on the hosts there is a service called Kube-proxy that ships traffic around Master Manages resources Plans Schedules Monitors nodes Has the etcd cluster kube-apiserver Kube controller manager kube-scheduler Worker Node Kublet Kube-proxy Container run time supported: Docker, rkt ETCD ETCD is a distributed reliable key value store Key value store stores a key and a vlaue Normal Database example: Name Age Location Mike 32 Place 1 Not mike 54 Place 2 Whereas a key value store Key Value Name Mike Location Place 1 So querying a key value store is like: Put Name \"Mike\" and then querying is like: get name It's used for querying bits of operational data, and it's fast. Etcd listens on 2379 by default. The role it plays ETCD stores all the stuff that you get when you run a kubectl get: Nodes PODs Configs Secrets Accounts Roles Bindings Others It's important to know that when etcd runs it's to be bound to the host IP, you should then point it to kube-apiserver List all keys etcd contains etcdctl get / --prefix -keys-only In an HA deployment, etcd installed a master server to each master instance, and that they know about eachother. This is set under the --initial-cluster option The Scheduler monitors the pods and when it realises there is a pod with no node, it assigns it, then notifies etcd Kube-api server is responsible for Authenticating users Validates requests Retireves data Update ETCD Scheduler Kubelet If you're setting up K8 the hard way, kubeapi-server is installed as a binary and needs to be on the master server There are alot of certificates that are used throughout the kube-api server To View kubeapi-server details","title":"CKA - Page 1"},{"location":"certifications/cka/cka-1/#cka-page-1","text":"","title":"CKA - Page 1"},{"location":"certifications/cka/cka-1/#resources","text":"Code for exam reduction: DEVOPS15 Github for additional stuff: https://github.com/kodekloudhub/certified-kubernetes-administrator-course","title":"Resources"},{"location":"certifications/cka/cka-1/#cluster-architecture","text":"These nodes may be physical or virtual and host applications in the form of containers This is responsible for storing the values and managing pods, containers, and then the values are stored in a key value store. This is under the master called Kube-scheduler, which checks taints, placement and restarts All actions are done via the Kubernetes API which is exposed via the kube-apiserver Everything is in the form of containers, so all the nodes need to have docker installed or a similar runtime egine (containerd or rokt) The Kubeapi server pulls from the kube-apiserver to see what needs to be done. The pods and containers need to be able to communicate, on the hosts there is a service called Kube-proxy that ships traffic around","title":"Cluster Architecture"},{"location":"certifications/cka/cka-1/#master","text":"Manages resources Plans Schedules Monitors nodes Has the etcd cluster kube-apiserver Kube controller manager kube-scheduler","title":"Master"},{"location":"certifications/cka/cka-1/#worker-node","text":"Kublet Kube-proxy Container run time supported: Docker, rkt","title":"Worker Node"},{"location":"certifications/cka/cka-1/#etcd","text":"ETCD is a distributed reliable key value store Key value store stores a key and a vlaue Normal Database example: Name Age Location Mike 32 Place 1 Not mike 54 Place 2 Whereas a key value store Key Value Name Mike Location Place 1 So querying a key value store is like: Put Name \"Mike\" and then querying is like: get name It's used for querying bits of operational data, and it's fast. Etcd listens on 2379 by default.","title":"ETCD"},{"location":"certifications/cka/cka-1/#the-role-it-plays","text":"ETCD stores all the stuff that you get when you run a kubectl get: Nodes PODs Configs Secrets Accounts Roles Bindings Others It's important to know that when etcd runs it's to be bound to the host IP, you should then point it to kube-apiserver","title":"The role it plays"},{"location":"certifications/cka/cka-1/#list-all-keys-etcd-contains","text":"etcdctl get / --prefix -keys-only In an HA deployment, etcd installed a master server to each master instance, and that they know about eachother. This is set under the --initial-cluster option The Scheduler monitors the pods and when it realises there is a pod with no node, it assigns it, then notifies etcd Kube-api server is responsible for Authenticating users Validates requests Retireves data Update ETCD Scheduler Kubelet If you're setting up K8 the hard way, kubeapi-server is installed as a binary and needs to be on the master server There are alot of certificates that are used throughout the kube-api server To View kubeapi-server details","title":"List all keys etcd contains"},{"location":"certifications/google-certs/gcp-architect/gcp-architect-1/","text":"Google Architect - page 1 GCP has 200+ services The exam tests your decision making Which services do you chose in which situation? How do you trade off between resilliance, performance and cost whilst not comprimising on security What is cloud and why do we need it? Before the cloud a company would have to do 'Peak load provisioning' where you buy servers for the peak load Before cloud the cost of purchasing infrastructure was high, and an upfront cost The infrastrucutre was under-utilized and you need a dedicated infrastrucutre team When you use the cloud you 'Provision' and 'rent' resources from the provider. For this you rent them and then return them back to the 'pool' once used. This is called Elasticity and 'On demand Provisioning' Trading capital expense for Variables Expense. You are benifiting from the 'Economic of Scale' where the cloud provider gets the best deals for you. You no longer need to spend money running a datacentre. Allows you to go global in minutes. GCP is one of the top 3 cloud providers, the other are AWS and Azure GCP provides 200+ services, and has provided to be reliable and secure. Is it the 'cleanest' cloud, as it's carbon neutral We move to the cloud due to on demand cloud provisioning. When we talk about cloud applications, we talk about multiple GCP services. Course content: Regions and Zones Imagine your application is deployed in the London Region. This means that users from other locations will have High Latency. If the DC crashes, your application goes down: Low Availability. If the entire Region of London is unavailable, we will have the same architecture in a separate Region. This is what we want to do with the cloud deployment, to have them as close to the user as possible, deployment across multiple regions. Understanding Regions and Zones in GCP All the cloud providers provide us with Regions. Google has 20 regions A region is a specific geographical location to host your resources Advantages: High Availability Low Latency Global Footprint Adherence to Government Regulations How do we deploy HA in one geographical location? within each region has multiple zones Each region has 3 (or more) availability zones Each zone has one or more 'Discreet clusters' Each Zone has one or more Datacentre. these zones are all connected with low latency connections. Google Compute engine Features When you want to deploy applicaitons, you need servers. In order to deploy to the cloud you need to deploy to a Virtual machine In order to provision a machine, you need to use Google Compute engine GCE helps you to: Create and manage the lifecycle of Virtual machines Load balancing and autoscaling instances Attach storage Manage network connectivity Creating an instance There are a lot of details you need to give You need to (and have the option to) give: name Labels Region and Zone OS Firewall Understanding machine types There are some important choices we made: Hardware OS When we talk about the hardware, we need to understand the Machine type and Machine family General Purpose: E2, N2, N2D, N1 Best price-performance ratio running Web applicatoins and small-medium sized databases, dev enciroments Memory Optimized Ultra high memory workloads M2,M1 Large in memory Database Compute Optimized Compute Intensive workloads C2 Gaming applications First choice is what machine family, then the machine type e2-standard-2 e2 - Machine type family Standard - Workload 2 - Number of CPU's Machine name vCPU's Memory Maximum number of PD's Max total PD size (TB) Local SSD Egress bandwith (Gbps) e2-standard-2 2 8 128 257 No 4 e2-standard-4 4 16 128 257 No 8 e2-standard-8 8 32 128 257 No 16 e2-standard-16 16 64 128 257 No 16 e2-standard-32 32 128 128 257 No 16 Memory, Disk and Networking capabilities increase with the vCPU's Second question is what OS do we want to run - This is chosen with the Image We can pick a public Image, which are maintained by Google or opensource third parties Understanding IP addressing in GCP External IP address are Internet addressable, can be reached over the internet Internal IP address are internal to the corperate network, so a VM with the IP of 10.128.0.2 address isnt reachable from your network You cant have 2 instances with the same public IP You have have 2 separate corporate networks with the same internal network To get a static IP address, we go to VPC networks > External IP address Here we chose what network tier as well as version to use Things to note: Static IP can be switched to another VM instance in the same project Static IP remains attached even after a reboot Static IP's are billed when you are not using it! Templates We can speed up the creation of instances by using a template It's used to create VM's as well as Managed instance groups We can define the Machine Type, Image, labels and Start up script etc once then apply to many! Once the template is created, we cant update it. We need to copy it You can specify an Image family which will pick the most recent non deprecated image. There is no cost associated with creating a template Images Installing OS patches at boot can take a while to boot the instance Create an image with the patches pre-installed You can create an image from: Instance Persistant Disk Snapshot Image File in gcs Can be shared accross projects Deprecate old images (And specify a replacement image) Harden an image - Customize images to your corperate standard Startup scripts take time, where as using a snapshot makes it quicker. GCP web console When people talk about the 'Conosle' It's the web interface You can make things as a favourite and they move to the top of the list. Under home you can see the GCP Dashboard, has the project info as well as GCP status Compute Engine Scenarios Scenario Solution Pre-reqs to create a VM Project Billing account Compute engine API Enabled Dedicated hardware for Compliance, Licensing and Management needs Sole tenant node group Node Template Name Node type Affinity Labels Create a VM Under management go to sole tennancy Thousands of VM's and update them and manage them VM Manager tool in GCP Login to server to install software SSH Don't want to expose the VM to the internet Configure Firewall Rules When you utilise a resource in GCP, you need to enable the API Instance groups and load balancing Instance groups Instance groups are used to manage similar vm's and have one lifecycle as a unit There are 2 types of Instance groups: Managed instance groups Identical VM's created using a template Same image, same machine type same verion Health check Check the server is responding Auto-scaling Scale the resources up based on a metric Managed releases can go from version to version with no downtime Unmanaged instance groups Have VM's with different configurations This is used to group vms with different configurations With the group you dont get any of the features of Autoscaling or Autohealing NOT recommended unless you need different kinds of VM's Location can be either Zonal or Regional Regional gives you HA MIG An identical set of VM's that are created with a template Maintain a number of instances If an instance crashes, MIG will replace it Detect an application failure using health checks Increase instances based on load (Autoscaling) Add a Load balancer to distribute the load Create Instances in multiple zones (Regional MIG's) Regional migs provide higher avalibliity compared to Zonal Migs Release new applications with no downtime Rolling updates Canary deployment (test new version of instance template and only push to a select few) Creating a MIG You need an instance template Configuring autoscaling Maximum number of instances Minimum number of instances Autoscaling metrics CPU, LB utilisation, Stackdriver metrics Cooldown period How long to wait before looking at the autoscale metrics again before scaling Scale in control You dont want a sudden drop in the number of instances, Example: don't scale down by more than 10% or 3 instances in 5 minutes Auto healing Configure a health check with an initial delay How long to wait from scaling the instance (Creating it) before you check the server's health When creating a MIG, you have 3 options: Stateless Supports: Autoscaling Autohealing Auto-updating Multi-zone deployments LB Statefull Disk and metadata perservation autohealing and updating Multi-zonal deployment Load balancing Unmanaged instance group LB Updating managed instance groups We can do a rolling upgrade Gradual update Specify the new template You can also select a new template for a canary deployment You set the instances to be swapped out once all is good they all switchout You can pick a set of instances to remove and switchout Specify how the update is done When should the update happen? Immediately When the instance group is resized How should they be updated Maximum surge: How many instances should be added at any point in time Maximum unavailable: How many instances can be offline Rolling restart/replace: Gradual restart of all instances in the group No change in template, but restart existing VM's Exam question: Q: How to update but have the same number of instances in the group? A: Maximum unavailable = 0 Exam question: Q: True or false: Unmanaged instance groups provide you with self-helaing and auto-scaling capabilities A: false Exam question: Q: Can a MIG contain different machine types? A: No E: This would be an umanaged instance group Exam question: Q: How can you prevent frequest scaling up and down of vm instances in a MIG A: Cool down period Load Balancing a Cloud LB distributes traffic between regions and instances Fully distributed software managed service Important features: Healthcheck Allows you to recover from failures Autoscaling Global load balancing with Anycast IP can serve global traffic with this IP address Internal load balancing Allows you to do vm to vm loadbalancing Enables: HA Autoscaling LB scales on requests Instances scale based on requests Resiliancy Because of health check it can distribute traffic to healthy instances Terminology: Backed - group of resources that can receive traffic Front end - Specify an IP address, port and protocal. This is the IP address for your clients for SSL, a cert must be assigned Host and path rules (For http(s) LB's) Defines the rules redirecting the traffic to different backends Based on a path : breadnet.co.uk/blog vs breadnet.co.uk/download Based on a Host: uk.breadnet.co.uk vs us.breadnet.co.uk Based on HTTP Headers (Auth headers) and methods (POst, GET, etc) SSL/TLS Termination/ Offloading Client to LB: Over the internet HTTPS is recommended LB to VM: Through internal network HTTP is ok where as HTTPS is preffered SSL/TLS termination/ Offloading Client to LB: HTTPS/TLS LB to VM: HTTP/TCP How to choose your LB This is important to know Load Balancer Type of traffic Proxy or Pass-through Destination ports External HTTP(s) Global, External, HTTP or HTTPS Proxy HTTP/80/8080 HTTPS/443 Internal HTTP(S) Regional, Internal, HTTP or HTTPS Proxy HTTP/80/8080 HTTPS/443 SSL Proxy Global, External, TCP without SSL offload Proxy Many TCP Proxy Global, External, TCP without SSL Offload Proxy Many External Network UDP/TCP Regional, External, TCP or UDP Pass-through Any Internal TCP/UDP Regional, Internal, TCP or UDP Pass-through Any Load balancing ac cross MIGs in multiple regions Regional MIG can distribute instances in different zones of a single region Create multiple regional MIG's in different regions (In the same project) HTTP(S) load balancing can distribute load to multiple MIGS behind a single External IP address User requests are redirected to the nearest Region Loadbalancing only sends traffic to healthy instances If a health check fails the instnace is restarted Ensure the healthcheck from the LB can reach the instance group (Firewall rules) All the backends within a region are unhealthy Traffic is distributed to healthy loads as always Multiregional Micro-services Global routing: Routes to the nearest instance group Needs network premium teir Forward rule and it's external rule are regional All back ends need to be in the same region Exam Question: Q: True or false: HTTPs LB can balance load between MIGS in different regions A: True Exam Question: Q: Which of these networking tiers is recommended if you want to use global HTTPS LB A: Premium Exam Question: Q: How many HTTPS LB's backends do you need to support 3 microservices each with 2 migs in 2 different regions O: 1 (One backend service can route between multiple microservices) O: 3 (One for each version of the Microservice) O: 6 (One for each MIG) A: 3: E: There are 3 micro-services, so url/ms1 url/ms2 url/ms3 each pointing to a backend, as you can have multiple backend groups per service Compute engine & Load balancing for Architects It's not sufficient to get things working. We want more! Build resiliency Increase availability Increase scalability Improve performance Improve security Lower costs Professional architect: Need to know the services Learn to build highly resillient, Highly avalible, scalable secure and perfomant with low cost Availability Percentage Downtime (Month) Comment 99.95 22 Minutes 99.99 4:30 minutes Most online/ SaaS aims for 99.99 99.999% :26 This is a tough one The Availability is the whole application! This includes the the API, Database, Front end etc High Availability architecture Multiple regional MIG's per Microservice Distribute load using Global HTTPS Load Balancer Configure health checks for MIG's and LB Enable Live Migration on the instnaces Advantages Instances distributed accross regions Even if a region is down, your application is avalible Global LB is HA Health checks ensure Auto-healing Compute engine Features: GPU How do you accelerate maths intensiveness and graphic intensive workloads Add GPU to your virtual machine High performance for math intensive and graphic workloads Higher cost Use images with libraries installed Otherwise GPU wont be used GPU restrictions: Not supported on all machine types On host Maintanance: Value must be terminate Recommended availability policy : Automatic restart - ON GCE Security & Performance Security Use firewall rules to restrict traffic Use internal IP address where possible Use Sole tennants where the regulatory needs Use hardened images to launch your vm's Performance Chose the correct machine size Use GPU and TPU to increase perfomrance Use GPU to accelerate math and graphic intensive workloads Use TPU's for massive matrix operations (Tensor processing unit for AI) Prefer creating hardened custom images opposed to installing software at startup Resiliency for GCE and LB Resiliency - Ability for a system to provide the needs it's expected to provide when one or more parts break Build resillient archiecture run VM's behind an LB in a MIG Have the right data avalible Use cloud monitoring (Stack driver) Install logging agent to send logs to cloud logging Be prepared for the unexpected (And changes) Enable Live migration and automatic restarts where Availible Configure the correct health checks Up to date image is copied to multiple regions Cost efficiency for GCE and LB Autoscaling have optimal number of VM instances running Understand sustained use discounts Make use of commuted use discounts Discounts Sustained use discount Automatic discounts for running VM instances for significant portions of time Example: If you use N1 and N2 machines for more than 25% of the month, you get a 20-50% discount on every incramental minute No action required on your part Applicable for instances running GKE Does not apply for E2 and A2 Does not apply when using App Engine flexible and Dataflow Committed use discount For workloads with predictable resource needs Commited for 1-3 years up to 705 discount based on machine type and GPU's Applicable for Instances created using GKE Does not apply when using App Engine flexible and Dataflow Running fault tolerant non-critical workloads Preemptive vms are a good choice. Short lived (up to 80% cheaper) can be stopped by GCP at any time within 24 hours you get 30 second warning before termination You should use them if Your application is fault tolerant You're very cost sensative workload is not Immediate Non-immediate batch processing jobs RESTRICTIONS Not always avalible No SLA and cannot be migrated to regular VM's No automatic restarts Free tier credits do not apply To save state, create metadata with the key of shutdown-script and a script on the server to run Billing for GCP You are billed by the second (After a minimum of one minute) (If you start an instance you are billed for a minute You are not billed when the instance is stopped You are billed for any storage attached that isn't deleted You should set up budget alerts Saving money Chose the right VM for the workload Discounts Sustained use discount Commited use discount Preemptive VM G cloud Most GCp services can be interfaced with gcloud You can create, delete update and read from the cli There are some services that have specific CLI tools Cloud storage: gsutil Big query: BQ Cloud Bigtable: CBT Kubernetes: kubectl for 75% of the resources you can use gsutil You can use `gcloud init` to initilize the gcloud command like tool you can use `gcloud config list` Gcloud command structure The command is split in to gcloud GROUP SUBGROUP ACTION Where it goes: Group: Config or compute or container ot dataflow Which service are you playing with Subgroup Instances, images, instance-templates etc Which sub group of the service do you want to play with Action Create, list, destroy etc Example: gcloud compute instances list To get all info about an instance you would use gcloud compute instance describe GCLOUD: Things to remember gcloud shell is backed by a vm instance 5GB of persistent storage in $HOME latest SDK's (Docker, gcloud etc) Instances inavice under 20 minutes are terminated after 120 days of inactivity even you $home is deleted cloudshell can be used to SSH in to individual machines Managed Servcices Running in the cloud You dont want to run in the cloud the same way you did before in a datacentre Terminology Iaas PaaS FaaS CaaS Serverless IaaS & PaaS IaaS is only using the VM's and setting everything up your self. You are responsible for: Application code Configuring LB Autoscaling OS updates and patches Avaliblity PaaS is when you use a platform from the cloud provider The cloud provider is responsible for the deployment and managment All you need to do is focus on the application code example is App Engine in GCP Containers/ Microservices Instead of building a large monolithic service, you build lots of small ones and build them in many languages Enterprise is heading towards microservices Build small focused microservices Flexibility to innovate Deployments become more comples This is where containers come in to play Docker You can create a docker image for each of your microservices Create a docker image for the MS Docker images have all your needs application run time application code and dependencies Ability to run anywhere Local machine Corporate data centre cloud Advantages Containers are lightweight Do not have a guest OS Isolation If there is an issue with the container, it wont affect anything Cloud agnostic/ neutral Container Orchestration There are a number of container orchestration solutoins When using it, you create a yaml deployment telling the orchestrator how many deployments Typical features Auto scaling service discovery Helps microservices to know where they are with no hard coding Load balancing distribute load Self healing Do healthchecks and replace failing instances Zero-downtime deployments Release a deployment with no downtime App engine App engine is the simplest way to deploy your applications in to GCP Supports: Go, Java, .NET, Node.js, PHP, Python, Ruby (Preconfigured run times) connect to a variaty of google cloud storage products No Usage charges Pay for resources provisioned Features: Automatic load balancing and Auto Scaling Managed platform updates and application health monitoring Application verisioning Traffic splitting Compute engine vs App Engine Compute engine: IAAS More flexibility More responsibility Choosing image Installing software Choosing hardware Fine grained access/ permissions Avaibility etc App Engine PaaS Server-less Lesser responsibility Lower flexibility App Engine Enviroments Standard applications run in language specific sandboxes Complete isolation from OS, Disks and other apps V1: Java, Python, PHP, Go (Old versions) Only python and PHP restricted network access Only white-listed extensions and libraries No such restrictions V2: Java, Python, PHP, Jode.js, Ruby, Go Full network access and no restrictions Flexible Applicaitons run within docker containers Make use of compute engine virtual machines Supports ANY runtime Provides access to background access and local disks App Engine: Application component hierarchy Application: One app per project (Acts as the container for the deployment (Not a docker container) Services: Multiple microservices or app components Each service can have different settings Was called modules Versions(s): Each version associated with code and configuration Each version can run in one or more instances Multiple versions can co-exist Options to roll back and split traffic Comparing app engine standard vs flexible Feature Standard Flexible Pricing factors Instance hours vCPU, Memory & PD Scaling Manual, basic, Automatic Manual, Automatic Scaling to zero Yes No Instance startup time seconds Minutes Rapid scaling Yes No Max. Request timeout 1-10 minutes 60 minutes Local Disk Mostly (Except for v1) can write to /tmp Yes; ephemeral. New disk on startup SSH for debugigng No Yes From the looks of it, flexible seems more like a glorified GCE App Engine: Scaling instances Automatic - Automatically scale instances based on the load Reccomended for continously running workloads Autoscale based on CPU Target thresholf Max concurrent requests Configure max and min instances Basic - Instances are created when requested Reccomended for Adhoc workloads Instances shutdown if ther eis ZERO requets tries to keep costs low High latency Not suported by app engine flexible Conficure max instances and idle timeout Idle timeout is the time from the last request Manual configure the number of instances GKE Managed Kubernetes service Minimize operatoins with auto-repair and auto-upgrade Provides pod and cluster autoscaling Runs on COS (Container optimized OS) Commands To connect to the cluster and set your kubectl: gcloud container clusters get-credentials cost-optimized-cluster-1 --zone us-central1-c --project fourth-jigsaw-307721 then you can use kubectl If you need specific workloads to run, you can add a pool. This can be a GPU workload for example Service and Ingress Service are a set of posds within anetwork that can be used for load balancing and discovery Ingress are a collection of rules for routing external http(s) traffic commands See Here Deployments You can deploy in YAML which is the suggested approach as yaml is \"declarative\" so you tell it what you want to do when you do this, you can use a file kubectl apply -f <file.yml> This still very much needs to follow the Order of operations tho Node pools when you want to deploy a service that for example needs access to a GPU, you can setup a new node pool gcloud container node-pools create <pool name> --cluster <cluster name> gcloud container node-pools list --cluster <cluster name> when it comes to using that node pool, in deployment.yml you will use: nodeSelector: cloud.google.com/gke-nodepool: <pool name> Understanding GKE cluster Cluster: Group of compute engine instances Master node: Manages the cluster worker node: Runs the workloads Master Node: (Control plane) API Server: Handles all communicatoin for a K8's cluster Scheduler Works out where to place things Control manager Managed deployments and replica sets etcd Distributed database storing the state of the cluster Worker nodes Runs your pods Kubelet Manages communication with the master node Type Description Zonal cluster Single Zone - Single control plane. nodes run in same zone Multi-zonal - Single control plane but nodes running in multiple zones Regional cluster Replicas of the contol plane runs in multiple zones of a given region. Nodes also run in the same zone where control planes run Private cluster VPC-Native cluster. Nodes only have internal IP address Alpha Cluster Access to early features for API Pods, containers etc A pod is the smallest depolyable unit It contains one or more containers Each pod is assigned one or more epeheral IP address All containers in a pod share: Network Storage IP Address Ports Volumes (Shared PD) They can have many status: Running, Pending, Succeeded, failed or unknown deployment vs replica set A deployment is created for each microservice kubectl create deployment m1 --image:m1:v1 deployment represents a microservice (With all it's releases) deployment manages new releases ensuring 0 downtime replica set ensures that a specific number of pods are running for a microservice Deployment is from shifting from one release to a new release replica set ensures that always has the correct number of pods Kubernetes - service Service Ensure that the external users are not inpacted when: Pod fails New release happens create a service exposes pods to the outside world using a stable IP Ensures the the external world does not get impacted Three types of service cluster IP: Internal to the cluster LoadBalanccer: Exposes the service via the cloud providers load balancer NodePort : Exposes service on each nodes' IP address Use case: You dont want to create an external load balancer for each microservice, so create an ingress component to balance the load) Kubernets Ingress This is the reccomened approach for providing access to services in a cluster Provides load balancing and SSL control traffic by defining rules Reccomendeatoin: Node Port service to each microservice. expose using an ingress rule Ingress allows you to use a single load balancer and control ingress in to multiple micro services Container registry Once you have created a docker image, you need to push it somewhere There is one fully managed by google called Google Container registry (GCR.io) (alternative) docker HUb Can be integrated with CICD (Cloud build) GCR also has the ability to scan your containers for vulnerabilities Naming: gcr.io/<project-name>/<container name>:<tag> Creating docker images Docker file contains what the container needs to do to be created FROM alpine:8.16.1-alpine WORKDIR /app COPY . /app RUN npm install EXPOSE 5000 CMD node index.js Docker file explination FROM: use a base image WORKDIR: where the commands are to take place RUN: execute a command EXPOSE: Expose a network port COPY: copy a file from local to remote CMD: when the container is used, what command should be run when the container starts Best practices: Image should be as small as possible Use small images (Alpine) Do not copy unescarry node modules Move the things that change the least to the top for each command, a layer is created To speed up the creation, use as little layers as possibe that changes Google Cloud functions Imagine you want to execute some code when an event happens A file is uploaded in cloud storage An error log is written to Cloud Logging A message arrives to pub/sub Enter Cloud Functions Run code in response to events Great thing with cloud functions is you don't need to worry about the scaling of the code Time bound Default: 1 Minute Maximum: 9 Minutes You cant use cloud functions to run a big batch job each run is run in a seperate instance so there is nothing shred cloud Functions: concepts Event: Upload an object Trigger: what function to trigger when an event happens When an HTTP call is recieved, you can run a job cloud run & cloudrun for anthos Cloud run: \"from container to production in seconds\" Fully managed, serverless platofrm Zero infrastructure to deploy Pay per use (for CPU, memory and requests as well as networking) fully integrated, end to end developer experience No limitations in languages easily portable as it's a container End to end develper experience cloud code - IDE Cloud Build - cicd Cloud monitoring - Monitoring tool Cloud ligging interacoitns - tracing Anthos - run K8's anywhere cloud, multi-cloud, anywhere Cloudrun for anthos Deploy the workloads to anthos clusters running on prmeise or on google cloud Description Command Deploy a new container gcloud run deploy <service name> --image <container image url> --revision-suffix v<number> first deployment creates a service and revision Next deployment for the same service create new revisions List available revision gcloud run revisions list Adjust traffic assigments gcloud run services update-traffic <service name> --to-revisions=v<number>=<number percentage>,v<other verison>=<number percentage> KMS Encryption Data at rest: Stored in a device or a backup data on a hard disk, in a database or in archives data in Motion data that is moving over the network 2 types: In and out of the cloud (from the internet) within the cloud Data in use: Active data in a non-persisted state Example: Data in your ram Symmetric key encryption Symmetric key encryption algorithms use the same key for encryption and decryption Key factor 1: choose the right encryption algorithm Key factor 2: How do we secure the Asymmetric key Encryption 2 keys: Public and private Also called Public Key Cyprography Encrypt data with public key and decrypt with private key Share the public key with everybody and keep the private key with you Cloud KMS Create and manage Cryptogrphic keys (Symmetric and Asymmetric) Control their use in GCP applications and services Provide an API to encrypt, decrypt or sign data Use existing cryptographic keys created on-premise Integrates with almost all GCP services that need data encrypted google-managed - No configuration required Customer managed- Use keys from KMS Customer supplied - Provide your own keys Protection level HSM Hardware Software Software You can pick what key to use when crating a VM Ensure that the service account has the correct IAM roles Storage Types Block storage Persistent disk Zonal: replicated in one zone Regional: Data replicated in multiple zones Local SSD's : Local block storage Scratch disk : Not all machine types support local ssd. File Storage: Filestore Block storage Hard drive Only can be attached to one server Can attach read only block devices to many instances You can connect multiple block storage devices to each VM Use: DAS SAN High performance Databases Local SSD Physically attached to the host of the vm instance Typically used to hold cache Lifecycle is tied to the VM instance Restart the instance and data is gone High IOPS Key is google managed Not all machine types support Local SSD Supports SCSI and NVMe Ensure that your image has support For better performance, get a bigger one. Higher IOPS, or more vCPU Cannot detatch and attach to another instance Persistant disk Network provisioned block sotrage Increase whilst running Performance increase with size Can remove and attach from instances Regional PD's are x2 more expensive than zonal PD's Feature Persistent Disks Local SSD's Attached to VM instance As a network drive Physically attached Lifecycle Seperate from VM instance Tied with VM Instance I/O speed Lower (Network latency) 10-100x of PD's Spanshots Yes No Use case Permanent storage Ephemeral storage Persistent Disks - Standard Feature Standard Balanced SSD Underlying Storage HDD SSD SSD Referred to as pd-standard pd-balanced pd-ssd Perfomance - Sequential IOPS (Big/data batch) Good Good Very good Performance - Random IOPS Bad Good Very good Cost Cheapest In between Expensive Use Cases Big data (Cost efficinet) Balance between cost and eprformance Persistent disks - Snapshots Take a Point in time snapshot of your PD's Schedule snapshots Also Auto-delete snapshots after x days Multi-regional Share across regions and projects Incremental Keep similar data together Keep only boot info on the boot disk Avoid taking the snapshots less than an hour apart Creating snapshots from disk is faster than creating from images But creating disks from image is faster than creating from snapshots Snapshots are incrimental If you are repeatidly creating disks from snapshots: Create an image then create disks Attaching gcloud compute instances attach-disk <instance-name> --disk <disk-name> list the block devices lsblk make the file system Format it mount it assign permissions Resizing gcloud compute disks resize <disk name> --size <size> File storage Where files are stored Media workflows For users to have quick and secure access Can be shared by several servers NFSv3 Provisioned capacity How large a filestore do you want High performance filestore 16gbps 480k IOPS Supports SSD and HDD Object storage Cloud storage Types Standard General storage Nearline Less than once a month Coldline Less than once a quater archive Less than once a year Treat the entire object as one block, if you want to update it, you have to push the whole image (for xample) Rest API to access the items Provides a CLI Not cloud gsutil When moving data the the cloud the best solution is to first move it to gcs then the product bucket names should contain only lower case, number letter hyphens and underscores 3-63 charachters should not contain google or start with goog Unlimited objects in a bucket Each object is identified with a unique key Maximum object size is 5TB Object versioning It's enabled at bucket level If you delete the live object, it becomes a non-current version each version is identified by an object key and a generation number Object lifecycle managment How to save costs You will use object managemnt lifecucle Use conditions Age CreatedBefore IsLive MatcehsStorageClass NumberOfNewerVersion Based on these ctiteria: Move the objects Delete the object All Automated Encrypting cloud storage Cloud storage encrypts data on the server side by default Cloud storage will encrypt the data 2 types Server side Depending on GCS to encrypt it Google Managed Customer managed key Ensure that the user has the correct IAM permissions Client side Encrypting before sending You need to send the correct key when you store the data Ensure that data is encrypted at rest Add in the API headers Metadata Items have metadata attached to them Fixed key metadata These are the google provided ones we cant change Cach-control - If the object is served to a user how long can they cache it for Compliance Configure data retention period You can lock/ unlock a retention policy By locking no one can edit the policy Action is permanent. You cant decrease it's retention period Same thing can be done on bucket creation Best practices Avoid sensitive names Store in the closest region Ramp up gradually the writes and reads per second Do not use sequential names Mount to a folder using cloud fuse Transferring data to the cloud Most popular solution is moving to GCS Good for one time use sub 1TB On premise or another google storage bucket Storage transfer service Transfer from other cloud providers Setup repeat schedules reliable and fault tolerant More than 1TB options GCS S3 Azure Transfer appliance Physical Data appliance that is shipped to your Datacentre Machine image Machine image is different from an image Multiple disks can be attached with a VM A machine image contains everything you need to create an instance Basically a 1:1 copy of the whole thing. Scenarios Machine image Persistent disk snapshot Custom image Instance template Single disk abckup Yes Yes Yes NO Multiple disk backup Yes No No No Differential backup Yes Yes No No Instance cloning and replication Yes No Yes Yes VM Instance configuration Yes No No Storage - Scenarios - Persistent Disks Scenario Solution Improve the performance Increase the size of the PD or add VCPU Increase durability of PD Regional PD Hourly backup Schedule hourly snapshots delete old snapshots from schedule Configure it as pert of your snapshot scheduling\u200b Review - Global, regional and zonal Global Images Snapshots Image snapshots Regional Regional MIG Regional MIG Zonal Zonal MIG Instances Persistent disk You can attach directly to an instnace","title":"Google architect - Page 1"},{"location":"certifications/google-certs/gcp-architect/gcp-architect-1/#google-architect-page-1","text":"GCP has 200+ services The exam tests your decision making Which services do you chose in which situation? How do you trade off between resilliance, performance and cost whilst not comprimising on security What is cloud and why do we need it? Before the cloud a company would have to do 'Peak load provisioning' where you buy servers for the peak load Before cloud the cost of purchasing infrastructure was high, and an upfront cost The infrastrucutre was under-utilized and you need a dedicated infrastrucutre team When you use the cloud you 'Provision' and 'rent' resources from the provider. For this you rent them and then return them back to the 'pool' once used. This is called Elasticity and 'On demand Provisioning' Trading capital expense for Variables Expense. You are benifiting from the 'Economic of Scale' where the cloud provider gets the best deals for you. You no longer need to spend money running a datacentre. Allows you to go global in minutes. GCP is one of the top 3 cloud providers, the other are AWS and Azure GCP provides 200+ services, and has provided to be reliable and secure. Is it the 'cleanest' cloud, as it's carbon neutral We move to the cloud due to on demand cloud provisioning. When we talk about cloud applications, we talk about multiple GCP services. Course content:","title":"Google Architect - page 1"},{"location":"certifications/google-certs/gcp-architect/gcp-architect-2/","text":"Google cloud architect - page 2 IAM Resources Humans Non-humans Identities can be: Users Groups Application running in GCP Application running in your Datacentre Unauthenticated users Granular control Limit a singular user Perform actions On a specific cloud resource From a specific IP address During a specific time IAM: Example Want to provide access for a colleague to specific cloud bucket Important generic concepts Member: My Colleague Resources: Specific cloud storage bucket Action: Upload/ Delete objects In google cloud IAM Roles: Set of permissions Roles do not know about members How do you assign permissions to a member: Policy: You assign (Or bind) a role to a member Choose a role with the right permissions Create a Policy binding the members with the role IAM: Roles Roles are permissions Perform some set of actions on some set of resources 3 types basic roles: (or primitive roles) Viewers (roles.viewer) read only access Editor (roles.editor) Viewer and edit action Owner (roles.owner) Editor + manage roles and permissions + Billing Not recommended: Don't use in prod Predefined rules - fine grained riles predefined and managed by google Custom roles - when pre-defined roles are not sufficient If creating a role, ensure you give it a good ID There are also role versions to use, so: Alpha, beta, General availability, Disabled Most of the roles will give you: resourcemanager.projects.get resourcemanager.projects.list IAM - Most important concepts You create a role which contains the permissions You bind the role to a user with a policy IAM - Service accounts An application needs to auth with something on the GCP platform You do not want to use an actual account for this, so you use a service account Identifiable via email: Does not have a password Has a private/public rsa key cant login via browser or cookies Service account types Default service account - Not reccomended to use User managed - User created Provides fine grained access control Add roles to the service account if you're backing up to GCS You cannot assign a service account to an on-prem application This is called long lived Create service account with right permissions Create service account user managed keu gcloud iam service-accounts keys create Make the service account key accessable by your application ENV variable should be: GOOGLE_APPLICATION_CREDENTIALS use google cloud client libraries - ADC (Application default credentials) Short lived: Few hours of access Credential types: Oauth2 OpenID Connect ID tokens Service to service authentication for short periods of time Self signed json web tokens IAM Best Practices Principle of least privelage Give the least possible privelage needed for a role Use service accoutns with Minimum privelages Seperation of Duties Involve at least 2 people Have seperate traffic deployer and traffic migrator roles Breaks up issues from one person not seeing something Constant monitoring Always view the logs and see who is accessing your service account Ensure audit logs are archived Use groups when ever possible Bind roles to groups Just add a new user to the group Understanding Identity managment Email used = Super Admin Google workspace If you use workspace, you can link them and manage groups from there Not using Workspace You can link your identity provider Federate google cloud with your identity provier Fererate You want to auth with your external idP (Identity provider) Example isActive directory We can 'Federate' cloud identity Example: Federate AD with GCDS then ADFS Corporate directory federation Federate identity provider with your external IDP Enable SSO Users go to google cloud Redirected to external IDP Users sign in SAML assertion is send to GCP Iam Members/ Identites Google account Service account Google group Unique email adress Manage permissions in one place Workspace Domain Cloud Identity as a service (IDaaS) ACL - Access control list Access Control Lists define who has access to your buckets and objects How is this different from IAM? IAM permissions apply to all objects in a bucket ACL's can be used to customize specific access to different objects Users get access if they are allowed by either IAM or ACL Use IAM for permissions to the whole bucket (Uniform access) ACL for access to individual objects Buckets have 2 levels: Fine-grained Per object access Uniform NO object level ACL Signed URL Allow users to access the objects for a short period of time No login required Create a signed URL with a service account with the desired permissions, then create a key for that URL, and assign it to the signed URL gsutil signurl -d <number>m <key> gs://<bucket>/<object path> Web servies Expose static site using cloud storage Create a bucket with the same dns name as your site Verify that you own it Copy files to the bucket Set uniform access to the bucket Set to `All Users` grant view all objects Databases Fundamentals Databases provice organized and Persistent storage for your data To choose between different databse types we would need to understand: Availability Durability RTO RPO Consistency transactions Availability and Durability Availability Will I be able to access my data when I need it Percentage of the time an application provides operations Durability Will my data be available in x years What does durability mean? Not losing data How to increase it Replicate the data Ensure sonsistency RTO & RPO RTO Recovery time objective Maximum acceptable downtime RPO Recovery point objective Maximum acceptable period of data loss Read Replicas If you have an application that has high reads, you can create a read replica If there is a master failure you can promote one of the slaves to master Consistency Strong consistency Synchronized to all replicas Transaction will slow down if you have alot of replicas Eventual constancy A little lag between replication Used when scalability is more important than data integrity Read after write consisteny Inserts are immediately available in all replicas Updates are doe via eventual consistency Categories Relational OLTP Online transaction processing OLAP Online analytics processing Document Key value Graph In memory Factors Do you want a fixed recovery? Transaction properties (Atomicity and consistency) Latency Transactions p/s Data stored Relational Databases This was the only option a while back Predefined schemas Strong transactional capabilities GCP Databases OLTP Applications where users make lots of small transactions (updates etc) Popular: Mysql Oracle SQL server Recommended managed service: CloudSQL CloudSpanner Unlimited scale and global scale OLP Running analytics Datawarehouse BigQuery Difference between OLTP and OLAP OLTP Databases use rows Each table row is stored together Efficient for small transactions OLAP Each table column in stored together High Compression Distribute data easily Execute query across multiple nodes NoSql New approach NoSQL = Not only SQL Flexible schema Lets the schema evolve with the data Horozontal schale to petabytes Typical nosql has scability and high performance Products Cloud Firestore Serverless Document storage storage ACID Firestore has SDK's Small to medium CloudBigtable Scalable Managed recommended for data greater than 10tb to petabytes Not reccomended for transactional No support for multi-row transactions Reccomened for: Large streams of data In memory databases In memory stored database Retreival is faster Service: Memory Store Summary Database type GCP Services Description Relational OLTP Databases Cloud SQL, Cloud Spanner Reansactional use cases needing pre-defined schema and a very strong transactinal capabilities. CloudSQL : Mysql, postgres sql CloudSpanner: Unlimited scale and 99.999% availability Relational OLAP Databases BigQuery Column storage with pre-defined schema. Datawarehouse and big data NoSQL Cloud firestore, Cloud Bigtable Apps needing quick evolving structure (Schema-less) Cloud firestore: Serverless transactional document DB, 0.5tb BigTable: Large database (10tb+) straming IOT and analytical workloads. Not serverless In memory databases/ cache Cloud Memory store Applications needing microsecond response CloudSQL fully managed Database Supports MySQL, PostgreSQL and SQL Server Regional Service SSD or HDD up to 416 RAM and 30TB Use cloud SQL for Simple relational Cases Migrate Local MySQL Reduce maintenance costs Use cloud Spanner instead of CloudSQL IF: Huge amounts of data Need infinite scaling Global distribution Higher availability To connect: gcloud sql connect <instance name> --user=root --quiet Features Automatic encryption (Tables and backup) Highavaliblity and failover Create a standby with automatic failover Pre-req: Automated backups and binary logging Read replicas Cross zone, cross region Automatic storage increase Point in time recovery Pre-req: Enable binary loggong Best practices Cloud SQL PROXY securly connect to the cloud SQL from your apps Understand Scalability Enable HA configuration for HA Read replicas help you offload read workloads Does not increase avalability Have a number of cloudSQL instances opposed to one large one Cant distribute the writes Backup Backups are lightweight and provide point in time recovery Cannot copy or do operations on the backups Cannot backup a single database or table have to backup the entire database Export Takes longer but more flexability Export single database or table Exporting large database impacts the performance Cloud Spaner Fully managed, Mission Critica relational sql and globally distributed Strong transactinal Scales to PB Scales horozontially pay per node and per storage Data export: Cloud console Data flow No glcoud No SQL Cloud Datastore and Firestore Datastore Highly scalable NoSQL Document Database Recommended for up to a few TB Structure: Kind > Entity flexible schema with transactions Export only from gcloud Export contains metadata file Being replaced with Firestore Firestore Offline mode and data synch Provides libraries Offers datastore in native moce Both Serverless Storing data in Firestore Collection One or more document Sort of like a table Document Has an ID Has fields and values Indexes Automatically created Provides a simple ability to search Ability to pick where you query Cloud firestore automatically indexes fields you add Exporting Entire database Collections Location: GCS Cloud Datastore Datastore is a document store with flexible schema Recommended for storing things like user profiles Use case: Index for objects stored in cloud storage You want to allow users to upload profile pictures Enable quick searching by storing metadata Design your keys and indexes carefully Avoid monotonically increasing values as keys Numbers, customer1... or timestamps Create only indexes that will be created in searches For adhoc, you'll want to use bigquery Prefer batch operations Cloud BigTable Petabyte scale, wide column NoSQL (HBase API Compatible) Designed for huge volumes of Analytical and Operational data Handles millions of reads and writes Single row transactions NOT Serverless Cannot export using console or gcloud Either use the .jar or HBase Command line cbt Wide Column Each table is a sorted key/ value map Each row is indexed using a row key Structure supports high read and write low throughtput latency Use cases: IOT Streams, graph data, real time analytics Transactional stock Designing BigTable tables What data do you want to store What would your frequent used queries look like? Design your key/value store Each table only has one index You can store multiple key segments, separated by a deliminator If you frequently search recent data Start backwards so most recent data is first Best practices Recommended for streaming IOT and Time Series Automatically shards data in to multiple tablets Tablets are distributed accross different nodes Goals: Same amount of data on each node Distribute reads and writes equally Pre-test with heavy load This is for allowing the nodes to balance them selves Supports HDD and SSD SSD for latency sesative Increase reliability and durability Create multiple replicated clusters Can create a cloud bigtable cluster with many clusters accross regions As well as cross zone Stored data indiepndantly Store the data closer to the users Configure an app policy with routing Networking VPC VPC Can contain many subnets Regional Subnets To close off things from public you use a subnet You have 2 subnets Private Nothing here can be access from the internet Public Creating a VPC Option 1: Auto Mode Subnets are created in each region Default VPC created automatically Option 2: Custom mode VPC No subnets are created You have complete control over subnets and their IP ranges Prod recommendation Options when creating a subnet Enables private IP access for communication with google network Enable Flow logs to see inbound and outbound network traffic Creating the VPC Creating the VPC Required values Name Region subnet Private google access Should vm's in the subnet be able to access google resources with no public IP Shared VPC Shared VPC Allowing shared VPC's to connect to multiple service projects Host project: Contains shared VPC Network Service projects: Attaches to host projects VPC Peering VPC Peering You can peer multipe VPC's together All communication happens inside Google's network Network Administration isn't changed Firewall rules Firewall rules Allow access in and out Stateful rules If in is allowed then out is allowed Each firewall rule has a priority 0: Highest 65535: Lowest Default rule: Egress allowed Deny Ingress Default VPC the default VPC has 4 default rules with the priority 65534 Called fefault-allow-internal Ingress rules: Target: Defines the destination Targeting it: Service account or tag Source: Defines where the traffic is coming from Targeting it: Tags or Service accont Each rule: Priority Action on match Allow or deny Protocal Enforcment status Enable or Disable Best practices Cloud load balancing instances: 130.211.0.0/22 35.191.0.0/16 Remove 0.0.0.0/0 from source IP Cloud Operations Operations Cloud Operations To operate cloud effectivly yous hould know Is my application healthy Does my DB have enough space Are my servers healthy Cloud monitoring Measurments Graphing Alerts Channel Condition Notifications Documentation Workspace In order for it to work you need a 'Workspace' It can monitor AWS and GCP Creation: Create a workspace in a host project Add AWS or GCP account Default metrics CPU Dist traffic Uptime Network traffic For more details: Add cloud monitoring agent works on collectd Logging Real time log managment and alaysis tool Allows for storing, searching and analyzin Excabyte scale, managed service Ingest logs from anywhere Key features: Log explorer Logs dashboard Log metrics Log router Route the logs based on a condition Collection Collection Most services already spit logs to GCP logging aggregation service Ingest from GCE Install logging agent Ingest from Onpremise Use BindPlane Audit and security logs Audit and securit Access transprenct Captures actions by the GCP team done on your content Only supported for Gold+ levels Cloud audit logs answers who did what, when and where Contains: Admin activity Data access System event audit Policy denied audit logs Feature Admin Activity logs Data access Logs System event logs Policy denied logs Logs for API calls or other actions that modify the configuration of resources Reading configuration of resources Google cloud administrative actions When user or service accounts are denied actions Default Enabled Yes No Yes Yes VM Examples VM creation, patching, change in IAM Listing resources On host maintanace Security policy violation logs Cloud storage Modify bucket or object Modify/ read bucket or object Access Needed Logging/ logs viewer or project/viewer Logging/private logs viewer or project.owner logging.logsviewer or project.viewer logging.logs viewer or project.viewer Log routing Log routing Logs from various sources Log router checks against configured rules Two types of log buckets _Required, holds admin activity Zero charge Cannt change the retention period _default All other logs You are billed based on cloud logging you can disable the default log Export You can export to a bucket You can export to a bigquery dataset You can export to pub/sub in base64 You need to create 'sinks' This allows you to send logs out to somewhere else Sinks and router You cant edit the audit bucket and router sink You can send logs to other locations Big query Cloud storage bucket Pub/SUb Splunk Other project Alerting When something happens you need to create an alet Setup alert channels then setup policy Setup a metric Setup the steps to fix Uptime checks You want to ensure that your applications run all the time Cloud Trace Cloud Trace Supports google cloud services Instriment applications Find out: How long does a service take to ahndle Average request latency Support for Multiple languages Cloud Debugger Cloud debugger Captures state of running applications Inspect the sate of the application Take snapshots of variables No need to add additional logging statements No need to redeploy Very lightweight Use in any environment (Even in prodiction) Cloud Profiler Cloud Profiler Low overhead profiler gathers Major components: Profiling agents Profiler interface (Vizuliation) Error Reporting How do you identify production problems Real time exception montoring Aggregate and displays errors reported from cloud services Centralize error management Error reporting tool can be viewed from desktop Aggregates them in to one place Uses stack trace Stack driver/ Cloud monitoring Stackdriver Service New service name Stackdriver Monitoring Cloud Monitoring Stackdriver Logging Cloud Monitoring Stackdriver Error Reporting Error Reporting Stackdriver Trace Cloud Trace Stackdriver Profiler Cloud profiler Cloud Operations scenarios Scenario Solution Record all operations on a bucket Turn on data access audit logging Trace requests accross multiple microservices Cloud Trace Identify prominent exceptions or errors for microservices Error reporting Debug problem in prodcution Cloud Debugger Look at logs for specific requesta Cloud Logging Organizing GCP Cloud Resources Resource Hierarchy Well defined hierarchy Organization > Folder > Project > Resources Resources are created in Projects A folder can contain multiple projects An organization can contain multiple folders Recommendations for Enterprises Create separate projects for different environments Complete isolation Separate folder for each environment Isolate environments from one department Create a shared folder for shared resources One project per application per environment App1 and app2 dev and prod Projects: A1-DEV A1-PROD A2-DEV A2-PROD Billing accounts Billing account is mandatory Every project has one billing account You can have multiple billing accounts in one organization Create billing accounts representing your organization two types of accounts Self serve: Billed directly Invoiced: Invoice is created and sent to accounts Managing billing Budget Should set up an alert Default threshold is at 50, 90 and 100% Can send ti: Email Pub/sub Billing data can be exported to: Big query File export (Now depricated) Organization Policy Service Centralized constraints on all resources Configure an organization policy Needs organization policy role IAM focues on WHO Organizatin policy focuses on WHAT Linux stuff SSH to VM Compute engine uses Key based auth two options: Metadata managed: Manually create and configure individual ssh keys OS Login: Manage ssh without managing individual ssh keys Your Linux account is automatically linked to your cloud identity Go to metadata and set: enable-oslogin to true Users need to have roles: Roled/compute.osLogin roles/compute.osAdminLogin Windows: Password Linux: Keybased Execute a shutdown script Can be used on pre-emptible and non-preemptible VM's Runs as: Root on linux System on windows Stored as metadata key: shutdown-script value: Script --metadata-from-file shutdown-script=script.sh from cli Can store the scripts in cloud storage Run on best effort: Not if you do a reset Wont run if exceeding grace period Troubleshoot VM startup Are there quota errors? Boot disk full? Serial port Each vm has 4 serial ports Serial port output: OS, Bios, Other system level entites can access it from cloud console, compute engine api and gcloud cli Can send this to cloud logging metadata: serial-port-logging-enabled = true command gcloud compute instances get-serial-port-output Valid file system attach that disk to another vm and see if you can access files Moving instances between zones and regions Can be moved between zones in the same region gcloud compute instances move <name> --zone <zone active> --destination-zone <destination> Cant move a MIG Cant do with local SSD Not terminated instances Not across regions Manual approach Snapshot PD's create copy of PD's in new region Pub/Sub Pub/Sub Synchronous communication Example: Application on your web server make synchronous calls to the logging service What if the logging service goes down? Solution: Create a topic and put the logs on it logging service picks them up when it's ready Advantages: Decouple apps Availability: Publisher remains up Scalability Durability: Message isn't lost if the service is down Pub/Sub Reliable scalable fully managed Backbone for HA and scalable solutions Autoscales Low cost Use cases: Event ingestion and delivery streaming Support push and pull message delivery How it works Publisher - Sender of a message Publisher sends a message by making a https request to pubsub.googlepais.com Subscriber - Reciever of the message Pull Subscriber makes the https call Push Messages are sent to subscribers Subscribers provide a web hook endpoint at the time of regestration When a message is recieved on the topic, a https post message is sent to the webhooks endpoint very flexible: Publishers and subscribers, one to many, many to one, many to many Getting ready with topic and subscriptions Topic is created Subscribers are created Subscribers regisert to the topic Each subscriber represents a discrete pull of messages Multiple clients pull the same subscriptions Multiple clients create a subscription Sending Publisher sends a message Message individually delivered to each and every subscription Subscribers receive messages Push Pull Subscriber acknowledges Messages are removed from subscriptions Snapshot: Point in time snapshot of that subscription Best practices Use cases: Convert synchronous messages to asynchronous Useful when consumers cant keep up with producer Opensources: RabbitMQ Apache Kafka Apply transformations to IOT stream Processing: In order Exactly one option --enable-message-ordering Add dataflow in to flow to enable messade dedupe (Exactly once processing) Dataflow Data ETL kinda? Streaming batch usaceses Real time fraud preventionn Sensor data processing Log data processing Prebuilt templates Hybrid Cloud Cloud VPN Cloud VPN Traffic flows over the internet Traffic encrypted using IKE (Internet key exchange) 2 types: HA VPN SLA of 99.99 2 external IP's BGP Classic VPN SLA of 99.9 Static Routing Dynamic BGP Go for VPN if: Low cost Lower throughput Network to encrypt it Just started experimenting with connectivity HA VPN HA VPN High Avaliblity Regional Needs a cloud HA VPN Gateway 2 interfaces Classic VPN Classic VPN No HA Needs a google compute engine vpn gateway VPN Gateway - Regional resource Cloud router enables dynamic routing: Enables automatic route update Cloud Interconnect Cloud interconnect High speed Low latency Not encrypted Dedicated interconnect High bandwith Minimum of 10gbps Options are: 10gbps 100gbps 8x10gbps 2x100gbps Can take 2-3 weeks Partner Interconnect ideal if you need a lower bandwith connection 50mbps to 10gbps Data exchanged over private network Communicate using VPC network Reduces egress costs Internet is not used Hybrid Connectivity Hybrid connectivity Use different IP ranges Backup? Cloud interconnect, also establish a VPN Direct peering You peer to google Not a GCP Service Not reccomended Should use a GCP Service BGP routing from the datacentre Data warehouse Big query Big Query Excabyte scale modern data warehouse Relational database (SQL Schema, cinsistency) Uses SQL like commands Organized in to Datasets Traditional (Storage + compute) + Modern (Realtime + serverless) when talking about data-warehousing: Importing and exporting is important Variety of formats: CSV, Json, Avro, Parquet, OCR, Datastore backup Export to GCS (Long term storage) Visulize (Data studio) Storing long term: Compress it CSV/ Json Avro Automatically expire data Configurable Table expiratoin External sources Use external sources in Bigquery: Cloud SQL BigTable Google drive Partitinoing We are able to partition the table by date, or some value When we do a query, only some segments are scanned Cluster: Group related data Example: By category Overview: Partitioning: Table is divided in to segments Clustering: Grouping related data by category Payment: Pay for storage Data used in the query Importing Data in to BigQuery Importing in to BQ Batch Free Import after processing by Cloud Dataflow and cloud Dataproc Dataproc: Managed Hadoop Stream Expensive Pub/sub Streaming Inserts BQ data transfer service Import from External: Saas S3 Other Datawarehouses Federation Streaming Data Streaming Data Not free Lots of limitations No InsertID Can stream up to 1gb per second per project InsertID Us and EU: 500000 per second Per table: 100000 Maximum bytes per secon: 100 If streaming millions, use BigTable Best Practices Estimate: bq --dry-run Cost: Expire data Load data in Bulk Long term storage: When not edited for 90 days, cost goes down Fast for complex queries Not optimized for narrow range queries Stream your audit logs from BigQuery to Bigquery View in Datastudio Caching MemroyStore Memory Store Fully managed Failover patching etc HA Monitoring setup using cloud monitoring Supports Redis and Memcached memcached Refrence data, database query caching Session store Redis Persistence Can be accessed from: Most of the compute services AppEngine Caching Appengine caching memcached Shared Memcached (free) Dedicated: Expensive: billed by the GB hour CDN CDN Best practices Always cache static content Be careful with caching dynamic conent Use custom cache keys to improve cache hit ratio Any combination of host protocal host query string Versioned URL setting a version like ?v=1 and then new verison is v=2 so you get a cache miss Devops CICD CICD Static code analysis Lint, Sonar Source code security Runtime checks Run vulnebility scanner Tests Unit tests Integration tests System test GCP Tools Tools Cloud Source repositories Container Registry Build containers Jenkins Cloud Build Spinnaker: Multi-cloud CD IaC Iac 2 parts: Privision the infra Config managment Chef, Pupper, ansbile Salt Cloud Deployment Manager Prevents config drift Avoid mistakes Put the script in version control deployment Manager Deployment manager Template using Python JinJa2 (Only for simple) SRE SRE DevOps++ At google SRE team focus on every aspect of an application Managed by SLO (Service level objectives) Convert business requirments in to measurable items Minimize Faffage Move fast by reducing cost Best practices: Handeling excess loads Load Shedding Reduce it at source API LImits Release Management Release managment: Goals Zero downtime Only one version live at a time Test with production traffic Best practices: Small frequent changes Automate release managemnt Recreate Recreate Install the new version on the new instances Most basic approach Terminate V1 and roll out V2 Advantage: Cost effective and fast Less compatibility issues Disadvantage: Rollback is a redeploy More downtime Canary Deployment Canary Roll out to a subset of instances Once metrics are okay Roll out to more Characheristics Fast Zero downtime No extra infra Needs backwards compatibility A/B Testing A/B Set up a small new feature and test with half the users See if users like somethng Rolling Approach Rolling: Roll out the version of the software to 5% at a time (For example) Charachterists: Slow Zero Downtime Needs automation No needed infrastrucute Less impact in a failure Rolling with additional batch zero reduction in the instances that are running the old version you add in another instance whilst it runs Blue Green Deployment Blue green Create a parralel enviroment with the new version Once all tests are done on V1, we switch to the new version As far as the end users are concerned, only one release is live Characteristics Zero downtime Easy rollback Needs additional Infra Zero reduction in ability to serve Backwards capacity Shadow testing: Mirror the traffic to both the versions Stubbing needs to be desigend as it may double process a payment (EG) GKE Option Detials Recreate Set stratergy > type on deployment to Recreate Use kubectl set image deploymenr or update deployment yaml yo perform deployment Rolling Update Set Stratergy > Type on deploymenr to rolling update Sue kubectl set image deployment or udpate deploymenr yaml to deployment Blue green deployment Create new deploymenr Control traffic using Ingress (or service) Canary deployment istio mesh Compliance and regulations Certifications GCP is compliant with several important certificatoins ISO 27001 (security that helps manage informatino risks 27017 (Info and security controls that provision cloud services) 27701 Global privacy standard PCI DSS Payment card industry data security standards SOC 1 - Book keeping SOC 2 - Security of provider controls COPPA : Childrens online privacy act HIPAA : Health insurance for handeling health data PHI : Patient health Informatin GDPR : Strengthen Data protection in europe Customers are responsible for building their applications to that compliance HIPAA HIPAA GCP is compliant Customers are responsible You can execute a Google cloud business associate agreement (BAA) Do not use anything not covered by BAA Best practices: Follow IAM Enable Object versioning Auditing is exported and to BQ Disable caching for PHI PCI DSS PCI DSS Enhance card holder security security Have seperate enviroment for processing cards New account Least privelages Control inbound HTTPS requests from customers responses from third party payment processor Office network - allowed for auditing Strictly control outbount HTTPS requests to your payment processor GKE and GCE are reccomended App engine isnt allowing egress forewall rules Harden your images Only install software that is needed Automate as much as you can Configuration managment Impliment Forseti security Opensource tools that improve the security of GCP Inventory Scanner Enforcer explain Enable VPC flow logs Enable transparency logs Enable firewall logs use google DLP (Data loss prevention) when accessing data Migration Planning Plan Asses the workloads Take inventory Experiment and make POC Caclulate costs Chose the workloads to move first Plan the foundation What type of network How to connect Security Deploy Move the data gsutil transfer appliance Deploying: Automation Automate build and deploy IaC Optimize the environment Ensure that there is logging Ensure that managed services are being used Optimize costs Approaches Appriaches: Lift and Shift Replatforming Few adjustments to suit the cloud Example: Bang it in a container Repurchase Move to a cloud native porduct Refactor Serverless compute Retire EOL Retain Keep it on premise Responsibilities Business requirements Business requirments Reduce Captial Expenditure Just pay OpEx Licensing costs Computing Costs Storage costs Network costs Personal Costs (people) Other costs SLA miss API Reduce csots: Managed services Autoscaling Preepmtible-vms Increase innovation Devops Microservices Reduce mean time to recovery Improve regulation Technical Requirments technical requirments Functional Use containers Use hardened OS Container orchestrations Nosql for flexible schema Store high volumes at low cost Private network (Traffic not over the internet) Non-functional aspects Avaibility Scalability Durability Security Planning for HA HA Geographical distribution Planning for Scalability Scalability VM's in a MIG Autoscale Many regions Unmanaged No autocaling GKE Pod and cluster autoscaling for GKE Be cautions with resources that cannot scale fast Statefull applications are more difficult to scale PD: Scale Vertically and Horozontally Database: Pub/sub, BQ Cloud Datastore are serverless Bigtable, spanner cloud sql, dataproc are not serverless Planning for Security Secuity Confidentiality Data is encrypted at rest and transit Ensure HTTPS in transit Integrity Follow IAM best practice Role based Access Seperation of duties Hash verification Increases data integrity Avalibility Firewall Auto-failover Redundancy DDOS Anti-spoofing is provided by GCP Google frontend App engine sits behind google front end Firewalling Reduce attack surface Isolate internal traffic Send as much as you can over internal traffic Cloud Armout Cloud Armout Prevents against denial of service Prevents against: OWASP xss sql injetctions Provices security policies Use cases: Enable access for users with a specific IP address Create an allow list Block users from an IP Create a block list Digital Signatures Digital signatures Ensure the integrity of the data received nonrepudiation Workflow Sender performs key operation to create data signature Recipient used the public key to verify the digital signature Can use CloudKMS Provides api and commands to create digital signatures Use cases: Validating log files Validating code builds Secret Manager Secret manager Manage how your secret is encrypted You can create a Key in KMS You can access the key by using the API calling for the password. Stake holders Stakeholder managment Execs Business owners Archi team Scrum master Dev team Testing team Sec team Compliance team Communiation Early clear communication Identify the stakeholders and communicate Change Management change managment People, process and technology (systems) Understanding the change and what it means Who can make it happen Best cycle Plan, do check, act Do small changes and do multiple iterations Business continuity planning BCP How to keep the business running in case of a disaster Focuses on IT operations alone DR ENV Cloud as a DR enviroment Backup connection to the cloud Incident Management Incident management How best to avoid incidents How best to react to it Post mortem: Not who to blame, but a how to fix Data Managment Data management How the data comes in Rate it comes in What kind of Data? How much data How long is it stored for Life cycle policies","title":"Google architect - Page 2"},{"location":"certifications/google-certs/gcp-architect/gcp-architect-2/#google-cloud-architect-page-2","text":"","title":"Google cloud architect - page 2"},{"location":"certifications/google-certs/gcp-architect/gcp-architect-3/","text":"Google cloud architect - page 3 Additional Services Cloud Scheduler Cloud Scheduler Fully managed cloud enterprise scheduling tool Batch, data jobs, cloud infra opertaions Integrats with: App engine, pub/sub, cloud logging, http endpoint Automatic endpoint Use case: Schedule a message on PubSub Needs an app-engine app in the project App Engine Cron Service Runs on top of this Cloud DNS DNS You can setup DNS management via GCP Public Web accessable sites Private Only accesible from a VPC or subnet Pricing calculator Pricing calculator How to estimate it? use the google cloud pricing calculator Estimates for 40+ services These are just estimates Anthos Anthos Run your K8 cluster and on premise Multi-cluster management Consistent managed k8's Central config managment Central git repo where it's managed from Logically grouping Environs Provides a service mesh Istio A/B testing Canary rollouts Machine Learning ML Prebuilt ML No in house ML Easy to use Cloud AutoML Build custom ML models with developers having limited ML Expertise AI Platform Help Data scientists build custom models (tensorflow) Data Managment Big query ML can actually build models directly Apigee ApIgee Rest API Managing a REST API isn't easy Implimenting multiple versins of your API isnt easy Design secure and publish your API manage the complete lifecycle Provides AI powered monitoring Enable caching with cloudCDN Allows developers to access a simple development poeral Google Cloud architecture framework Operational Excellence Operational Exellence Monitor the business objectives SLA SLO KPI Test DR Increase software releases and veolcity Business health Latency Traffic Errors Saturation Logging Ensure efficient amount of logs DR RTO Recovery time objective RPO Recovery point objective Regularly test this plan Schedule a persistent disk snapshot and copy across regions Use cloudDNS Security, Privacy and compliance Plan security controls and privacy Strategies implement least privilege Build a layered approach Automate deployment of sensitive tasks Manage auth Follow IAM best practices Understand when to use a service account Use organization policy service Allowing what can and cant be done in the account Enable node-autoupgrade for GKE Use GKE sandbox when running untrusted code Secure the network Use a carefully desigend VPC Isolate workloads in to each VPC per project Can control ingress with Ingress and egress rules Use Network intelligence centre Use Object versioning Use DLP for sanitizing data Audit with Infrastructure logs Reliability Reliability Measrable goals Architect for HA, scale and automatic change managment Stratergies KPI, SLA, SLO Small changes Rollback Instriment systems for observability Document and automate emergency responses Degrade services gracefully Serve a static page when a site is down Predict peak traffic events Scale and plan Build flexible Ensure all changes can be rolled back Slow progressive rollouts Build efficient alerting Reducing mean time to detect (MTTD) Performance and cost optimization Use autoscaling and data processing Try serverless options Distribute load with a global LB Identify apps to tune Cloud tracing cloud debugging cloud profiler Analyze costs Export billing data to BQ Use google Data studio Use preemptible vms for non-critical fault tolerant vms","title":"Google architect - Page 3"},{"location":"certifications/google-certs/gcp-architect/gcp-architect-3/#google-cloud-architect-page-3","text":"","title":"Google cloud architect - page 3"},{"location":"certifications/google-certs/gcp-architect/gcp-load-balancer/","text":"GCP Load Balancer","title":"GCP Load balancer"},{"location":"certifications/google-certs/gcp-architect/gcp-load-balancer/#gcp-load-balancer","text":"","title":"GCP Load Balancer"},{"location":"certifications/google-certs/gcp-architect/kubectl-commands/","text":"Kubectl commands Connect to the cluster gcloud container clusters get-credentials <cluster name> --zone us-central1-c --project <project name> Get pods kubectl get pods Get deployment kubectl get deployments Deploy a deployment kubectl create deployment <name> --image=<docker id>/<container>:<version> Scale a deployment kubectl scale deployment <name of deployment> --replicas <how many replicas> Create an LB kubectl expose deployment <deployment name> --type=LoadBalancer --port=<external port to expose on> See services kubectl get services Resize a node pool in the cluster gcloud container clusters resize <cluster anme> --node-pool <node pool name> --num-nodes=<number> Enable HPA kubectl autoscale deployment <deployment name> --max=<max containers> --min=<number> --cpu-percent=<number> Get HPA kubectl get hpa Delete HPA kubectl delete hpa <hpa name> Auto-scaling the cluster gcloud container clusters update <cluster name> --enable-autoscaling --min-nodes=<min> --max-nodes=<max> Create a config map kubctl create configmap <name> --from-literal=<variable>=<data> Describe the details of something kubectl describe <thing> Create a secret kubectl create secret generic <name> --from-literal=<password name>=<password> Update an image kubectl set image deployment <deployment name> <deployment name>=<docker hub id and version> Get the service IP range SVCRANGE=$(echo '{\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"name\":\"tst\"},\"spec\":{\"clusterIP\":\"1.1.1.1\",\"ports\":[{\"port\":443}]}}' | kubectl apply -f - 2>&1 | sed 's/.*valid IPs is //') echo $SVCRANGE Enable Autocomplete source <(kubectl completion bash) Update the image for the deployment kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record View the rollout status kubectl rollout status deployment.v1.apps/nginx-deployment View Deployment History kubectl rollout history deployment nginx-deployment Rollback the deployment kubectl rollout undo deployments nginx-deployment Create config map from file kubectl create secret generic <name> \\ --from-file=key.json=$HOME/<file>.json","title":"Kubectl commands"},{"location":"certifications/google-certs/gcp-architect/kubectl-commands/#kubectl-commands","text":"","title":"Kubectl commands"},{"location":"certifications/google-certs/gcp-associate/compute-engine/","text":"Compute Engine HA Live migration and availability policy Your running instance is migrated to another host in the same zone Does not change any attributes Supports Local SSD's NOT supported for GPU;s It's configured under Availability policy Default is to migrate You can also configure terminate and automatic restart. Custom machine types You are able to create your own machine types and instance settings You are billed per vcpu and memory provisioned GPU You are able to add a GPU to your machines They are good for AI/ML use Should have GPU libraries Higher cost Not supported on all machine types Cant do live migration on GPU machines. Should be terminate instance. Important nat things to remember. VM's are created underProjects Machine types and avability differ from region to region You can only change a machine type after the instance has stopped VM's can be filtered on the page They are Zonal so only run in a specific zone Images are global and can be added to other projects if needs be Templates are global unless you use zonal resources Automated Basic Monitoring","title":"Compute Engine"},{"location":"certifications/google-certs/gcp-associate/compute-engine/#compute-engine","text":"","title":"Compute Engine"},{"location":"certifications/google-certs/gcp-associate/costs/","text":"GCP Costs When using the cloud you want to keep the costs as low as possible Sustained use discounts A sustained use discount is when you are using a machine for more than 25% of the month, you get 20-50% on every minute. The discount increases the longer you use the machine. No action required on your part. Important restrictions! Only applicable for GKE and Compute engine Not applied on A2 and E2 Not applied to App Engine Flexible and Dataflow Committed Use discounts For predictable workloads Committed to 1-3 years Up to 70% discount dependent on machine type and GPU's Pre-emptible vm's Short lived cheaper instances Can be stopped by GCP at any time. Maximum time you can run it is around 24 hours You get a 30 second warning before You use a pre-emptible VM if your application is fault tolerant Workload is cost sensative workload isn't immediate No SLA Cant be migrated to a regular VM Free Tier credits dont apply Billing You are billed by the second after a minimum of one minute You will want to create billing alerts under Billing > budget You need to create a budget then an alert","title":"Costs"},{"location":"certifications/google-certs/gcp-associate/costs/#gcp-costs","text":"When using the cloud you want to keep the costs as low as possible","title":"GCP Costs"},{"location":"certifications/google-certs/gcp-associate/gce-google-compute-engine/","text":"Google Compute Engine In datacenters, applications are deployed to Physical machines, where as in the cloud we provision a virtual server. We use Google compute engine which allows us to manage the lifecycle of VM's Allows you to manage Load balancing and Auto scaling for them. Allows you to also manage the network connectivity For the example we will create instances and an LB Creating a machine is simple, pick your region, size and if you want the simple firewall rules The Instance names are built logically: e2-standard-2 is: e2 : Machine family standard : type of workload 2 : Number of CPU's Memory disk and networking increase with vcpu increase There are many images on GCP. We have access to many public images We are also able to make custom images","title":"Google Compute engine"},{"location":"certifications/google-certs/gcp-associate/gce-google-compute-engine/#google-compute-engine","text":"In datacenters, applications are deployed to Physical machines, where as in the cloud we provision a virtual server. We use Google compute engine which allows us to manage the lifecycle of VM's Allows you to manage Load balancing and Auto scaling for them. Allows you to also manage the network connectivity For the example we will create instances and an LB Creating a machine is simple, pick your region, size and if you want the simple firewall rules The Instance names are built logically: e2-standard-2 is: e2 : Machine family standard : type of workload 2 : Number of CPU's Memory disk and networking increase with vcpu increase There are many images on GCP. We have access to many public images We are also able to make custom images","title":"Google Compute Engine"},{"location":"certifications/google-certs/gcp-associate/gcp/","text":"GCP https://docs.google.com/document/d/1u6pXBiGMYj7ZLBN21x6jap11rG6gWk7n210hNnUzrkI/edit https://medium.com/@sathishvj/writing-and-passing-the-google-cloud-associate-engineer-certification-a60c2f6d99c2 https://github.com/sathishvj/awesome-gcp-certifications 1. Setting up a cloud solution environment 1.1 Setting up cloud projects and accounts. Activities include: Creating projects Assigning users to predefined IAM roles within a project Managing users in Cloud Identity (manually and automated) Enabling APIs within projects Provisioning one or more Stackdriver workspaces 1.2 Managing billing configuration. Activities include: Creating one or more billing accounts Linking projects to a billing account Establishing billing budgets and alerts Setting up billing exports to estimate daily/monthly charges 1.3 Installing and configuring the command line interface (CLI), specifically the Cloud SDK (e.g., setting the default project). 2. Planning and configuring a cloud solution 2.1 Planning and estimating GCP product use using the Pricing Calculator 2.2 Planning and configuring compute resources. Considerations include: Selecting appropriate compute choices for a given workload (e.g., Compute Engine, Google Kubernetes Engine, App Engine, Cloud Run, Cloud Functions) Using preemptible VMs and custom machine types as appropriate 2.3 Planning and configuring data storage options. Considerations include: Product choice (e.g., Cloud SQL, BigQuery, Cloud Spanner, Cloud Bigtable) Choosing storage options (e.g., Standard, Nearline, Coldline, Archive) 2.4 Planning and configuring network resources. Tasks include: Differentiating load balancing options Identifying resource locations in a network for availability Configuring Cloud DNS 3. Deploying and implementing a cloud solution 3.1 Deploying and implementing Compute Engine resources. Tasks include: Launching a compute instance using Cloud Console and Cloud SDK (gcloud) (e.g., assign disks, availability policy, SSH keys) Creating an autoscaled managed instance group using an instance template Generating/uploading a custom SSH key for instances Configuring a VM for Stackdriver monitoring and logging Assessing compute quotas and requesting increases Installing the Stackdriver Agent for monitoring and logging 3.2 Deploying and implementing Google Kubernetes Engine resources. Tasks include: Deploying a Google Kubernetes Engine cluster Deploying a container application to Google Kubernetes Engine using pods Configuring Google Kubernetes Engine application monitoring and logging 3.3 Deploying and implementing App Engine, Cloud Run, and Cloud Functions resources. Tasks include, where applicable: Deploying an application, updating scaling configuration, versions, and traffic splitting Deploying an application that receives Google Cloud events (e.g., Cloud Pub/Sub events, Cloud Storage object change notification events) 3.4 Deploying and implementing data solutions. Tasks include: Initializing data systems with products (e.g., Cloud SQL, Cloud Datastore, BigQuery, Cloud Spanner, Cloud Pub/Sub, Cloud Bigtable, Cloud Dataproc, Cloud Dataflow, Cloud Storage) Loading data (e.g., command line upload, API transfer, import/export, load data from Cloud Storage, streaming data to Cloud Pub/Sub) 3.5 Deploying and implementing networking resources. Tasks include: Creating a VPC with subnets (e.g., custom-mode VPC, shared VPC) Launching a Compute Engine instance with custom network configuration (e.g., internal-only IP address, Google private access, static external and private IP address, network tags) Creating ingress and egress firewall rules for a VPC (e.g., IP subnets, tags, service accounts) Creating a VPN between a Google VPC and an external network using Cloud VPN Creating a load balancer to distribute application network traffic to an application (e.g., Global HTTP(S) load balancer, Global SSL Proxy load balancer, Global TCP Proxy load balancer, regional network load balancer, regional internal load balancer) 3.6 Deploying a solution using Cloud Marketplace. Tasks include: Browsing Cloud Marketplace catalog and viewing solution details Deploying a Cloud Marketplace solution 3.7 Deploying application infrastructure using Cloud Deployment Manager. Tasks include: Developing Deployment Manager templates Launching a Deployment Manager template 4. Ensuring successful operation of a cloud solution 4.1 Managing Compute Engine resources. Tasks include: Managing a single VM instance (e.g., start, stop, edit configuration, or delete an instance) SSH/RDP to the instance Attaching a GPU to a new instance and installing CUDA libraries Viewing current running VM inventory (instance IDs, details) Working with snapshots (e.g., create a snapshot from a VM, view snapshots, delete a snapshot) Working with images (e.g., create an image from a VM or a snapshot, view images, delete an image) Working with instance groups (e.g., set autoscaling parameters, assign instance template, create an instance template, remove instance group) Working with management interfaces (e.g., Cloud Console, Cloud Shell, GCloud SDK) 4.2 Managing Google Kubernetes Engine resources. Tasks include: Viewing current running cluster inventory (nodes, pods, services) Browsing the container image repository and viewing container image details Working with node pools (e.g., add, edit, or remove a node pool) Working with pods (e.g., add, edit, or remove pods) Working with services (e.g., add, edit, or remove a service) Working with stateful applications (e.g. persistent volumes, stateful sets) Working with management interfaces (e.g., Cloud Console, Cloud Shell, Cloud SDK) 4.3 Managing App Engine and Cloud Run resources. Tasks include: Adjusting application traffic splitting parameters Setting scaling parameters for autoscaling instances Working with management interfaces (e.g., Cloud Console, Cloud Shell, Cloud SDK) 4.4 Managing storage and database solutions. Tasks include: Moving objects between Cloud Storage buckets Converting Cloud Storage buckets between storage classes Setting object life cycle management policies for Cloud Storage buckets Executing queries to retrieve data from data instances (e.g., Cloud SQL, BigQuery, Cloud Spanner, Cloud Datastore, Cloud Bigtable) Estimating costs of a BigQuery query Backing up and restoring data instances (e.g., Cloud SQL, Cloud Datastore) Reviewing job status in Cloud Dataproc, Cloud Dataflow, or BigQuery Working with management interfaces (e.g., Cloud Console, Cloud Shell, Cloud SDK) 4.5 Managing networking resources. Tasks include: Adding a subnet to an existing VPC Expanding a subnet to have more IP addresses Reserving static external or internal IP addresses Working with management interfaces (e.g., Cloud Console, Cloud Shell, Cloud SDK) 4.6 Monitoring and logging. Tasks include: Creating Stackdriver alerts based on resource metrics Creating Stackdriver custom metrics Configuring log sinks to export logs to external systems (e.g., on-premises or BigQuery) Viewing and filtering logs in Stackdriver Viewing specific log message details in Stackdriver Using cloud diagnostics to research an application issue (e.g., viewing Cloud Trace data, using Cloud Debug to view an application point-in-time) Viewing Google Cloud Platform status Working with management interfaces (e.g., Cloud Console, Cloud Shell, Cloud SDK) 5. Configuring access and security 5.1 Managing identity and access management (IAM). Tasks include: Viewing IAM role assignments Assigning IAM roles to accounts or Google Groups Defining custom IAM roles 5.2 Managing service accounts. Tasks include: Managing service accounts with limited privileges Assigning a service account to VM instances Granting access to a service account in another project 5.3 Viewing audit logs for project and managed services.","title":"Associate cert exam"},{"location":"certifications/google-certs/gcp-associate/gcp/#gcp","text":"","title":"GCP"},{"location":"certifications/google-certs/gcp-associate/networking-and-lb/","text":"Networking and LB You get an internal network, and an external. You cant have 2 resources with the same IP address However you can have separate resources with the same IP address if they are internal You need at least one IP address per VM, internal or External To get a static IP you can assign a static IP to the VM, but you will want to use a Load balancer You are then able to reserve an IP address and then move it between multiple VM's The IP address stays attached even once you stop the instance. You are billed for a resource when you are even not using one","title":"Networking and LB"},{"location":"certifications/google-certs/gcp-associate/networking-and-lb/#networking-and-lb","text":"You get an internal network, and an external. You cant have 2 resources with the same IP address However you can have separate resources with the same IP address if they are internal You need at least one IP address per VM, internal or External To get a static IP you can assign a static IP to the VM, but you will want to use a Load balancer You are then able to reserve an IP address and then move it between multiple VM's The IP address stays attached even once you stop the instance. You are billed for a resource when you are even not using one","title":"Networking and LB"},{"location":"certifications/google-certs/gcp-associate/regions-and-zones/","text":"Regions and Zones Only deploying in to one DC can cause latency and issues, should your DC burn down, you lose the application. Secondly it can increase latency. You will want to deploy to many regions as it allows you to have minimal latency, allows for global footprint and adhere to gov regulations. In each zone, there are Discreet clusters, so many datacenters. Each region has 3 zones. Example would be: us-west1 which splits in to us-west1-a us-west1-b us-west1-c The cloud gives you: The ability to trade capital expense for Variable expense (PAYG opposed to purchase servers upfront) Benifits from massive economics of scale Stop guessing capacity (Ability to scale up and down easily)","title":"Regions and Zones"},{"location":"certifications/google-certs/gcp-associate/regions-and-zones/#regions-and-zones","text":"Only deploying in to one DC can cause latency and issues, should your DC burn down, you lose the application. Secondly it can increase latency. You will want to deploy to many regions as it allows you to have minimal latency, allows for global footprint and adhere to gov regulations. In each zone, there are Discreet clusters, so many datacenters. Each region has 3 zones. Example would be: us-west1 which splits in to us-west1-a us-west1-b us-west1-c The cloud gives you: The ability to trade capital expense for Variable expense (PAYG opposed to purchase servers upfront) Benifits from massive economics of scale Stop guessing capacity (Ability to scale up and down easily)","title":"Regions and Zones"},{"location":"certifications/google-certs/gcp-associate/vm-bootstrapping-templates-and-images/","text":"VM Bootstrapping, templates and Images You can bootstrap an instance with a startup script You are then also able to create templates for configuring your specs With templates, you can use it to create VM Instances as well as Managed Instance groups. You dont have to specify an OS, you can create one with a family and it will chose the most up to date one. You should aim to use templates and only edit things when needed. Now we are able to cut down the time of boot by creating an image. Images can be shared across projects and you're able to deprecate images after a while. You can then tell users to use a new recommended one - This allows you to push security standards to all images that users may deploy. Best to use a custom image opposed to a startup script. To create an image, you go to compute engine > disks > actions > create image You will want to stop the machine when creating an image. An image is basically just a snapshot you can deploy.","title":"VM Bootstrapping, templates and Images"},{"location":"certifications/google-certs/gcp-associate/vm-bootstrapping-templates-and-images/#vm-bootstrapping-templates-and-images","text":"You can bootstrap an instance with a startup script You are then also able to create templates for configuring your specs With templates, you can use it to create VM Instances as well as Managed Instance groups. You dont have to specify an OS, you can create one with a family and it will chose the most up to date one. You should aim to use templates and only edit things when needed. Now we are able to cut down the time of boot by creating an image. Images can be shared across projects and you're able to deprecate images after a while. You can then tell users to use a new recommended one - This allows you to push security standards to all images that users may deploy. Best to use a custom image opposed to a startup script. To create an image, you go to compute engine > disks > actions > create image You will want to stop the machine when creating an image. An image is basically just a snapshot you can deploy.","title":"VM Bootstrapping, templates and Images"},{"location":"certifications/google-certs/gcp-security/1-the-exam-topic/","text":"GCP Security, exam topics Section 1. Configuring access within a cloud solution environment 1.1 Configuring Cloud Identity. Considerations include: a. Managing Cloud Identity b. Configuring Google Cloud Directory Sync c. Managing super administrator account d. Automating user lifecycle management process e. Administering user accounts and groups programmatically 1.2 Managing service accounts. Considerations include: a. Protecting and auditing service accounts and keys b. Automating the rotation of user-managed service account keys c. Identifying scenarios requiring service accounts d. Creating, authorizing, and securing service accounts e. Securely managing API access management f. Managing and creating short-lived credentials 1.3 Managing authentication. Considerations include: a. Creating a password policy for user accounts b. Establishing Security Assertion Markup Language (SAML) c. Configuring and enforcing two-factor authentication 1.4 Managing and implementing authorization controls. Considerations include: a. Managing privileged roles and separation of duties b. Managing IAM permissions with basic, predefined, and custom roles c. Granting permissions to different types of identities d. Understanding difference between Cloud Storage IAM and ACLs e. Designing identity roles at the organization, folder, project, and resource level f. Configuring Access Context Manager 1.5 Defining resource hierarchy. Considerations include: a. Creating and managing organizations b. Designing resource policies for organizations, folders, projects, and resources c. Managing organization constraints d. Using resource hierarchy for access control and permissions inheritance e. Designing and managing trust and security boundaries within Google Cloud projects Section 2. Configuring network security 2.1 Designing network security. Considerations include: a. Configuring network perimeter controls (firewall rules; Identity-Aware Proxy (IAP)) b. Configuring load balancing (global, network, HTTP(S), SSL proxy, and TCP proxy load balancers) c. Identifying Domain Name System Security Extensions (DNSSEC) d. Identifying differences between private versus public addressing e. Configuring web application firewall (Google Cloud Armor) f. Configuring Cloud DNS 2.2 Configuring network segmentation. Considerations include: a. Configuring security properties of a VPC network, VPC peering, Shared VPC, and firewall rules b. Configuring network isolation and data encapsulation for N tier application design c. Configuring app-to-app security policy 2.3 Establishing private connectivity. Considerations include: a. Designing and configuring private RFC1918 connectivity between VPC networks and Google Cloud projects (Shared VPC, VPC peering) b. Designing and configuring private RFC1918 connectivity between data centers and VPC network (IPsec and Cloud Interconnect) c. Establishing private connectivity between VPC and Google APIs (Private Google Access, Private Google Access for on-premises hosts, Private Service Connect) d. Configuring Cloud NAT Section 3. Ensuring data protection 3.1 Protecting sensitive data. Considerations include: a. Inspecting and redacting personally identifiable information (PII) b. Configuring pseudonymization c. Configuring format-preserving substitution d. Restricting access to BigQuery datasets e. Configuring VPC Service Controls f. Securing secrets with Secret Manager g. Protecting and managing compute instance metadata 3.2 Managing encryption at rest. Considerations include: a. Understanding use cases for Google default encryption, customer-managed encryption keys (CMEK), customer-supplied encryption keys (CSEK), Cloud External Key Manager (EKM), and Cloud HSM b. Creating and managing encryption keys for CMEK, CSEK, and EKM c. Applying Google's encryption approach to use cases d. Configuring object lifecycle policies for Cloud Storage e. Enabling confidential computing Section 4. Managing operations in a cloud solution environment 4.1 Building and deploying secure infrastructure and applications. Considerations include: a. Automating security scanning for Common Vulnerabilities and Exposures (CVEs) through a CI/CD pipeline b. Automating virtual machine image creation, hardening, and maintenance c. Automating container image creation, verification, hardening, maintenance, and patch management 4.2 Configuring logging, monitoring, and detection. Considerations include: a. Configuring and analyzing network logs (firewall rule logs, VPC flow logs, packet mirroring) b. Designing an effective logging strategy c. Logging, monitoring, responding to, and remediating security incidents d. Exporting logs to external security systems e. Configuring and analyzing Google Cloud audit logs and data access logs f. Configuring log exports (log sinks, aggregated sinks, logs router) g. Configuring and monitoring Security Command Center (Security Health Analytics, Event Threat Detection, Container Threat Detection, Web Security Scanner) Section 5. Ensuring compliance 5.1 Determining regulatory requirements for the cloud. Considerations include: a. Determining concerns relative to compute, data, and network b. Evaluating security shared responsibility model c. Configuring security controls within cloud environments d. Limiting compute and data for regulatory compliance e. Determining the Google Cloud environment in scope for regulatory compliance","title":"Google Security - Exam topics"},{"location":"certifications/google-certs/gcp-security/1-the-exam-topic/#gcp-security-exam-topics","text":"","title":"GCP Security, exam topics"},{"location":"certifications/google-certs/gcp-security/gcp-security-1/","text":"GCP Security, page 1 Google cloud Professional Security Engineer The exam itself, is a multiple choice 2 hour test. Regions and Zones When we architect our application we need to deploy our application closes to our users, as well as spanning multiple zones. Google cloud is split in to Regions and Zones Region A geographical location that the DC sits in Example being europe-west2 Zone A specific Data Centre in the Region Example being: europe-west2-c Some applications (Like the ones I am working on currently) - We are not allowed to store Date outside of the border of the United Kingdom. Whilst google don't release information about their exact locations, we are able to make a good guess it's in Slough These data centers are connected via High speed connections (Fibre cables) GCP Services Encryption Allows Google Managed keys Allows uploading your own keys VPC Secure private cloud VPC Peering Peers VPC's between projects and accounts Routes traffic VPC Sharing Allows projects to put resources on a shared VPC Hybrid Connectivity Secure private connection between your premise and the cloud Data Loss Detect sensitive Data and scrub it Security Command Centre Allows us to view all security issues Binary Authorization Allows only certain docker images to run Web Security Scanner Scans internal web apps for security vulnerabilities IAM & Admin IAM Identity and Access Management Who can do what, when and with what Security at google and how it helps What google does to secure your app/ data How google does all these Security Mechanisms at different layers Shared Responsibility model Tools provided by GCP Regulatory Compliance Why trust google They have over 7 billion users Security is a main concern for them Your applicaiton is deployed in the same infrastructure as Google.com and google workspaces applications They have dedicated engineers 24/7 working for security How google Secure their infrastructure Hardware layer Less than 1% of google employees have access to the Data Centre Google builds all their own hardware Routers, switches, etc IAM Identity and access management Allows users to do some stuff and blocks them to do others User Management Google Account authentication SMAL support Enforce user rules 2fa Minimum password Storage data Encrypts all your data Keys: Google managed by default Customer Supplied Customer Managed IAP Identity Aware Proxy Secures Applications via google login Built in DDOS prevention DLP Inspects Data Can be configured to Find and Redact Transform data Can be used to re-identify it VPC Firewall rules Cloud Armour Ingress/ Egress rules Operations Logging Monitoring Tracing Profiling Regulatory Compliance Encryption, hardware, VPC firewall is technical aspect of security Compliance is another important factor Google cloud follows these standards Shared Responsibility Model Google is responsible for securing some aspects of your deployment, like the underlying hardware/ infra, where as you need to ensure that your application is patched (At a bare minimum) The above chart shows who plays more roles around where, and by whom We can see that for On-prem, it's entirely your problem Where as when we go to Software As A Service (SaaS) - All the underlying infrastructure, usage, deployment etc is the providers problem. Cloud Identity There are 5 'Cloud Identities': Google Account Service Account Google workspace Cloud Identity Domain Google Groups The above are then able to interact with Google Cloud Platform (GCP) In every single identity, they are similar to Email address' Cloud Identity is a Managed service that manages users. Cloud Identity - Google Workspace This is the renames product previously called G-Suit Similar to Office 365 Slides Sheets Docs Etc You can get a verified domain Domain like breadnet.co.uk - Verify ownership You get complete user management for all the users under the domain Free 15 days trial, then $6 per user per month Access via admin.google.com If you already have Google workspace, then it automatically verifies your google cloud domain Sections have been skipped on Udemy as knowledge is pre-existing Admin Console The below section is about the google cloud admin console Resource Hierarchy At the top you have the Organization Below the ORG you can have folders Below folders you have projects At each level you are able to set IAM and policies You can nest folders up to 10 ( ten ) levels deep. A parent folder cannot contain more than 300 folders. This refers to direct child folders only. Those child folders can, in turn, contain additional folders or projects. Folder display names must be unique within the same level of the hierarchy. Organization Policies For arguments sake, we will look at how to enable/ disable 3 policies: Disable Service Account Creation Enforce Uniform bucket level access Skip Default network creation How to Enable IAM > Organization Policies > Disable Service account creation Here we are able to define what level the policy applies at. How to disable on a project when it's inheriting If you need to remove something that is inherited, you select Customize > Enforcement > Off IAM Intro IAM stands for Identity and Access management Defines: Who can do what and on which Who: Identity Member Email What Roles Collections of permissions Which Resources: Compute Engine app engine Bigquery etc Roles There are 3 types of roles in Google Cloud Platform Roles are defined as a Collection of Permissions Primitive These are super simple roles, and should usually be avoided as best as possible Owner Editor Viewer Pre-defined These roles are roles on a single service Examples: Compute admin Network Viewer BQ Job User Custom roles These are roles you make for your org or project based on specifics from predefined roles, They usually follow the layout like service.resource-type.verb","title":"Google Security - Page 1"},{"location":"certifications/google-certs/gcp-security/gcp-security-1/#gcp-security-page-1","text":"","title":"GCP Security, page 1"},{"location":"certifications/google-certs/workspace-admin/managing-gsuit/","text":"Managing Gsuit Enabling services for everyone You are able to enable and disable services for everyone under the apps section. This can be configured per OU This is much like AD where permissions and status is inherited from the top level. To enable/ disable a service you select the OU and then disable it there If only one user requires access to a service, you should add them to a group. You will want to create that group to have access:restricted then under the service you will need to search for it: When creating groups: It can and will take up to 24 hours Groups cant be used to deny access, the order of presidence is OU then group Access groups can contain users from any OU Ensure you use a naming practice like ag.<name> for groups (where ag is access group) Service Release Track Google has 2 release tracks you can subscribe to: Rapid release This is the track where releases are rolled out as soon as they are released to the public. These products have gone through the same QC as other apps but your users will see them at the same time you do. Scheduled release (Recommended and enabled by default) this gives you extra time to train your users on things that have been released. Releases are made only on Tuesdays at least a week after the rapid release For most gsuit orgs, you want the scheduled release track for stability. (see side bar for calendar) To edit this, it's a global option so will be under profile for the company You can set the track for both features and products Configure Common User Settings You are able to set (as usual) settings for Gmail on a OU based structure where you can disable and enable features. This would include features like: Confidential mode Gmail offline Mail delegation Smart compiose As usual these things can be controlled at an OU level and can have over-rides You can enable Gmail labs which has experimental features: Enable G Suite Sync for Outlook You are able to disable and enable Synch for Outlook per OU This can be found under 'End user access' and can be applied per OU Compliance Under Advanced settings you have the ability to create compliance settings for the users. This can be set per OU: Auto deletion OCR Archiving Appending footer Restricting domains Attachment compliance TLS MTS-STS Email routing Here you can set rules on what to do with emails that contain certain files. You can: Modify Add headers Add subject Change route Change Recipient Bypass spam filter Remove attachments Deliver to more people Encrypt Reject Quarantine From the gmail sergice settings page you cant set default language. This is set globally Users set: Display language for their inbox pop3 accounts Sharing calendars You will want to ensure that only free and busy info is enabled for external users. To manage sharing you will need to edit the calendar from your account. Invite people from there and then set their permissions from that tab. As as admin in calendar control you can set: Highest level of sharing for externals Default level of internal sharing Google Drive You will want to set the best sharing permissions for Drive. As usual this can be done at an OU level. You can set an option where you can share externally, but only with whitelisted domains. This is important if you run a secretive business. These options override all options for shared drives. You have the ability to transfer ownership of all documents. This is under settings for Drive and docs > transfer ownership. This can also be done under the drive API Restore Deleted Files Google keeps files for 25 days, in which you're able to restore them. Go to the user's tab and select 'Restore data' then pick the date window and then the Application. Once done the files will appear again. Offline access to documents Users can be granted access to 'Offline access to documents' via OU or group as usual. This can be found under Apps > google workspace > Settings for drive and docs > Feature and Applications > offline Enable this for users under an OU or group. You will require google chrome and the offline sync plugin for this to work Stream or Sync files There are 2 ways you can access files in drive: Drive file stream This streams the files from the cloud to your computer Backup and Sync This is the software you've installed for google backup This is enabled under Features and Applications Things to note: You can allow file stream and backup for users. If they are running a backup whilst streaming it will ask them to stop. If you enable download links for both, then only file stream link is allowed Shared Drive Creation Shared drive creation needs to be enabled from Sharing centre in Drive and Docs. Login as a user then create the shared drive. You then can add users to the shared drive from their user profile or by creating a group and adding them there Once created, you will want to remove access for users with full access to modify permissions","title":"Managing Gsuit"},{"location":"certifications/google-certs/workspace-admin/managing-gsuit/#managing-gsuit","text":"","title":"Managing Gsuit"},{"location":"certifications/google-certs/workspace-admin/mdm/","text":"MDM To enforce these policies on Android devices, your users must install the Google Apps Device Policy app on their device. This app ensures that your domain policies are set properly on the user's Android device before synchronizing any data. See Google Apps Device Policy overview for more details. If the app isn't already installed when the user adds their corporate account to their phone, the app is typically installed automatically as part of the sign up process. Adding a device requires that you install the google mobile device management application on your phoen","title":"Google MDM"},{"location":"certifications/google-certs/workspace-admin/mdm/#mdm","text":"To enforce these policies on Android devices, your users must install the Google Apps Device Policy app on their device. This app ensures that your domain policies are set properly on the user's Android device before synchronizing any data. See Google Apps Device Policy overview for more details. If the app isn't already installed when the user adds their corporate account to their phone, the app is typically installed automatically as part of the sign up process. Adding a device requires that you install the google mobile device management application on your phoen","title":"MDM"},{"location":"certifications/google-certs/workspace-admin/user-administration/","text":"Gsuit - User administration Global info can be set under `Account settings` but this has changed from `Company profile` Users Creating users can be done using: CSV upload Click and point SDK API Google cloud directory sync (GCDS) For the CSV upload it can take up to 24 hours for the users to appear as being searchable If you are uploading more than 500 users, you should split the files in to smaller lots of around 300 give or take users for concurrent processing --- Sync users with GSuit using GCDS You are able to sync gsuit with Microsoft active directory, or ldap server. it allows you to ensure that gsuit groups and contacts and users remain in sync Updates only flow up to google. Google does not make changes to a local directory Configuration files are stored as XML You need to download software on the microsoft server Under google domain config, you add the primary domain name, then select to replace users emails and domains If it fails: Ensure code is corerct No spaces Computer's time zone is correct You're able to limit users by adding an exclusion rule. Ldap is the single source of truth so exclude admin Chose what to sync. E Ensure that you select 'Dont suspend or delete google domain admins not found in ldap' You can now synch passwords with AD, but can use Gsuit password synch You can setup a search rule, but with AD you can select to use defaults. If your delete and suspend limits excheed you will get an error `sync -c file.xml` does a dry run `sync -c file.xma -a` runs a full sync --- Groups There are 2 types of groups: Admin console groups : Groups used from collaboration, communiation and administratiom Only managed by admins Groups for business : Groups for communication and collab (managed from groups.google.com) Managed by users and admins For a user to be able to email a group, you need to have the users who will want to email people set as a maanger: --- Licensing You must assign a license to a use, how ever if you use gsuit enterprise it's automatic Suspended users will not be able to login and will not get emails or calendar invites --- Organisational units You are able to assign users to an OU much like how you would in Active Directiory You need tos et the top level to off then maange per OU --- Directory overview Directories allow adding custom fields to users as well as a 'virtual card' about that user so people can see stuff like location etc This can also be managed per OU You can populate users profiles as well as create custom directories You can add external vendors as well This is used for users cards to help with booking meeting rooms You can populate info through: Cloud sync Google user management Admin SDK To set different access to directories you can use OU's: You can contain custom directories, create a google group, then add the group to the directories. Enable direcotry editing by going to Directory > directory settings Here you can enable things that people are able to edit. As usual you can assign this based on an OU system You should add work location so GCal can see where they are and suggest meeting rooms for users. If you add in the employee ID then you can use this as a login verification If you are creating a user that should not be able to see the whole directory, you will need to add them to an OU (create one if not already done) then a group, then your custom directory. First create a new OU Create the group for the OU Create the users Add the users to the group as a manager Go to the OU under Directory Settings > visibility settings Set it as `Users in a custom directory` then create new and name it then add a group to it: This is a change that takes up to 24 hours to process --- Shared contacts During business operations you will deal with external vendors frequently and can add them to your directory for all users that can see that directory, to be able to see details about them. Users have their own contacts that are not shared with the org. --- Admin roles: Admin roles are roles given to an admin that allows them to perform actions in the gsuit org. You can give a user roles to manage users or to the whole domain To assign an admin role you can select the user's account under admin.google.com then assign roles. You can not edit the pre-set roles, but you can create custom roles based off of the pre-defined options Super admin can do all operations Help desk admin can reset passwords for the users Things to note: The more administrators you have, the more it affects your account recovery options Ensure you trust the users you are giving access to admin and ensure 2fa is enforced Like previously mentioned you are able to create custom roles. To do so, go to the admin page then users then roles, then create new role: You from here can select what roles to assign to the role. You want to create more roles with less privileges to assign to a user than one fat rule.","title":"Gsuit User administration"},{"location":"certifications/google-certs/workspace-admin/user-administration/#gsuit-user-administration","text":"Global info can be set under `Account settings` but this has changed from `Company profile`","title":"Gsuit - User administration"},{"location":"cloud/aqua/aqua-page-1/","text":"Aqua - Page 1 Introduction Aqua has a market around the security of cloud deployments due to the lack of full stack control, down to restructuring and refactoring. Cluster Hygiene Image Hygiene Limit blast radius Attack prevention RBAC CIS MINIMAL OS Vulnerability can Approved base images Run only on trustes images Least privileges Service mesh encryption and auth Container Sandboxing Container Immutability Anomaly detection and prevention. The issue from the cloud shift is the lack of knowledge of what and where to secure. You need to be asking how you can continuously be checking security and where things can be secured. Different enviroments, same rules Risk mitigation, vulnerabilities and integrity Deployment authorization, visibilty, inventory Operation admin and change control Secret management Network segmentation of microservices SOC and audit-ibility and incident response How can aqua help? Cloud Native focus Opensource Built for enterprise scale Broad platform support Customer Partnership Module 2: Cloud Native Security building blocks What are the cloud native building blocks How to secure the build We want to be introducing the security at the build level, so building the security in to the deployment from the start. Ensuring that the underlaying infrastructure is secure Securing the workloads The Challenges: Securing builds Known vulnerabilities Malware Configuration Sensitive Data Non-approved software Securing infrastructure Security Configuration Vulnerabilities Missing Patches Access control RBAC Leas privileges possible Security Posture Management Malicious Activity Changes to the image running things that weren't meant to be accessed Strange network activity Intrusion Prevention Segmentation Containing the risk Damage control Limited exposure and the scope of the issue Drift prevention Make sure that run time builds are enforced and that containers are Immutable Event Auditing Securing the build Scans all build artifacts for vulnerabilities, secrets bad config, malware and permissions Prioritize issues based on application contextual risk Detects hidden malware and prevents supply chain attacks Flexible assurance policies to flag and block bad artifacts Application Assurance Integrate Aqua in the build process Applying assurance policies to stop builds of images that do not follow best practices Integrates with all CICD Vulnerability lifecycle management Determine the actions Aqua vShield (Requires additional license) Aqua vShield is a virtual patching mechanism Detects and prevents exploits of known vulnerabilities Does not change image code or require and dev intervention Acts as a compensating control Will temporarily apply a policy to that runtime, to prevent malicious code running Dynamic Image Inspection (Aqua DTA) License per scan Run containers in a sandbox env Identify container behavior at runtime Identify malicious behavior based on heuristics maintained by Aquas research team. You can do image profiling Image is marked as Non-compliant The run is done after the container is built, depending on the outcome, you can set a runtime protection that prevents the container from running. DTA will run the image for you, and observes what the image does (Runs on AQUAS infrastructure, no offline functionality, nor on client infrastructure) Securing the infrastructure Asses and audit the security Single pane of glass view across clouds (Including orcale) Get remediation advice or set automated remediation for out of policy service configurations K8, docker and CIS benchmarks Runs cheks aginst all 100_ cis tests Provides scored report Schedule to run daily CIS certified Answers the question: Is my infa secure Kube-hunter: Integrated k8s penetration testing tests clusters against real world attacks Get risk assessment Runs both in passive or active mode, as an external unauthorized user, or with in a pod as a authorized user Securing the workload Visualize and prioritize risks in the environment Wide array of purpose built enforcers Provides granular visibility Drift prevention 'Workloads firewall' micro-segment apps where they run Sounds similar to a service mesh Don't plan on replacing the mesh rules, just boosting them Enforcing Immutability Ensuring the workloads don't change Immutable workloads are easier to protect Any change in runtime are not legitamate If a change is detected, it's blocked Behavior whitelisting Aqua uses activity profiling to learn workload behaviour during testing Whitelist only capabilities Files syscalls network connections Executable Network Firewall Apply firewall rules for contextual application micro-segmentation Rules based on Service: DNS Identities URL's Ip address Reputation Alert on or block Risk Explorer Enforcer automatically records Serverless security Seamless attach aqua nano enforcer to serverless functions Provides visibility and prevention capabilities Performance optimized for Serverless: Minimal impact function on start time (2ms) and memory use (2mb) Language agnostic Opensource tools Trivy Scanning Manages vulnerability Detects vulnerabilities in Cloudsploit Enforces cloud compliance Suports aws azure gcp oracle and github Ectensive plugin Restful ASPI Kube-bench Securely configure your k8 cluster Auto-detects per node Customisable though yaml Kube-hunter test your k8 setup Penetration testing that simulates dozens of attack vendors tracee Runtime security eBPF technology Trace your system and applications at run time Module 3 Architecture Aqua sec is made up of 3 items Server, UI, Console Database Gateways The server and the gateways live close together Database should be on a cloud database, customer managed The database is the most important part This can deal with up to 20k hosts on this solution This can run on K8, docker Database and Cybrecentre Don't compromise the database All the configuration is stored in the database Policies, user roles are stored here Needs to support postgesql (Cloud sql supports) Aqua Cybercentre Can be used locally, but on a daily basis it's updated so needs to be pulled Runs on cloud Possible to run on an air-gapped infrastructure Gateway All the aqua enforcers connect to the gateway Handles communications between the aqua server Must be a minimum of one Communicates over grpc (https) Handles all interactions Can go through envoy proxies Can load balance GRPC connections When doing an update it terminates at envoy so no disruption Uses a headless service Tennant manager is for avoiding cross region data tranfers Aqua Scanners Primary foundation of the aqua scanner is to scan the follwing objects: Container images Cloud Foundry 2 main roles: Object scanning is supported by the cyber centre, who maintain data on vulnerabilities Secondary function include registering the container image All calls are done via API's What it can scan: Registries Images that hace been pushed to public and private regs Local host Scan images that are created locally before being pushed xRunning workload images Images used previously as well as running workloads As soon as you deploy the platform, there is a scanner that is deployed in the web server, but the second the scanner is installed then the internal one is disabled You are also able to scan other files using the scanner, running as a binary. Enforcers Provide run time security Enforce runtime Ensure only compliant images are schedules Can be installed on both Linux and Windows Micro enforcer: Installed within in the Image Can be embedded in the image that you want to protect Only supported on Linux Kube Enforcer Runs as an admission controller and deployed as a pod on a single node in a cluster Examples of security: Block image found to be non-compliant Unrestricted images OPA custom checks Additional License Pod Enforcer Enforcer that dynamically injects a sidecar in to the k8 pod Enforces a few basic runtime policies like drift prevention More limited functionality Important: Aqua pod enforcer is supported only on linux VM Enforcers Provides enforcement for hosts Ensure: Host assurance policies Host runtime policies Firewall policies Important notes: Supported only on Linux It's required that you deploy vm enforcer on each host Vm enforcers require a separate license Nano enforcer Nano enforcer is a dedicated bianty for AWS Lambda functions Runtimes policies provide runtime protection for AWS Lambda functions Things to note: Aqua Node-enforcer is supported on Linux platforms only Enforcers: There are groups Enforcer group should only be used to group by: Type Scope Profile Can be used to filter traffic, what to capture data about and what to ignore Module 4: Installation and Configuration Checking Release notes Check the release notes They are not following version releases properly They support backwards 2 major releases If you're in 5.3, and want 6.0, it wont work very well Major releases are quarterly Methods of deployment Helm Docker Manifests Openshift Operator Aquactl The general deployment flow Create namepsace create registry configure rbac configure aqua secrets create cm configure htt[s are you using package bd Yes create db configure db deploy main package No Deploy with main components Expose with envoy Deploy aqua Always pin it to a version Scanner User The scanner needs to be created as a user, you will need to create a user as it connects to the API Image Assurance Assurance = Before running Basics of image assurnace What image scanning is avalible What are image assurance policihes Use case of image assurance Detects, asses and reports security in the images. Scanning can be done locally Image assurance Activity Scanning images Evaluation of images Reporting info in the UI Risk Managment Scanning CICD Image Hosts Image scanning as well as artifacts Image scanning Details Can check if you've left a container with a key in it. Checks for Certificate files Image Assurance Policy Default policy Always present, cant be deleted Custom policy Vulnerability Scoring You can pick a slider or a set value for the severability on how bad you will allow to run in the container Module 6: Runtime policy What are the policies Container Host Function What makes up a policy Policy components Application scope Set of containers Set of hosts Set of clusters Status: Enabled or disabled Enforcment mode: How it handles the container and violations Controls Security related policies Default Policy Cant be deleted Always present If you want sometihng not to apply in a scope, you can use bypass control Custom Policy Created by Aqua admins for specific scopes These policies can be used to prevent containers from running They are only applied when using an \"Enforcer\" Compliance CIS HIPAA NIST PCI DSS Commonly used container contorls Bypass scope Block volumes Drift prevention Block unregistered images Block non-compliant images","title":"Aqua Page 1"},{"location":"cloud/aqua/aqua-page-1/#aqua-page-1","text":"","title":"Aqua - Page 1"},{"location":"cloud/aqua/aqua-page-2/","text":"Aqua - Page 2 Module 7 Services Comprises of the following Enforcment mode: How aqua handles the policy Firewall Logical groups if workloads Defined by scope of service main purpose is to apply one or more firewall policies Policies contain rules Can add rules in 2 ways Add firewall rules though the UI View during running then lock down Module 8: Secrets How to integrate with existing key store How to inject it in to the workload Central management Automatically injects in to running container Secrets are encrypted in transit Secrets need to be added as an env variable, image needs tags spec: containers: - env: - name: bradley value: '{aqua.bradley}' Module 9: Serverless GCP is supported when scanning before run Only AWS supports runtime protection How it protects: Secures serverless functions at run time Runtime assurance Nano-enforcer allows Aqua to carry out runtime protection Does not work for GCP Module 11: Aqua-api and aquacltl Aquactl can be used to inject nano-enforcers as well as Download Manifests Module 12: Aqua for dev Before building, understand the use case Evaluation: Make use of opensource tools Can you use the API Validation Check audit/ci check running containers Check risk explorer to get high level overviews Module 13: Aqua for Admin Upgrading Database and vm enforcers are required for the same version backwards compatible : Enforcers and vm enforcers up to 2 versions Micro enforcers and kube enforcers: 1 Major version","title":"Aqua Page 2"},{"location":"cloud/aqua/aqua-page-2/#aqua-page-2","text":"","title":"Aqua - Page 2"},{"location":"cloud/aqua/partnership-info/","text":"Aqua partnership info Worst kind of partners is those who throw things from customer to partner to company as it's bad communication. We are treated as a level 1 support team. Status What has been done What do you think it could be Couple of questions to narrow it down Data: Knowing the architecture Increase the logging across the architecture Understand what the flow is Support is for: Documentation KB updates Support","title":"Aqua partnership notes"},{"location":"cloud/aqua/partnership-info/#aqua-partnership-info","text":"Worst kind of partners is those who throw things from customer to partner to company as it's bad communication. We are treated as a level 1 support team. Status What has been done What do you think it could be Couple of questions to narrow it down Data: Knowing the architecture Increase the logging across the architecture Understand what the flow is Support is for: Documentation KB updates Support","title":"Aqua partnership info"},{"location":"cloud/digitalocean/digitalocean-get-list-of-images/","text":"Get list of Digtal Ocean images You will need to have the doctl command installed already Install doctl brew install doctl List Images doctl compute image list --public List Ubuntu Images doctl compute image list --public | grep ubuntu","title":"Get list of Digital ocean images"},{"location":"cloud/digitalocean/digitalocean-get-list-of-images/#get-list-of-digtal-ocean-images","text":"You will need to have the doctl command installed already","title":"Get list of Digtal Ocean images"},{"location":"cloud/digitalocean/digitalocean-get-list-of-images/#install-doctl","text":"brew install doctl","title":"Install doctl"},{"location":"cloud/digitalocean/digitalocean-get-list-of-images/#list-images","text":"doctl compute image list --public","title":"List Images"},{"location":"cloud/digitalocean/digitalocean-get-list-of-images/#list-ubuntu-images","text":"doctl compute image list --public | grep ubuntu","title":"List Ubuntu Images"},{"location":"cloud/gcp/curl-to-iap/","text":"Curl to IAP You will need to have a service account already, and the ability to impersonate that account. Your account will need to have the permission Service Account Token Creator OAUTH_CLIENT_ID = <> AUTHORIZED_SA = <> URL = https://<> ID_TOKEN = $( gcloud auth print-identity-token \\ --audiences $OAUTH_CLIENT_ID \\ --include-email \\ --impersonate-service-account $AUTHORIZED_SA ) curl --header \"Authorization: Bearer $ID_TOKEN \" $URL","title":"Curl to IAP"},{"location":"cloud/gcp/curl-to-iap/#curl-to-iap","text":"You will need to have a service account already, and the ability to impersonate that account. Your account will need to have the permission Service Account Token Creator OAUTH_CLIENT_ID = <> AUTHORIZED_SA = <> URL = https://<> ID_TOKEN = $( gcloud auth print-identity-token \\ --audiences $OAUTH_CLIENT_ID \\ --include-email \\ --impersonate-service-account $AUTHORIZED_SA ) curl --header \"Authorization: Bearer $ID_TOKEN \" $URL","title":"Curl to IAP"},{"location":"cloud/gcp/export-to-terraform-using-gcloud-cli/","text":"Export to terraform using gcloud cli Create a bucket if your GCP account has location constraints enabled Enable cloudasset.googleapis.com on GCP gcloud beta resource-config bulk-export --reseource-format = terraform --storage-path = 'gs://tf-eport-bucket-qwjhef' This will print out to stdout, which is useful and also not. We can append > file.tf so it's like gcloud beta resource-config bulk-export --resource-format = terraform --storage-path = 'gs://tf-eport-bucket-qwjhef' > file.tf There should now be a file called file.tf in your current directory. You can view it using cat file.tf If you are going to re-structure it, I suggest moving the file to a git repo, committing and pushing it, then making changes. Try and split out your resources in to new files","title":"Export to terraform using gcloud cli"},{"location":"cloud/gcp/export-to-terraform-using-gcloud-cli/#export-to-terraform-using-gcloud-cli","text":"Create a bucket if your GCP account has location constraints enabled Enable cloudasset.googleapis.com on GCP gcloud beta resource-config bulk-export --reseource-format = terraform --storage-path = 'gs://tf-eport-bucket-qwjhef' This will print out to stdout, which is useful and also not. We can append > file.tf so it's like gcloud beta resource-config bulk-export --resource-format = terraform --storage-path = 'gs://tf-eport-bucket-qwjhef' > file.tf There should now be a file called file.tf in your current directory. You can view it using cat file.tf If you are going to re-structure it, I suggest moving the file to a git repo, committing and pushing it, then making changes. Try and split out your resources in to new files","title":"Export to terraform using gcloud cli"},{"location":"cloud/gcp/grafeas/","text":"Grafeas Can store the metadata in a range of databases: PostgreSQL BoltDB Spanner OracleDB It defines an API for managing metadata for Software supply chain, including (but not limited to) Container images VM's .jar files scripts it works by taking the metadata (The additional details about the files and packages) and splitting it in to: Notes Notes are high level descriptions of a type of metadata Occurrences Occurrences are instantiations (noun. Countable) of the Notes, and where they happen. By doing this, it allows third party metadata providers to create and manage metadata. Allows for fine-grained access to the types of metadata Sources: https://opensource.google/projects/grafeas https://grafeas.io","title":"Grafeas"},{"location":"cloud/gcp/grafeas/#grafeas","text":"Can store the metadata in a range of databases: PostgreSQL BoltDB Spanner OracleDB It defines an API for managing metadata for Software supply chain, including (but not limited to) Container images VM's .jar files scripts it works by taking the metadata (The additional details about the files and packages) and splitting it in to: Notes Notes are high level descriptions of a type of metadata Occurrences Occurrences are instantiations (noun. Countable) of the Notes, and where they happen. By doing this, it allows third party metadata providers to create and manage metadata. Allows for fine-grained access to the types of metadata Sources: https://opensource.google/projects/grafeas https://grafeas.io","title":"Grafeas"},{"location":"cloud/terraform/building-infrastructure/","text":"Terraform - Building infrastructure Picking your region First we need to set which cloud provider we will be using. In this example we will be using GCP Start with defining the cloud privodor and then the version. Usually this is not needed! From google cloud console, you will need to get the project ID. This can be found on the dashboard of the project. The we will nee to define the region and zone. Today I will be using us-central1 and then the zone of c This becomes us-central1- c in the format of <country>-<area><dcnumber>-<a,b,c...> Below is a list of regions and zones. If you are scaling and need low latency, use the same zone as they are physically closer! (wow) Regions and Zones Region Zones Location asia-east1 a, b, c Changhua County, Taiwan asia-east2 a, b, c Hong Kong asia-northeast1 a, b, c Tokyo, Japan asia-northeast2 a, b, c Osaka, Japan asia-northeast3 a, b, c Seoul, South Korea asia-south1 a, b, c Mumbai, India asia-southeast1 a, b, c Jurong West, Singapore australia-southeast1 a, b, c Sydney, Australia europe-north1 a, b, c Hamina, Finland europe-west1 b, c, d St. Ghislain, Belgium europe-west2 a, b, c London, England, UK europe-west3 a, b, c Frankfurt, Germany europe-west4 a, b, c Eemshaven, Netherlands europe-west6 a, b, c Z\u00fcrich, Switzerland northamerica-northeast1 a, b, c Montr\u00e9al, Qu\u00e9bec, Canada southamerica-east1 a, b, c Osasco (S\u00e3o Paulo), Brazil us-central1 a, b, c, f Council Bluffs, Iowa, USA us-east1 b, c, d Moncks Corner, South Carolina, USA us-east4 a, b, c Ashburn, Northern Virginia, USA us-west1 a, b, c The Dalles, Oregon, USA us-west2 a, b, c Los Angeles, California, USA us-west3 a, b, c Salt Lake City, Utah, USA Networking Next we will be creating a network in the us-central1-c zone called google_compute_network.vpc_network and giving it a name of terraform-network For this, we will define a a resource. resource \"google_compute_network\" \"vpc_network\"{ name = \"terraform-network\" } Depending on the need, you can assign the vm's a static ip by defining another resource, this time calling it by the google_compute_address resource \"google_compute_address\" \"vm_static_ip\" { name = \"terraform-static-ip\" } Creating the Instance As always, when creating something in terraform, we need to define it as a resource. In this case we will use the google_compute_instance.vm_instance which tells google, bro, spin me up a vm Just like everything else, we need to give it a name. This not only helps with admin tasks, but we can call it by name for later tasks! Then we need to pick what machine type to use! I am going to be using these found here We can add tags. This is useful for sorting machines by production, pre deployment and dev. Next thing we will need to do is add the boot disk. We can do this by creating a boot_disk In here we define the type of disk we want. Here we have gone for a container optimized server boot_disk { initialize_params{ image = \"cos-cloud/cos-stable\" } } More networking Finally we add the IP address and allow nat: network_interface { network = google_compute_network.vpc_network.name access_config { nat_ip = google_compute_address.vm_static_ip.address } } Provision er is a tool we can use to get details of the vm we created. Here we will get the public ip address and print it to a file in the folder we are in provisioner \"local-exec\" { command = \"echo ${google_compute_instance.vm_instance.name}:${google_compute_instance.vm_instance.network_interface[0].access_config[0].nat_ip} >> ip_address.txt\" } provider \"google\" { version = \"3.5.0\" credentials = file(\"terraform-c8b2b88693d4.json\") project = \"absolute-access-271419\" region = \"us-central1\" zone = \"us-central1-c\" } resource \"google_compute_network\" \"vpc_network\"{ name = \"terraform-network\" } resource \"google_compute_address\" \"vm_static_ip\" { name = \"terraform-static-ip\" } resource \"google_compute_instance\" \"vm_instance\" { name = \"terraform-instance\" machine_type = \"f1-micro\" tags = [\"test\",\"firsttry\"] provisioner \"local-exec\" { command = \"echo ${google_compute_instance.vm_instance.name}:${google_compute_instance.vm_instance.network_interface[0].access_config[0].nat_ip} >> ip_address.txt\" } boot_disk { initialize_params{ image = \"cos-cloud/cos-stable\" } } network_interface { network = google_compute_network.vpc_network.name access_config { nat_ip = google_compute_address.vm_static_ip.address } } }","title":"Building Infrastructure in terraform"},{"location":"cloud/terraform/building-infrastructure/#terraform-building-infrastructure","text":"","title":"Terraform - Building infrastructure"},{"location":"cloud/terraform/google-iap/","text":"Google IAP Get the brands available gcloud alpha iap oauth-brands list --project = <> Create terraform resource \"google_iap_brand\" \"thing\" { support_email = \"bradley@breadnet.co.uk\" application_title = \"Cloud IAP protected Application\" project = var.project } Import terraform import google_iap_brand.thing <name from previous command> Resources Issue 6704","title":"Google IAP"},{"location":"cloud/terraform/google-iap/#google-iap","text":"","title":"Google IAP"},{"location":"cloud/terraform/google-iap/#get-the-brands-available","text":"gcloud alpha iap oauth-brands list --project = <>","title":"Get the brands available"},{"location":"cloud/terraform/google-iap/#create-terraform","text":"resource \"google_iap_brand\" \"thing\" { support_email = \"bradley@breadnet.co.uk\" application_title = \"Cloud IAP protected Application\" project = var.project }","title":"Create terraform"},{"location":"cloud/terraform/google-iap/#import","text":"terraform import google_iap_brand.thing <name from previous command>","title":"Import"},{"location":"cloud/terraform/google-iap/#resources","text":"Issue 6704","title":"Resources"},{"location":"cloud/terraform/openstack/","text":"Openstack Terraform Error Error: One of 'auth_url' or 'cloud' must be specified Solution source /path/to/file.sh # (1)! Where file.sh is the openrc file","title":"Openstack error"},{"location":"cloud/terraform/openstack/#openstack-terraform","text":"","title":"Openstack Terraform"},{"location":"cloud/terraform/openstack/#error","text":"Error: One of 'auth_url' or 'cloud' must be specified","title":"Error"},{"location":"cloud/terraform/openstack/#solution","text":"source /path/to/file.sh # (1)! Where file.sh is the openrc file","title":"Solution"},{"location":"cloud/terraform/recursive-delete-of-terraform/","text":"Recursive delete of .terraform directory find ./ -type d -name \".terraform\" -exec rm -rf {} +","title":"Recursive delete of .terraform"},{"location":"cloud/terraform/recursive-delete-of-terraform/#recursive-delete-of-terraform-directory","text":"find ./ -type d -name \".terraform\" -exec rm -rf {} +","title":"Recursive delete of .terraform directory"},{"location":"cloud/terraform/remote-data/","text":"Remote data in terraform What is remote data Remote data is the system in terraform that allows us to use outputs from other configs, in ours. It sets the dependency on external resources, so should ideally be avoided. We should really use a data \"\" \"\"{} block We have split this up in to 2 parts. Part A: Contains the infrastructure Part B: The infrastructure that needs data from Part A Part A ip.tf resource \"google_compute_address\" \"nat\" { region = var.region project = var.project name = \"${var.region}-nat\" } output.tf output \"gke-nat-ip-address\" { value = google_compute_address.nat.address } Part B data.tf data \"terraform_remote_state\" \"init\" { backend = \"gcs\" config = { bucket = \"ip-setup\" prefix = \"nat\" } } We can then reference it via it's UID: data.terraform_remote_state.init.outputs.gke-nat-ip-address","title":"Remote Data"},{"location":"cloud/terraform/remote-data/#remote-data-in-terraform","text":"","title":"Remote data in terraform"},{"location":"cloud/terraform/remote-data/#what-is-remote-data","text":"Remote data is the system in terraform that allows us to use outputs from other configs, in ours. It sets the dependency on external resources, so should ideally be avoided. We should really use a data \"\" \"\"{} block We have split this up in to 2 parts. Part A: Contains the infrastructure Part B: The infrastructure that needs data from Part A","title":"What is remote data"},{"location":"cloud/terraform/remote-data/#part-a","text":"ip.tf resource \"google_compute_address\" \"nat\" { region = var.region project = var.project name = \"${var.region}-nat\" } output.tf output \"gke-nat-ip-address\" { value = google_compute_address.nat.address }","title":"Part A"},{"location":"cloud/terraform/remote-data/#part-b","text":"data.tf data \"terraform_remote_state\" \"init\" { backend = \"gcs\" config = { bucket = \"ip-setup\" prefix = \"nat\" } } We can then reference it via it's UID: data.terraform_remote_state.init.outputs.gke-nat-ip-address","title":"Part B"},{"location":"cloud/terraform/terraform-plugin-cannot-locate-module-locally-unknown-reason/","text":"Terraform plugin: Cannot locate module locally, unknown reason Terraform changed how they structure modules in the config fie So previously a module would look like this: module \"cloudrun-binding\" { source = \"git::ssh://git@github.com/userbradley/terraform-module-google-iam-binding.git//serviceaccount?ref=2022.03.08\" email = google_service_account.cloudrun.email project = var.project role = google_project_iam_custom_role.cloudrun.role_id } We get the error Terraform plugin: Cannot locate module locally, unknown reason To resolve this, change the format to: source = \"git::ssh://git@github.com/userbradley/terraform-module-google-iam-binding.git?ref=2022.03.08//serviceaccount\" We got this from the modules.json file","title":"Terraform plugin Cannot locate module locally, unknown reason"},{"location":"cloud/terraform/terraform-plugin-cannot-locate-module-locally-unknown-reason/#terraform-plugin-cannot-locate-module-locally-unknown-reason","text":"Terraform changed how they structure modules in the config fie So previously a module would look like this: module \"cloudrun-binding\" { source = \"git::ssh://git@github.com/userbradley/terraform-module-google-iam-binding.git//serviceaccount?ref=2022.03.08\" email = google_service_account.cloudrun.email project = var.project role = google_project_iam_custom_role.cloudrun.role_id } We get the error Terraform plugin: Cannot locate module locally, unknown reason To resolve this, change the format to: source = \"git::ssh://git@github.com/userbradley/terraform-module-google-iam-binding.git?ref=2022.03.08//serviceaccount\" We got this from the modules.json file","title":"Terraform plugin: Cannot locate module locally, unknown reason"},{"location":"cloud/terraform/tfupdate/","text":"Tfupdate tfupdate terraform -r ./ tfupdate provider -r google-beta ./ tfupdate provider -r google-beta ./ tfupdate release latest terraform-providers/terraform-provider-google tfupdate provider google-beta -v \">= $( tfupdate release latest terraform-providers/terraform-provider-google ) \" -r ./ tfupdate terraform -v \">= $( tfupdate release latest hashicorp/terraform ) \" -r ./ Resources Github","title":"tfupdate"},{"location":"cloud/terraform/tfupdate/#tfupdate","text":"tfupdate terraform -r ./ tfupdate provider -r google-beta ./ tfupdate provider -r google-beta ./ tfupdate release latest terraform-providers/terraform-provider-google tfupdate provider google-beta -v \">= $( tfupdate release latest terraform-providers/terraform-provider-google ) \" -r ./ tfupdate terraform -v \">= $( tfupdate release latest hashicorp/terraform ) \" -r ./","title":"Tfupdate"},{"location":"cloud/terraform/tfupdate/#resources","text":"Github","title":"Resources"},{"location":"cloud/terraform/to-string-from-list/","text":"To string from list in Terraform Sometimes, like for example when you're trying to crowbar a WAF together you want to pass a list. Example of list asn_block = [ \"16276\" , \"9009\" , \"13213\" , \"15395\" , \"15510\" , \"20738\" , \"20860\" , \"21321\" , \"25369\" , \"29302\" , \"29550\" , \"35017\" , \"35662\" , \"39326\" , \"42831\" , \"51159\" , \"57230\" , \"58305\" , \"59764\" , \"60011\" , \"60068\" , \"61317\" , \"62217\" , \"62240\" , \"199883\" , \"200039\" , \"57773\" , \"198047\" ] Converting it to a List asn_block_render = [ join ( \" || \", [for s in local.asn_block : format(\"origin.asn == %v\" , s )]) ] This will now render it like the below \"origin.asn == 16276 || origin.asn == 9009 || origin.asn == 13213 || origin.asn == 15395 || origin.asn == 15510 || origin.asn == 20738 || origin.asn == 20860 || origin.asn == 21321 || origin.asn == 25369 || origin.asn == 29302 || origin.asn == 29550 || origin.asn == 35017 || origin.asn == 35662 || origin.asn == 39326 || origin.asn == 42831 || origin.asn == 51159 || origin.asn == 57230 || origin.asn == 58305 || origin.asn == 59764 || origin.asn == 60011 || origin.asn == 60068 || origin.asn == 61317 || origin.asn == 62217 || origin.asn == 62240 || origin.asn == 199883 || origin.asn == 200039 || origin.asn == 57773 || origin.asn == 198047\" However, it's still as [\"\"] - So to consume it as a string, we call it like the below thing = local.asn_block_render[0 ]","title":"To string from list in terraform"},{"location":"cloud/terraform/to-string-from-list/#to-string-from-list-in-terraform","text":"Sometimes, like for example when you're trying to crowbar a WAF together you want to pass a list. Example of list asn_block = [ \"16276\" , \"9009\" , \"13213\" , \"15395\" , \"15510\" , \"20738\" , \"20860\" , \"21321\" , \"25369\" , \"29302\" , \"29550\" , \"35017\" , \"35662\" , \"39326\" , \"42831\" , \"51159\" , \"57230\" , \"58305\" , \"59764\" , \"60011\" , \"60068\" , \"61317\" , \"62217\" , \"62240\" , \"199883\" , \"200039\" , \"57773\" , \"198047\" ]","title":"To string from list in Terraform"},{"location":"cloud/terraform/to-string-from-list/#converting-it-to-a-list","text":"asn_block_render = [ join ( \" || \", [for s in local.asn_block : format(\"origin.asn == %v\" , s )]) ] This will now render it like the below \"origin.asn == 16276 || origin.asn == 9009 || origin.asn == 13213 || origin.asn == 15395 || origin.asn == 15510 || origin.asn == 20738 || origin.asn == 20860 || origin.asn == 21321 || origin.asn == 25369 || origin.asn == 29302 || origin.asn == 29550 || origin.asn == 35017 || origin.asn == 35662 || origin.asn == 39326 || origin.asn == 42831 || origin.asn == 51159 || origin.asn == 57230 || origin.asn == 58305 || origin.asn == 59764 || origin.asn == 60011 || origin.asn == 60068 || origin.asn == 61317 || origin.asn == 62217 || origin.asn == 62240 || origin.asn == 199883 || origin.asn == 200039 || origin.asn == 57773 || origin.asn == 198047\" However, it's still as [\"\"] - So to consume it as a string, we call it like the below thing = local.asn_block_render[0 ]","title":"Converting it to a List"},{"location":"home/","text":"What This section and it's sub-pages contains documentation and instructions on the items in the home Which The following items are documented: Oven Washing machine Fuse box","title":"Home documentation"},{"location":"home/#what","text":"This section and it's sub-pages contains documentation and instructions on the items in the home","title":"What"},{"location":"home/#which","text":"The following items are documented: Oven Washing machine Fuse box","title":"Which"},{"location":"home/cu/consumer-unit/","text":"Notice You should not have to interface with this during normal operations. Location Laundry Room Photo Problems you may have No power If you're using a device and the power goes out suddenly (Eg: Lights all go off) - It's probably the Consumer Unit Safety warning Ensure your hands are dry, and you have not been in contact with water Open the cover, by hinging it upwards. Once it stops, do not push any harder. Locate the switch that is down. A good example of this is the heaters in the above photo Flip the switch up in a fast jerking motion Water spilled on heater Turn off the main switch on the right hand side, and leave off till Bradley is home to resolve. This is to prevent electrical shock, or loss of life. Switches that should remain down During the summer months, and when we go away on holiday, we may turn off devices from the fuse board. However, during normal operation we will not have the heating turned on. This is because we cant afford the power bill - Wikipedia","title":"Consumer Unit"},{"location":"home/cu/consumer-unit/#location","text":"Laundry Room","title":"Location"},{"location":"home/cu/consumer-unit/#photo","text":"","title":"Photo"},{"location":"home/cu/consumer-unit/#problems-you-may-have","text":"","title":"Problems you may have"},{"location":"home/cu/consumer-unit/#no-power","text":"If you're using a device and the power goes out suddenly (Eg: Lights all go off) - It's probably the Consumer Unit Safety warning Ensure your hands are dry, and you have not been in contact with water Open the cover, by hinging it upwards. Once it stops, do not push any harder. Locate the switch that is down. A good example of this is the heaters in the above photo Flip the switch up in a fast jerking motion","title":"No power"},{"location":"home/cu/consumer-unit/#water-spilled-on-heater","text":"Turn off the main switch on the right hand side, and leave off till Bradley is home to resolve. This is to prevent electrical shock, or loss of life.","title":"Water spilled on heater"},{"location":"home/cu/consumer-unit/#switches-that-should-remain-down","text":"During the summer months, and when we go away on holiday, we may turn off devices from the fuse board. However, during normal operation we will not have the heating turned on. This is because we cant afford the power bill - Wikipedia","title":"Switches that should remain down"},{"location":"home/oven/oven/","text":"Location Kitchen Photo Using General Cooking Set the left hand dial to the 6'oclock position Left hand side Right hand side Setting the temperature You can use the right hand side dial to set the temperature. Note: It's in Degrees Celsius Grilling Move a shelf in the oven as high as it will go, or within a certain range to ensure that there is efficient use of energy Set the Left hand dial to Grilling Set the temperature between 200 and 250 Using the heat Setting the temperature higher does not cause it to heat up faster Left hand side Right hand side Light Note The light comes on at all times when the oven is in use You may want to turn the light on, on its own when cleaning the oven Set the left hand dial to light Left hand side Right hand side Null Defrost Do not use We do not have the money to use the oven to defrost things. This setting is used more to exhaust hot air from the oven to the apartment to ensure we use the heat well Open the door fully and switch the oven to Fan Left hand side Right hand side Null","title":"Oven"},{"location":"home/oven/oven/#location","text":"Kitchen","title":"Location"},{"location":"home/oven/oven/#photo","text":"","title":"Photo"},{"location":"home/oven/oven/#using","text":"","title":"Using"},{"location":"home/oven/oven/#general-cooking","text":"Set the left hand dial to the 6'oclock position Left hand side Right hand side Setting the temperature You can use the right hand side dial to set the temperature. Note: It's in Degrees Celsius","title":"General Cooking"},{"location":"home/oven/oven/#grilling","text":"Move a shelf in the oven as high as it will go, or within a certain range to ensure that there is efficient use of energy Set the Left hand dial to Grilling Set the temperature between 200 and 250 Using the heat Setting the temperature higher does not cause it to heat up faster Left hand side Right hand side","title":"Grilling"},{"location":"home/oven/oven/#light","text":"Note The light comes on at all times when the oven is in use You may want to turn the light on, on its own when cleaning the oven Set the left hand dial to light Left hand side Right hand side Null","title":"Light"},{"location":"home/oven/oven/#defrost","text":"Do not use We do not have the money to use the oven to defrost things. This setting is used more to exhaust hot air from the oven to the apartment to ensure we use the heat well Open the door fully and switch the oven to Fan Left hand side Right hand side Null","title":"Defrost"},{"location":"home/washing/washing-machine/","text":"Location Washing room Photo Using the machine Buttons The washing machine has 9 buttons Button name Location Use on/off Top left above selector knob Turning the machine on and off Mode selector Centre, large button surrounded by numbers Selects what washing mode to use from the chart on the left hand side Time saver First button from the left on the top row Enables time save on certain washing modes 1/2 spin Second button from the left on the top row Enables half time spin cycle on certain washing modes Intensive wash Third button from the left on the top row Sets the machine to wash more aggressively if materials are highly soiled Delay Timer Fourth button from the left on the top row Sets a delay in increments of 3 hours, See Time lights Start/ Pause Far right hand side button at top with Starts or pauses a wash - See Door does not open Temperature selector Second knob from the left Used to set water temprature for a wash Drying mode Third knob from the left Used to set the temprature the machine should dry clothes at - See Drying Status lights Time left on a wash This function is useless, as the 3h light will always be lit up. To work out when the machine is done, you can wait until the machine beeps, and you hear two Tick noises. Hour lights When using the delay mode, the lights will light up in order of 3h to 9h Setting a delay We use this mode if we are starting the machine during the night, and do not wish to be woken up as the machines works. Know the rough time you want the washing to go off. By default, I have the machine go off at 6am roughly, as this allows me enough time to wake up and hang the clothing up before the clothe smell Press the Time delay and the 3h light will come on. Press the Time delay button again, and the 6h button will come on. Pressing the button each time will increment it, setting the time from now Foreign washing machines As this machine is made for the domestic market, it's internal clock will be set to 50hz , however the grid fluctuates, so setting 3 hours can be within +/- 25 minutes Generic wash This type of wash is for the generic clothing that you wear day to day. Eg: Jeans, trousers, shirts etc. We are able to mix colors for the most part. Bright colors Take special care to items that are brightly colored, and have not been washed before. They should be washed on their own 4-5 times before Set the Selector Knob to the number 2 - Which signifies Mix Colors Set the Temprature selector to 40c Drying Selector Ensure that the Drying Mode is set to 0 otherwise you run the risk of damaging clothes, or causing large power bills Depending on the amount of clothing in the drum, put between 2-3 caps worth of detergent in the washing machines drum directly Close the door and press the button Sensitive items wash This type of wash is for the sensitive clothing you wear. Eg: Jumpers that are of a sensitive nature We are able to mix colors for the most part. Bright colors Take special care to items that are brightly colored, and have not been washed before. They should be washed on their own 4-5 times before Set the Selector Knob to the number 5 - Which signifies Silk & Delicates Set the Temprature selector to 30c Drying Selector Ensure that the Drying Mode is set to 0 otherwise you run the risk of damaging clothes, or causing large power bills Depending on the amount of clothing in the drum, put between 2-3 caps worth of detergent in the washing machines drum directly Close the door and press the button Soiled materials wash Please select below how soiled the materials are Name Justification Highly soiled For items that have large amounts of mud or biological material on them (eg: Blood or fecal matter Lightly soiled Items that have light dirt on them (eg: Dust, spilled juices, or offensive smells) Highly soiled Lightly soiled We are able to mix colors for the most part. Bright colors Take special care to items that are brightly colored, and have not been washed before. They should be washed on their own 4-5 times before Set the Selector Knob to the number 8 - Which signifies Sport intensive Set the Temprature selector to 90c Drying Selector Ensure that the Drying Mode is set to 0 otherwise you run the risk of damaging clothes, or causing large power bills Depending on the amount of clothing in the drum, put between 3-5 caps worth of detergent in the washing machines drum directly Close the door and press the button We are able to mix colors for the most part. Bright colors Take special care to items that are brightly colored, and have not been washed before. They should be washed on their own 4-5 times before Set the Selector Knob to the number 2 - Which signifies Mixe Colors Press the Intensive wash button Set the Temprature selector to 50c Drying Selector Ensure that the Drying Mode is set to 0 otherwise you run the risk of damaging clothes, or causing large power bills Depending on the amount of clothing in the drum, put between 3-5 caps worth of detergent in the washing machines drum directly Close the door and press the button Spin Sometimes if we are washing large loads of clothing the machine may not spin enough to remove most of the water content We are able to request a second spin cycle. Ensure the door is closed and the previous cycle has concluded Set the Selector knob to 15 Press the button The cycle is completed when the machine unlocks Drying Warning This should not be used unless authorised to do so Troubleshooting Door is not opening This can happen when the machine is in a failed state. Do not force the door open Doing so can cause the machine to break. Once the cycle has finished, the machine will beep. This is loud enough to hear from anywhere in the apartment. If the door does not open immediately, wait 3-10 minutes till you hear 2 clicks. These are not as loud, but are not a common noise - so you will hear it If the door does not open still, power the machine off using the switch on the right hand side wall visibly labelled Washing Machine It's the second switch in from the light switch. Wait 3-5 minutes and power the machine back on Wait 3-5 minutes Try and open the door Door continues to fail to open Please contact the management company for the apartment.","title":"Washing Machine"},{"location":"home/washing/washing-machine/#location","text":"Washing room","title":"Location"},{"location":"home/washing/washing-machine/#photo","text":"","title":"Photo"},{"location":"home/washing/washing-machine/#using-the-machine","text":"","title":"Using the machine"},{"location":"home/washing/washing-machine/#buttons","text":"The washing machine has 9 buttons Button name Location Use on/off Top left above selector knob Turning the machine on and off Mode selector Centre, large button surrounded by numbers Selects what washing mode to use from the chart on the left hand side Time saver First button from the left on the top row Enables time save on certain washing modes 1/2 spin Second button from the left on the top row Enables half time spin cycle on certain washing modes Intensive wash Third button from the left on the top row Sets the machine to wash more aggressively if materials are highly soiled Delay Timer Fourth button from the left on the top row Sets a delay in increments of 3 hours, See Time lights Start/ Pause Far right hand side button at top with Starts or pauses a wash - See Door does not open Temperature selector Second knob from the left Used to set water temprature for a wash Drying mode Third knob from the left Used to set the temprature the machine should dry clothes at - See Drying","title":"Buttons"},{"location":"home/washing/washing-machine/#status-lights","text":"","title":"Status lights"},{"location":"home/washing/washing-machine/#time-left-on-a-wash","text":"This function is useless, as the 3h light will always be lit up. To work out when the machine is done, you can wait until the machine beeps, and you hear two Tick noises.","title":"Time left on a wash"},{"location":"home/washing/washing-machine/#hour-lights","text":"When using the delay mode, the lights will light up in order of 3h to 9h","title":"Hour lights"},{"location":"home/washing/washing-machine/#setting-a-delay","text":"We use this mode if we are starting the machine during the night, and do not wish to be woken up as the machines works. Know the rough time you want the washing to go off. By default, I have the machine go off at 6am roughly, as this allows me enough time to wake up and hang the clothing up before the clothe smell Press the Time delay and the 3h light will come on. Press the Time delay button again, and the 6h button will come on. Pressing the button each time will increment it, setting the time from now Foreign washing machines As this machine is made for the domestic market, it's internal clock will be set to 50hz , however the grid fluctuates, so setting 3 hours can be within +/- 25 minutes","title":"Setting a delay"},{"location":"home/washing/washing-machine/#generic-wash","text":"This type of wash is for the generic clothing that you wear day to day. Eg: Jeans, trousers, shirts etc. We are able to mix colors for the most part. Bright colors Take special care to items that are brightly colored, and have not been washed before. They should be washed on their own 4-5 times before Set the Selector Knob to the number 2 - Which signifies Mix Colors Set the Temprature selector to 40c Drying Selector Ensure that the Drying Mode is set to 0 otherwise you run the risk of damaging clothes, or causing large power bills Depending on the amount of clothing in the drum, put between 2-3 caps worth of detergent in the washing machines drum directly Close the door and press the button","title":"Generic wash"},{"location":"home/washing/washing-machine/#sensitive-items-wash","text":"This type of wash is for the sensitive clothing you wear. Eg: Jumpers that are of a sensitive nature We are able to mix colors for the most part. Bright colors Take special care to items that are brightly colored, and have not been washed before. They should be washed on their own 4-5 times before Set the Selector Knob to the number 5 - Which signifies Silk & Delicates Set the Temprature selector to 30c Drying Selector Ensure that the Drying Mode is set to 0 otherwise you run the risk of damaging clothes, or causing large power bills Depending on the amount of clothing in the drum, put between 2-3 caps worth of detergent in the washing machines drum directly Close the door and press the button","title":"Sensitive items wash"},{"location":"home/washing/washing-machine/#soiled-materials-wash","text":"Please select below how soiled the materials are Name Justification Highly soiled For items that have large amounts of mud or biological material on them (eg: Blood or fecal matter Lightly soiled Items that have light dirt on them (eg: Dust, spilled juices, or offensive smells) Highly soiled Lightly soiled We are able to mix colors for the most part. Bright colors Take special care to items that are brightly colored, and have not been washed before. They should be washed on their own 4-5 times before Set the Selector Knob to the number 8 - Which signifies Sport intensive Set the Temprature selector to 90c Drying Selector Ensure that the Drying Mode is set to 0 otherwise you run the risk of damaging clothes, or causing large power bills Depending on the amount of clothing in the drum, put between 3-5 caps worth of detergent in the washing machines drum directly Close the door and press the button We are able to mix colors for the most part. Bright colors Take special care to items that are brightly colored, and have not been washed before. They should be washed on their own 4-5 times before Set the Selector Knob to the number 2 - Which signifies Mixe Colors Press the Intensive wash button Set the Temprature selector to 50c Drying Selector Ensure that the Drying Mode is set to 0 otherwise you run the risk of damaging clothes, or causing large power bills Depending on the amount of clothing in the drum, put between 3-5 caps worth of detergent in the washing machines drum directly Close the door and press the button","title":"Soiled materials wash"},{"location":"home/washing/washing-machine/#spin","text":"Sometimes if we are washing large loads of clothing the machine may not spin enough to remove most of the water content We are able to request a second spin cycle. Ensure the door is closed and the previous cycle has concluded Set the Selector knob to 15 Press the button The cycle is completed when the machine unlocks","title":"Spin"},{"location":"home/washing/washing-machine/#drying","text":"Warning This should not be used unless authorised to do so","title":"Drying"},{"location":"home/washing/washing-machine/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"home/washing/washing-machine/#door-is-not-opening","text":"This can happen when the machine is in a failed state. Do not force the door open Doing so can cause the machine to break. Once the cycle has finished, the machine will beep. This is loud enough to hear from anywhere in the apartment. If the door does not open immediately, wait 3-10 minutes till you hear 2 clicks. These are not as loud, but are not a common noise - so you will hear it If the door does not open still, power the machine off using the switch on the right hand side wall visibly labelled Washing Machine It's the second switch in from the light switch. Wait 3-5 minutes and power the machine back on Wait 3-5 minutes Try and open the door","title":"Door is not opening"},{"location":"home/washing/washing-machine/#door-continues-to-fail-to-open","text":"Please contact the management company for the apartment.","title":"Door continues to fail to open"},{"location":"kb/authentication/fingerprint-on-linux-mint/","text":"Fingerprint on Linux Mint Use fprintd (lacks gui) sudo apt install fprintd libpam-fprintd Example code for enrolling your (i) specific finger or (ii) all your fingers: fprintd-enroll -f left-index-finger All Fingers for finger in { left,right } - { thumb, { index,middle,ring,little } -finger } ; do sudo fprintd-enroll -f \" $finger \" ; done Follow the prompt and swipe your finger across your scanner 5 times. Finally, enable access by marking Fingerprint option with * using the spacebar key in: sudo pam-auth-update Appeared here","title":"Fingerprint on Linux Mint"},{"location":"kb/authentication/fingerprint-on-linux-mint/#fingerprint-on-linux-mint","text":"Use fprintd (lacks gui) sudo apt install fprintd libpam-fprintd Example code for enrolling your (i) specific finger or (ii) all your fingers: fprintd-enroll -f left-index-finger All Fingers for finger in { left,right } - { thumb, { index,middle,ring,little } -finger } ; do sudo fprintd-enroll -f \" $finger \" ; done Follow the prompt and swipe your finger across your scanner 5 times. Finally, enable access by marking Fingerprint option with * using the spacebar key in: sudo pam-auth-update Appeared here","title":"Fingerprint on Linux Mint"},{"location":"kb/aws/aws-cli/","text":"AWS CLI Locate resources based on IP aws ec2 describe-network-interfaces --filters Name = addresses.private-ip-address,Values = <ip> Find an EC2 instance from its security group aws ec2 describe-network-interfaces --filters Name = group-id,Values = <group-id> --region <region> --output json","title":"AWS CLI"},{"location":"kb/aws/aws-cli/#aws-cli","text":"Locate resources based on IP aws ec2 describe-network-interfaces --filters Name = addresses.private-ip-address,Values = <ip> Find an EC2 instance from its security group aws ec2 describe-network-interfaces --filters Name = group-id,Values = <group-id> --region <region> --output json","title":"AWS CLI"},{"location":"kb/aws/cloud-init-sg1/","text":"AWS Cloud-init Content-Type: multipart/mixed ; boundary = \"//\" MIME-Version: 1 .0 --// Content-Type: text/cloud-config ; charset = \"us-ascii\" MIME-Version: 1 .0 Content-Transfer-Encoding: 7bit Content-Disposition: attachment ; filename = \"cloud-config.txt\" #cloud-config cloud_final_modules: - [ scripts-user, always ] --// Content-Type: text/x-shellscript ; charset = \"us-ascii\" MIME-Version: 1 .0 Content-Transfer-Encoding: 7bit Content-Disposition: attachment ; filename = \"userdata.txt\" #!/bin/bash /bin/echo \"Hello World\" >> /tmp/testfile.txt --//--","title":"AWS Cloud-init"},{"location":"kb/aws/cloud-init-sg1/#aws-cloud-init","text":"Content-Type: multipart/mixed ; boundary = \"//\" MIME-Version: 1 .0 --// Content-Type: text/cloud-config ; charset = \"us-ascii\" MIME-Version: 1 .0 Content-Transfer-Encoding: 7bit Content-Disposition: attachment ; filename = \"cloud-config.txt\" #cloud-config cloud_final_modules: - [ scripts-user, always ] --// Content-Type: text/x-shellscript ; charset = \"us-ascii\" MIME-Version: 1 .0 Content-Transfer-Encoding: 7bit Content-Disposition: attachment ; filename = \"userdata.txt\" #!/bin/bash /bin/echo \"Hello World\" >> /tmp/testfile.txt --//--","title":"AWS Cloud-init"},{"location":"kb/cloud/cloud-init/","text":"cloud-init Cloud init is awesome, but a pain to set up. Hopefully if you're reading this then we're on an adventure! Create a vm of ubuntu Set it up with your usual username and password Log on and mount the xen tools. (guest-tools.iso on xcp) on the vm mount /dev/cdrom /mnt bash ./mnt/Linux/install.sh apt-get install cloud-init cloud-initramfs-growroot Delete the stuff in /var/lib/cloud/instance rm -rvf /var/lib/cloud/instance/* Probably worth doing any custom shit like dot files or default ssh keys, ntp settings, routes and dns settings. Whilst this can be done though cloud-init, it does help to have it in the template Unmount any disks and clear anything off the VM like text files or crap you've left there Shutdown the VM and convert it to a template on xcp-ng When you create a new VM, ensure to use the name of the template vm. Pick your cloud-init type, be in a full file or just ssh keys. Sauce: http://www.whiteboardcoder.com/2016/04/install-cloud-init-on-ubuntu-and-use.html","title":"cloud-init"},{"location":"kb/cloud/cloud-init/#cloud-init","text":"Cloud init is awesome, but a pain to set up. Hopefully if you're reading this then we're on an adventure! Create a vm of ubuntu Set it up with your usual username and password Log on and mount the xen tools. (guest-tools.iso on xcp) on the vm mount /dev/cdrom /mnt bash ./mnt/Linux/install.sh apt-get install cloud-init cloud-initramfs-growroot Delete the stuff in /var/lib/cloud/instance rm -rvf /var/lib/cloud/instance/* Probably worth doing any custom shit like dot files or default ssh keys, ntp settings, routes and dns settings. Whilst this can be done though cloud-init, it does help to have it in the template Unmount any disks and clear anything off the VM like text files or crap you've left there Shutdown the VM and convert it to a template on xcp-ng When you create a new VM, ensure to use the name of the template vm. Pick your cloud-init type, be in a full file or just ssh keys. Sauce: http://www.whiteboardcoder.com/2016/04/install-cloud-init-on-ubuntu-and-use.html","title":"cloud-init"},{"location":"kb/disk-management/expanding-a-filesystem/","text":"Expanding a file system Locate the root partition df -h Should look like the below [ stannardb@xc- ] :~$ df -h Filesystem Size Used Avail Use% Mounted on udev 3 .9G 0 3 .9G 0 % /dev tmpfs 386M 1 .1M 385M 1 % /run /dev/xvda2 59G 13G 44G 22 % / tmpfs 1 .9G 0 1 .9G 0 % /dev/shm tmpfs 5 .0M 0 5 .0M 0 % /run/lock tmpfs 1 .9G 0 1 .9G 0 % /sys/fs/cgroup /dev/loop1 90M 90M 0 100 % /snap/core/8268 /dev/loop2 92M 92M 0 100 % /snap/core/8592 tmpfs 386M 0 386M 0 % /run/user/1000 Then run growpart /dev/xvda 2 resize2fs /dev/xvda2","title":"Expanding a file system"},{"location":"kb/disk-management/expanding-a-filesystem/#expanding-a-file-system","text":"Locate the root partition df -h Should look like the below [ stannardb@xc- ] :~$ df -h Filesystem Size Used Avail Use% Mounted on udev 3 .9G 0 3 .9G 0 % /dev tmpfs 386M 1 .1M 385M 1 % /run /dev/xvda2 59G 13G 44G 22 % / tmpfs 1 .9G 0 1 .9G 0 % /dev/shm tmpfs 5 .0M 0 5 .0M 0 % /run/lock tmpfs 1 .9G 0 1 .9G 0 % /sys/fs/cgroup /dev/loop1 90M 90M 0 100 % /snap/core/8268 /dev/loop2 92M 92M 0 100 % /snap/core/8592 tmpfs 386M 0 386M 0 % /run/user/1000 Then run growpart /dev/xvda 2 resize2fs /dev/xvda2","title":"Expanding a file system"},{"location":"kb/disk-management/formatting-drive-automount/","text":"Formatting drive and auto mount List logical disks and partitions sudo fdisk -l Partition the disk sudo fdisk /dev/sdb Press n to create a partition Press p or l to create primary or logical partitions Press w to write your changes or q to quit Format the partition sudo mkfs -t ext4 /dev/sdb1 sudo mkfs -t ext4 -N 2000000 /dev/sdb1 - This will manually set the number of inodes to 2,000,000 Mount disk mount - Shows what is mounted mkdir /mnt/mydrive mount -t ext4 /dev/sdb1 /mnt/mydrive Get disk's UUID ls -al /dev/disk/by-uuid/ or blkid Mount at boot Add the following line to your /etc/fstab file adjusting the UUID to your device's id and the directory to where you want to mount: UUID=811d3de0-ca6b-4b61-9445-af2e306d9999 /mnt/mydrive ext4 defaults 0 0 mount -a - remounts filesystems from /etc/fstab","title":"Formatting drive and Auto mount"},{"location":"kb/disk-management/formatting-drive-automount/#formatting-drive-and-auto-mount","text":"","title":"Formatting drive and auto mount"},{"location":"kb/disk-management/gpt-pmbr-size-mismatch-will-be-corrected-by-write/","text":"gpt pmbr size mismatch will be corrected by write Fdisk used to not work at all on gpt partitioned drives, it just reported that drive was gpt partitioned. Better to use parted, gparted or gdisk. Gdisk has been the command line tool for gpt drives. Post these: sudo parted -l or sudo parted /dev/sda unit s print or sudo gdisk -l /dev/sda Then: Expand the filesystem","title":"GPT PMBR Size Mismatch will be corrected by w(rite)"},{"location":"kb/disk-management/gpt-pmbr-size-mismatch-will-be-corrected-by-write/#gpt-pmbr-size-mismatch-will-be-corrected-by-write","text":"Fdisk used to not work at all on gpt partitioned drives, it just reported that drive was gpt partitioned. Better to use parted, gparted or gdisk. Gdisk has been the command line tool for gpt drives. Post these: sudo parted -l or sudo parted /dev/sda unit s print or sudo gdisk -l /dev/sda Then: Expand the filesystem","title":"gpt pmbr size mismatch will be corrected by write"},{"location":"kb/disk-management/mount-a-new-drive/","text":"Mount a new drive How to mount a new drive on a linux system over command line First, list the physically installed disks fdisk -l Next, Pick the disk you want to mount mkfs.ext4 /dev/<disk path> Mount the drive mount /dev/<disk path> /<location to mount> To auto mount it, edit fstab Firstly, found out it's UUID blkid should look like the below, here we want /dev/xvdb [ stannardb@nextcloud ] :~$ blkid /dev/xvdb: UUID = \"02ed6055-ccea-40d7-a735-4045a36df5d7\" TYPE = \"ext4\" /dev/xvda2: UUID = \"0fa95d3f-7231-4b98-94ca-3d407d88f600\" TYPE = \"ext4\" PARTUUID = \"8ecf3453-59dd-4535-94e6-656f66c289f7\" /dev/sr0: UUID = \"2019-07-16-08-51-27-00\" LABEL = \"XCP-ng Tools\" TYPE = \"iso9660\" sudo nano /etc/fstab UUID=02ed6055-ccea-40d7-a735-4045a36df5d7 /nextcloud ext4 defaults 0 0 This mounts the disk with the UUID of 02ed6055-ccea-40d7-a735-4045a36df5d7 to /nextcloud as an ext4 drive Old Method Device Mountpoint fstype option dump fsck /dev/sdb1 /home/yourname/mydata ext4 defaults 0 1","title":"Mount a new drive"},{"location":"kb/disk-management/mount-a-new-drive/#mount-a-new-drive","text":"How to mount a new drive on a linux system over command line First, list the physically installed disks fdisk -l Next, Pick the disk you want to mount mkfs.ext4 /dev/<disk path> Mount the drive mount /dev/<disk path> /<location to mount> To auto mount it, edit fstab Firstly, found out it's UUID blkid should look like the below, here we want /dev/xvdb [ stannardb@nextcloud ] :~$ blkid /dev/xvdb: UUID = \"02ed6055-ccea-40d7-a735-4045a36df5d7\" TYPE = \"ext4\" /dev/xvda2: UUID = \"0fa95d3f-7231-4b98-94ca-3d407d88f600\" TYPE = \"ext4\" PARTUUID = \"8ecf3453-59dd-4535-94e6-656f66c289f7\" /dev/sr0: UUID = \"2019-07-16-08-51-27-00\" LABEL = \"XCP-ng Tools\" TYPE = \"iso9660\" sudo nano /etc/fstab UUID=02ed6055-ccea-40d7-a735-4045a36df5d7 /nextcloud ext4 defaults 0 0 This mounts the disk with the UUID of 02ed6055-ccea-40d7-a735-4045a36df5d7 to /nextcloud as an ext4 drive Old Method Device Mountpoint fstype option dump fsck /dev/sdb1 /home/yourname/mydata ext4 defaults 0 1","title":"Mount a new drive"},{"location":"kb/docker/basics-of-docker/","text":"Basics of docker To list all images","title":"Basics of docker"},{"location":"kb/docker/basics-of-docker/#basics-of-docker","text":"To list all images","title":"Basics of docker"},{"location":"kb/docker/bulk-retag/","text":"Bulk retag This page is now superseded by Copy images between repositories docker images <old repo> --filter \"since=<old repo>\" --format \"docker tag {{.Repository}}:{{.Tag}} <new repo>:{{.Tag}} | docker push <new repo>:{{.Tag}}\" | bash","title":"Bulk retag"},{"location":"kb/docker/bulk-retag/#bulk-retag","text":"This page is now superseded by Copy images between repositories docker images <old repo> --filter \"since=<old repo>\" --format \"docker tag {{.Repository}}:{{.Tag}} <new repo>:{{.Tag}} | docker push <new repo>:{{.Tag}}\" | bash","title":"Bulk retag"},{"location":"kb/docker/copy-containers-between-repos/","text":"Copy containers between repos What This is especially useful when moving from gcr.io to the new Artifact registry that Google offer. That's not to say that you can't use this for copying between private repositories (as long as you're authenticated) How We can use a tool called gcrane Install gcrane go install github.com/google/go-containerregistry/cmd/gcrane@latest This will build and install the package to your $PATH List images gcrane ls userbradley/searchsploit We should see something similar to the below index.docker.io/userbradley/searchsploit:alpine-0.0.1 index.docker.io/userbradley/searchsploit:alpine-0.0.2 index.docker.io/userbradley/searchsploit:alpine-base index.docker.io/userbradley/searchsploit:ubuntu-0.0.1 index.docker.io/userbradley/searchsploit:ubuntu-1 Copy all images to AR gcrane cp -r userbradley/searchsploit europe-west2.pkg.dev/bradley/searchsploit/searchsploit External sources Google Documentation Github source code footnotes If you are not copying to GAR You may want to look at the below https://github.com/containers/skopeo","title":"Copy images between repositories"},{"location":"kb/docker/copy-containers-between-repos/#copy-containers-between-repos","text":"","title":"Copy containers between repos"},{"location":"kb/docker/copy-containers-between-repos/#what","text":"This is especially useful when moving from gcr.io to the new Artifact registry that Google offer. That's not to say that you can't use this for copying between private repositories (as long as you're authenticated)","title":"What"},{"location":"kb/docker/copy-containers-between-repos/#how","text":"We can use a tool called gcrane","title":"How"},{"location":"kb/docker/copy-containers-between-repos/#install-gcrane","text":"go install github.com/google/go-containerregistry/cmd/gcrane@latest This will build and install the package to your $PATH","title":"Install gcrane"},{"location":"kb/docker/copy-containers-between-repos/#list-images","text":"gcrane ls userbradley/searchsploit We should see something similar to the below index.docker.io/userbradley/searchsploit:alpine-0.0.1 index.docker.io/userbradley/searchsploit:alpine-0.0.2 index.docker.io/userbradley/searchsploit:alpine-base index.docker.io/userbradley/searchsploit:ubuntu-0.0.1 index.docker.io/userbradley/searchsploit:ubuntu-1","title":"List images"},{"location":"kb/docker/copy-containers-between-repos/#copy-all-images-to-ar","text":"gcrane cp -r userbradley/searchsploit europe-west2.pkg.dev/bradley/searchsploit/searchsploit","title":"Copy all images to AR"},{"location":"kb/docker/copy-containers-between-repos/#external-sources","text":"Google Documentation Github source code","title":"External sources"},{"location":"kb/docker/copy-containers-between-repos/#footnotes","text":"If you are not copying to GAR You may want to look at the below https://github.com/containers/skopeo","title":"footnotes"},{"location":"kb/docker/docker-architecture/","text":"Docker Architecture export DOCKER_DEFAULT_PLATFORM = linux/amd64","title":"Docker Architecture"},{"location":"kb/docker/docker-architecture/#docker-architecture","text":"export DOCKER_DEFAULT_PLATFORM = linux/amd64","title":"Docker Architecture"},{"location":"kb/docker/docker-intro-and-notes/","text":"title: Docker: Intro and notes outdated: true Docker: Intro and notes What are containers? completely isolated environment. They can have their own mounts, networks and applications. Docker runs off of lxc containers and provides a high level management interface. Docker containers share the kernal. They can run anything on top of it (even an os) but it will have the same kernel version. Why do you need it? Allows for running services that require different versions of prereqs. Allows for cross os compatibility. Alot of companies and programs have got their software published to docker hub Install docker: $ curl -fsSL https://get.docker.com -o get-docker.sh $ sudo sh get-docker.sh sudo usermod -aG docker your-user Done! You can check the docker verson using sudo docker version Nhow we can see if it works by running: docker pull docker/whalesay Which pulls a docer image from the previous mentioned docker hub. called Whale say. sudo docker run docker/whalesay cowsay hello which looks like this! root@DocketHost:~/docker/graphhopper# sudo docker run docker/whalesay cowsay hello _______ < hello > ------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ now we know docker is working Run docker containers Running docker run <name> runs a docker container. If the image is not found, it will find and pull the docker but the next time, it will use the same image you're already pulled docker ps will show you info about the docker container. To see all containers, use -a docker ps -a Stopping a container docker stop <name or id> to remote a docker, docker rm <name or id> To view a list of images we have on the server, run docker images to remove images, use docker rmi -f <id> To run a command on a docker, run docker exec <name> command Running a docker image in interactive mode is achived using docker run -i container and to interact with it, use docker run -ti container Data persistance In order to ensure that the data persists, you need to map an external folder to the docker container docker run -V /opt/data:/var/lib/mysql mysql Create docker Image Networks in docker All containers get an internal IP address that other containers can connect to. This is all good and well for internal applications but useless for external applicaitons. In order to access the port and network of the container, we must use the -p command which maps the port to the container Example: docker run -p 80:5000 kodekloud/simple-webapp This maps external port 80 to internal port 5000 Seurprisingly you cant map to the same port the same time. Docker compose Docker concepts in depth Docker for windows/ mac Docker swarm Docker vs k8","title":"Docker: Intro and notes"},{"location":"kb/docker/docker-intro-and-notes/#docker-intro-and-notes","text":"","title":"Docker: Intro and notes"},{"location":"kb/docker/exporting-and-importing/","text":"Exporting and importing Sometimes you will need to export an image as a zip file because you cant commit it to a repo docker export <image>:<tag> -o <image>.zip Copy the file where ever you need it to be done When importing it doesnt pull over the run command, just the file sytem. We need to import it differently docker import \\ --change 'CMD [\"command\", \"goes\", \"here\"]' \\ <file>.zip <image>:<tag> if you're still having issues, try docker load < <file>.zip","title":"Exporting and importing"},{"location":"kb/docker/exporting-and-importing/#exporting-and-importing","text":"Sometimes you will need to export an image as a zip file because you cant commit it to a repo docker export <image>:<tag> -o <image>.zip Copy the file where ever you need it to be done When importing it doesnt pull over the run command, just the file sytem. We need to import it differently docker import \\ --change 'CMD [\"command\", \"goes\", \"here\"]' \\ <file>.zip <image>:<tag> if you're still having issues, try docker load < <file>.zip","title":"Exporting and importing"},{"location":"kb/docker/installing-docker/","text":"Installing docker Step 1 \u2014 Installing Docker The Docker installation package available in the official Ubuntu repository may not be the latest version. To ensure we get the latest version, we\u2019ll install Docker from the official Docker repository. To do that, we\u2019ll add a new package source, add the GPG key from Docker to ensure the downloads are valid, and then install the package. First, update your existing list of packages: sudo apt update Next, install a few prerequisite packages which let apt use packages over HTTPS: sudo apt install apt-transport-https ca-certificates curl software-properties-common Then add the GPG key for the official Docker repository to your system: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Add the Docker repository to APT sources: sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\" Next, update the package database with the Docker packages from the newly added repo: sudo apt update Make sure you are about to install from the Docker repo instead of the default Ubuntu repo: apt-cache policy docker-ce You\u2019ll see output like this, although the version number for Docker may be different: docker-ce: Installed: (none) Candidate: 18.03.1~ce~3-0~ubuntu Version table: 18.03.1~ce~3-0~ubuntu 500 500 https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages Notice that docker-ce is not installed, but the candidate for installation is from the Docker repository for Ubuntu 18.04 (bionic). Finally, install Docker: sudo apt install docker-ce Docker should now be installed, the daemon started, and the process enabled to start on boot. Check that it\u2019s running: sudo systemctl status docker","title":"Installing Docker"},{"location":"kb/docker/installing-docker/#installing-docker","text":"Step 1 \u2014 Installing Docker The Docker installation package available in the official Ubuntu repository may not be the latest version. To ensure we get the latest version, we\u2019ll install Docker from the official Docker repository. To do that, we\u2019ll add a new package source, add the GPG key from Docker to ensure the downloads are valid, and then install the package. First, update your existing list of packages: sudo apt update Next, install a few prerequisite packages which let apt use packages over HTTPS: sudo apt install apt-transport-https ca-certificates curl software-properties-common Then add the GPG key for the official Docker repository to your system: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Add the Docker repository to APT sources: sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\" Next, update the package database with the Docker packages from the newly added repo: sudo apt update Make sure you are about to install from the Docker repo instead of the default Ubuntu repo: apt-cache policy docker-ce You\u2019ll see output like this, although the version number for Docker may be different: docker-ce: Installed: (none) Candidate: 18.03.1~ce~3-0~ubuntu Version table: 18.03.1~ce~3-0~ubuntu 500 500 https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages Notice that docker-ce is not installed, but the candidate for installation is from the Docker repository for Ubuntu 18.04 (bionic). Finally, install Docker: sudo apt install docker-ce Docker should now be installed, the daemon started, and the process enabled to start on boot. Check that it\u2019s running: sudo systemctl status docker","title":"Installing docker"},{"location":"kb/docker/installing-jellyfin/","text":"Installing jellyfin curl -fsSL https://get.docker.com -o get-docker.shsh get-docker.sh apt-get install docker-compose Before running the Jellyfin Container. We need to make 3 folders: Make a folder called Jellyfin, and create the 3 folders inside the Jellyfin Folder. Config Cache Media mkdir -p Path/To/Config mkdir -p Path/To/Cache mkdir -p Path/To/Media (may need sudo) docker pull jellyfin/jellyfin Docker-Compose Create a docker-compose.yml file with the following contents: version : \"3\" services : jellyfin : image : jellyfin/jellyfin user : 1000:1000 network_mode : \"host\" volumes : - /path/to/config:/config - /path/to/cache:/cache - /path/to/media:/media Then while in the same folder as the docker-compose.yml run: docker-compose up","title":"Installing jellyfin"},{"location":"kb/docker/installing-jellyfin/#installing-jellyfin","text":"curl -fsSL https://get.docker.com -o get-docker.shsh get-docker.sh apt-get install docker-compose Before running the Jellyfin Container. We need to make 3 folders: Make a folder called Jellyfin, and create the 3 folders inside the Jellyfin Folder. Config Cache Media mkdir -p Path/To/Config mkdir -p Path/To/Cache mkdir -p Path/To/Media (may need sudo) docker pull jellyfin/jellyfin Docker-Compose Create a docker-compose.yml file with the following contents: version : \"3\" services : jellyfin : image : jellyfin/jellyfin user : 1000:1000 network_mode : \"host\" volumes : - /path/to/config:/config - /path/to/cache:/cache - /path/to/media:/media Then while in the same folder as the docker-compose.yml run: docker-compose up","title":"Installing jellyfin"},{"location":"kb/google-cloud/api-get-project/","text":"Get GCP project using compute metadata api server Why If you're running a workload in GKe, sometimes it's useful to know what project that pod runs under How curl -H \"Metadata-Flavor: Google\" http://metadata.google.internal/computeMetadata/v1/project/project-id Read next Get SA","title":"Get project using GCE Metadata API"},{"location":"kb/google-cloud/api-get-project/#get-gcp-project-using-compute-metadata-api-server","text":"","title":"Get GCP project using compute metadata api server"},{"location":"kb/google-cloud/api-get-project/#why","text":"If you're running a workload in GKe, sometimes it's useful to know what project that pod runs under","title":"Why"},{"location":"kb/google-cloud/api-get-project/#how","text":"curl -H \"Metadata-Flavor: Google\" http://metadata.google.internal/computeMetadata/v1/project/project-id","title":"How"},{"location":"kb/google-cloud/api-get-project/#read-next","text":"Get SA","title":"Read next"},{"location":"kb/google-cloud/api-get-sa/","text":"Get GCP Service Account using compute metadata api server Why If you're using workload identity in Kubernetes, sometimes you need to know what external (being GCP) Service account youre running as How curl -H \"Metadata-Flavor: Google\" http://169.254.169.254/computeMetadata/v1/instance/service-accounts/ Read Next Get Project","title":"Get serviceAccount using GCE Metadata API"},{"location":"kb/google-cloud/api-get-sa/#get-gcp-service-account-using-compute-metadata-api-server","text":"","title":"Get GCP Service Account using compute metadata api server"},{"location":"kb/google-cloud/api-get-sa/#why","text":"If you're using workload identity in Kubernetes, sometimes you need to know what external (being GCP) Service account youre running as","title":"Why"},{"location":"kb/google-cloud/api-get-sa/#how","text":"curl -H \"Metadata-Flavor: Google\" http://169.254.169.254/computeMetadata/v1/instance/service-accounts/","title":"How"},{"location":"kb/google-cloud/api-get-sa/#read-next","text":"Get Project","title":"Read Next"},{"location":"kb/google-cloud/projects-resources-iam-users-roles-permissions-apis-and-cloud-shell/","text":"Projects, Resources, IAM Users, Roles, Permissions, APIs, and Cloud Shell Permissions Role Name Permissions roles/viewer Permissions for read-only actions that do not affect state, such as viewing (but not modifying) existing resources or data. roles/editor All viewer permissions, plus permissions for actions that modify state, such as changing existing resources. roles/owner All editor permissions and permissions for the following actions Manage roles and permissions for a project and all resources within the project. Set up billing for a project. Regions (Central US) are physical countries. Zones are the data centre (Physical) like us-central1-a follows country-zone-count gcloud List active account name gcloud auth list List ProjectID gloud config list project Creating an instance though the cloud command line gloud compute instances create <name> --machine-type <machine type> --zone <see zone location>","title":"Projects, Resources, IAM Users, Roles, Permissions, APIs, and Cloud Shell"},{"location":"kb/google-cloud/projects-resources-iam-users-roles-permissions-apis-and-cloud-shell/#projects-resources-iam-users-roles-permissions-apis-and-cloud-shell","text":"","title":"Projects, Resources, IAM Users, Roles, Permissions, APIs, and Cloud Shell"},{"location":"kb/google-cloud/projects-resources-iam-users-roles-permissions-apis-and-cloud-shell/#permissions","text":"Role Name Permissions roles/viewer Permissions for read-only actions that do not affect state, such as viewing (but not modifying) existing resources or data. roles/editor All viewer permissions, plus permissions for actions that modify state, such as changing existing resources. roles/owner All editor permissions and permissions for the following actions Manage roles and permissions for a project and all resources within the project. Set up billing for a project. Regions (Central US) are physical countries. Zones are the data centre (Physical) like us-central1-a follows country-zone-count","title":"Permissions"},{"location":"kb/google-cloud/projects-resources-iam-users-roles-permissions-apis-and-cloud-shell/#gcloud","text":"","title":"gcloud"},{"location":"kb/google-cloud/projects-resources-iam-users-roles-permissions-apis-and-cloud-shell/#list-active-account-name","text":"gcloud auth list","title":"List active account name"},{"location":"kb/google-cloud/projects-resources-iam-users-roles-permissions-apis-and-cloud-shell/#list-projectid","text":"gloud config list project","title":"List ProjectID"},{"location":"kb/google-cloud/projects-resources-iam-users-roles-permissions-apis-and-cloud-shell/#creating-an-instance-though-the-cloud-command-line","text":"gloud compute instances create <name> --machine-type <machine type> --zone <see zone location>","title":"Creating an instance though the cloud command line"},{"location":"kb/google-cloud/serverless-vpc-access-for-cloudrun-across-projects/","text":"Serverless VPC access for Cloudrun across Projects Firstly locate the Project numerical ID, this can be done via terraform using the below resource: Note This will only work if you are creating the project through terraform, otherwise use the Data block google_project.vpc-cloudrun.number Create a network with a /28 subnet resource \"google_compute_subnetwork\" \"custom_test\" { provider = google-beta name = \"vpc-con\" ip_cidr_range = \"10.2.0.0/28\" project = google_project.vpc-host.name network = google_compute_network.vpc_network.name } Now create the connection, and ensure you set the project field otherwise you will have issues resource \"google_vpc_access_connector\" \"connector\" { provider = google-beta region = \"europe-west2\" project = google_project.vpc-cloudrun.name name = \"vpc-conn-test-${random_integer.priority.result}\" max_instances = 3 min_instances = 2 subnet { name = google_compute_subnetwork.custom_test.name project_id = google_project.vpc-host.name } machine_type = \"f1-micro\" # (1)! } Depending on how much network traffic you plan to send over this, the machine type needs to be larger. The default is e2-micro You will need to give the vpc service account editor on the host resource \"google_project_iam_member\" \"project\" { project = google_project.vpc-host.name role = \"roles/editor\" member = \"serviceAccount:${google_project.vpc-cloudrun.number}@cloudservices.gserviceaccount.com\" }","title":"Serverless VPC access for Cloudrun across Projects"},{"location":"kb/google-cloud/serverless-vpc-access-for-cloudrun-across-projects/#serverless-vpc-access-for-cloudrun-across-projects","text":"Firstly locate the Project numerical ID, this can be done via terraform using the below resource: Note This will only work if you are creating the project through terraform, otherwise use the Data block google_project.vpc-cloudrun.number Create a network with a /28 subnet resource \"google_compute_subnetwork\" \"custom_test\" { provider = google-beta name = \"vpc-con\" ip_cidr_range = \"10.2.0.0/28\" project = google_project.vpc-host.name network = google_compute_network.vpc_network.name } Now create the connection, and ensure you set the project field otherwise you will have issues resource \"google_vpc_access_connector\" \"connector\" { provider = google-beta region = \"europe-west2\" project = google_project.vpc-cloudrun.name name = \"vpc-conn-test-${random_integer.priority.result}\" max_instances = 3 min_instances = 2 subnet { name = google_compute_subnetwork.custom_test.name project_id = google_project.vpc-host.name } machine_type = \"f1-micro\" # (1)! } Depending on how much network traffic you plan to send over this, the machine type needs to be larger. The default is e2-micro You will need to give the vpc service account editor on the host resource \"google_project_iam_member\" \"project\" { project = google_project.vpc-host.name role = \"roles/editor\" member = \"serviceAccount:${google_project.vpc-cloudrun.number}@cloudservices.gserviceaccount.com\" }","title":"Serverless VPC access for Cloudrun across Projects"},{"location":"kb/google-workspace/setup-routing/","text":"Google Workspace: Setup routing Go to: Apps > Google Workspace > Gmail Select Gmail Select Default Routing Add the full email address you want to have emails sent to, so if our domain was breadnet.co.uk and we wanted to receive at test123 , we would put test123@breadnet.co.uk Select the below Perform this action on non-recognized and recognized address Select Add more Recipients and fill it out like below: First put the email you want it to go to Then select Basic and change it to Advanced Then select Save","title":"Setup Routing"},{"location":"kb/google-workspace/setup-routing/#google-workspace-setup-routing","text":"Go to: Apps > Google Workspace > Gmail Select Gmail Select Default Routing Add the full email address you want to have emails sent to, so if our domain was breadnet.co.uk and we wanted to receive at test123 , we would put test123@breadnet.co.uk Select the below Perform this action on non-recognized and recognized address Select Add more Recipients and fill it out like below: First put the email you want it to go to Then select Basic and change it to Advanced Then select Save","title":"Google Workspace: Setup routing"},{"location":"kb/helm/create-helm-chart/","text":"Creating a helm chart Pre-requisites helm Installing Helm Brew One liner Other brew install helm curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash See Installing Helm Create a helm chart Simple way Using name of current folder helm create <name> helm create $( basename $( pwd ))","title":"Create a helm chart"},{"location":"kb/helm/create-helm-chart/#creating-a-helm-chart","text":"","title":"Creating a helm chart"},{"location":"kb/helm/create-helm-chart/#pre-requisites","text":"helm","title":"Pre-requisites"},{"location":"kb/helm/create-helm-chart/#installing-helm","text":"Brew One liner Other brew install helm curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash See Installing Helm","title":"Installing Helm"},{"location":"kb/helm/create-helm-chart/#create-a-helm-chart","text":"Simple way Using name of current folder helm create <name> helm create $( basename $( pwd ))","title":"Create a helm chart"},{"location":"kb/helm/force-rollout-on-configmap-update/","text":"Force containers to update when config map changes in helm Why When we create a config map in Kubernetes, it syncs with the container the first time it creates, and then never again. Sometimes in our applications we will update an env on the fly, or use a range block to create infinite ones. How Adding the below in to the deployment.yaml file will cause helm to basically cat configmap.yaml | sha256sum on the rendered template. Base Directory Subdirectory This is for when the helm chart has all the template files in the same direcrory template : metadata : annotations : checksum/config : {{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }} This is for when you have a folder splitting out say, backend and frontend template : metadata : annotations : checksum/config : {{ include (print $.Template.BasePath \"/backend/configmap.yaml\") . | sha256sum }}","title":"Force containers to update when config map changes in helm"},{"location":"kb/helm/force-rollout-on-configmap-update/#force-containers-to-update-when-config-map-changes-in-helm","text":"","title":"Force containers to update when config map changes in helm"},{"location":"kb/helm/force-rollout-on-configmap-update/#why","text":"When we create a config map in Kubernetes, it syncs with the container the first time it creates, and then never again. Sometimes in our applications we will update an env on the fly, or use a range block to create infinite ones.","title":"Why"},{"location":"kb/helm/force-rollout-on-configmap-update/#how","text":"Adding the below in to the deployment.yaml file will cause helm to basically cat configmap.yaml | sha256sum on the rendered template. Base Directory Subdirectory This is for when the helm chart has all the template files in the same direcrory template : metadata : annotations : checksum/config : {{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }} This is for when you have a folder splitting out say, backend and frontend template : metadata : annotations : checksum/config : {{ include (print $.Template.BasePath \"/backend/configmap.yaml\") . | sha256sum }}","title":"How"},{"location":"kb/helm/helm-repo-gcs/","text":"Using GCS as a helm repo We are able to use GCS (Google cloud storage) as a helm repo. This is especially useful where we want to package charts to be used by people who do not always have access to teh repo. Install plugin We need to have the helm gcs plugin to use this helm plugin install https://github.com/hayorov/helm-gcs.git Upgrading Plugin helm plugin update gcs Init the bucket Note We only need to do this once when we create a new bucket. helm gcs init gs://bysd-helmstore Add repo to Helm helm repo add bysd-store gs://bysd-helmstore Push chart to repo helm gcs push chart.tar.gz bysd-store As a biproduct of this, it will update the index.yaml file for you Update the cache helm repo update Remove chart helm gcs rm chart repo-name Further Reading Github","title":"Use GCS as helm repo"},{"location":"kb/helm/helm-repo-gcs/#using-gcs-as-a-helm-repo","text":"We are able to use GCS (Google cloud storage) as a helm repo. This is especially useful where we want to package charts to be used by people who do not always have access to teh repo.","title":"Using GCS as a helm repo"},{"location":"kb/helm/helm-repo-gcs/#install-plugin","text":"We need to have the helm gcs plugin to use this helm plugin install https://github.com/hayorov/helm-gcs.git","title":"Install plugin"},{"location":"kb/helm/helm-repo-gcs/#upgrading-plugin","text":"helm plugin update gcs","title":"Upgrading Plugin"},{"location":"kb/helm/helm-repo-gcs/#init-the-bucket","text":"Note We only need to do this once when we create a new bucket. helm gcs init gs://bysd-helmstore","title":"Init the bucket"},{"location":"kb/helm/helm-repo-gcs/#add-repo-to-helm","text":"helm repo add bysd-store gs://bysd-helmstore","title":"Add repo to Helm"},{"location":"kb/helm/helm-repo-gcs/#push-chart-to-repo","text":"helm gcs push chart.tar.gz bysd-store As a biproduct of this, it will update the index.yaml file for you","title":"Push chart to repo"},{"location":"kb/helm/helm-repo-gcs/#update-the-cache","text":"helm repo update","title":"Update the cache"},{"location":"kb/helm/helm-repo-gcs/#remove-chart","text":"helm gcs rm chart repo-name","title":"Remove chart"},{"location":"kb/helm/helm-repo-gcs/#further-reading","text":"Github","title":"Further Reading"},{"location":"kb/helm/push-chart-to-ar/","text":"Push helm chart to Artifact Registry Pre-requisites You will need to have an Artifact registry repository created Create AR Repo resource \"google_artifact_registry_repository\" \"helm-store\" { repository_id = \"helm-store\" description = \"helm chart store\" format = \"DOCKER\" project = var.project location = var.region labels = { creator = \"bstannard\" use = \"helm-chart-storage\" user = \"bstannard\" } } Update the version field in Chart.yaml Before After apiVersion : v2 name : dummy-demo-chart description : A Helm chart for Kubernetes type : application version : 0.1.0 appVersion : \"1.16.0\" apiVersion : v2 name : dummy-demo-chart description : A Helm chart for Kubernetes type : application version : 0.2.0 appVersion : \"1.16.0\" Once you have bumped the version, you can package it Packaging the chart helm package <path to chart> Push the chart helm push <chart name>-*.tgz oci://europe-west2-docker.pkg.dev/<your GCP project name>/helm-store/ What to read next Using GCS as a helm repo","title":"Push chart to Artifact Registry"},{"location":"kb/helm/push-chart-to-ar/#push-helm-chart-to-artifact-registry","text":"","title":"Push helm chart to Artifact Registry"},{"location":"kb/helm/push-chart-to-ar/#pre-requisites","text":"You will need to have an Artifact registry repository created Create AR Repo resource \"google_artifact_registry_repository\" \"helm-store\" { repository_id = \"helm-store\" description = \"helm chart store\" format = \"DOCKER\" project = var.project location = var.region labels = { creator = \"bstannard\" use = \"helm-chart-storage\" user = \"bstannard\" } } Update the version field in Chart.yaml Before After apiVersion : v2 name : dummy-demo-chart description : A Helm chart for Kubernetes type : application version : 0.1.0 appVersion : \"1.16.0\" apiVersion : v2 name : dummy-demo-chart description : A Helm chart for Kubernetes type : application version : 0.2.0 appVersion : \"1.16.0\" Once you have bumped the version, you can package it","title":"Pre-requisites"},{"location":"kb/helm/push-chart-to-ar/#packaging-the-chart","text":"helm package <path to chart>","title":"Packaging the chart"},{"location":"kb/helm/push-chart-to-ar/#push-the-chart","text":"helm push <chart name>-*.tgz oci://europe-west2-docker.pkg.dev/<your GCP project name>/helm-store/","title":"Push the chart"},{"location":"kb/helm/push-chart-to-ar/#what-to-read-next","text":"Using GCS as a helm repo","title":"What to read next"},{"location":"kb/linux-cli/blc/","text":"Broken link checker What Broken links on a site cause issues. During the migration of bookstack to mkdocs , there were a LOAD of links that broke How We can use a tool like blc to programmatically check for broken links Installing npm install broken-link-checker -g For more information, see stevenvachon/broken-link-checker Using blc https://bookstack.breadnet.co.uk Example output \u279c blc https://bookstack.breadnet.co.uk Getting links from: https://bookstack.breadnet.co.uk/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://squidfunk.github.io/mkdocs-material/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://breadnet.co.uk/favicon.png \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://git-scm.com/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/automation/ansible/basics/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/automation/ansible/python-install/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/automation/airflow/airflow-basics/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/terraform/building-infrastructure/ \u251c\u2500BROKEN\u2500 https://uk.linkedin.com/in/bradley-stannard (HTTP_999) \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/terraform/openstack/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/terraform/terraform-plugin-cannot-locate-module-locally-unknown-reason/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/terraform/google-iap/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://github.com/userbradley/documentation.breadnet.co.uk \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/terraform/tfupdate/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/terraform/recursive-delete-of-terraform/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/terraform/remote-data/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/gcp/curl-to-iap/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/gcp/export-to-terraform-using-gcloud-cli/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/gcp/grafeas/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/aqua/aqua-page-1/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://github.com/userbradley/documentation.breadnet.co.uk/edit/dev/docs/index.md \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/aqua/aqua-page-2/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/aqua/partnership-info/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-architect/gcp-architect-1/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-architect/gcp-architect-2/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-architect/gcp-architect-3/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-architect/kubectl-commands/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-architect/gcp-load-balancer/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-security/1-the-exam-topic/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-security/gcp-security-1/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://github.com/userbradley \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/workspace-admin/user-administration/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/workspace-admin/managing-gsuit/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/workspace-admin/mdm/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-associate/regions-and-zones/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-associate/gcp/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-associate/gce-google-compute-engine/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-associate/networking-and-lb/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-associate/vm-bootstrapping-templates-and-images/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-associate/costs/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-associate/compute-engine/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/cka/cka-1/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/disk-management/expanding-a-filesystem/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/disk-management/formatting-drive-automount/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/disk-management/mount-a-new-drive/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/disk-management/gpt-pmbr-size-mismatch-will-be-corrected-by-write/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/minio/old/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/minio/minio-over-s3fs/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/minio/s3-policies/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/php/install-php/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/php/wordpress-permissions/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/docker/installing-docker/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/docker/basics-of-docker/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/docker/installing-jellyfin/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/docker/docker-intro-and-notes/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/docker/exporting-and-importing/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/docker/docker-architecture/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/docker/bulk-retag/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/google-cloud/projects-resources-iam-users-roles-permissions-apis-and-cloud-shell/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/google-cloud/serverless-vpc-access-for-cloudrun-across-projects/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/google-cloud/api-get-project/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/google-cloud/api-get-sa/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/uninstall-netplan/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/dns-on-ubuntu/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/bringing-up-interfaces/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/netplan-2-interfaces/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/centos-iptables/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/null-routing/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/listen-on-a-port/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/list-of-unique-ips/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/connections-on-a-port/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/rvc-ip-range/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/ssh/ssh-keys/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/ssh/ssh-port-redirection/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/ssh/ssh-client-setup-using-keys/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/ssh/importing-ssh-keys-from-github/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/ssh/weird-bash/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/ssh/sshuttle/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/cachet/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/certbot/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/reverse-web-proxy/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/bookstack/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/jellyfin-s3/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/custom-headers/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/remove-server-headers/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/wildcard-certificates/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/send-test-email-on-passbolt/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/check-passbolt-is-healthy/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/installed-applications-reverse/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/nginxservice-failed-because-the-control-process-exited/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/other/downloading-the-breadnet-site-and-serving-a-stale-copy/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/wasabi/policies/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/wasabi/transport-endpoint-is-not-connected/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/authentication/fingerprint-on-linux-mint/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/aws/cloud-init-sg1/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/aws/aws-cli/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/google-workspace/setup-routing/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-cli/uuidgen-lowercase/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/networking/vpn-network-routing-mikrotik/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/networking/update-tough-switch/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/cloud/cloud-init/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/markdown/github-repos-and-stuff/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/markdown/custom-badges/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/markdown/built-by-badge/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/kubernetes/connect-to-container-that-has-sidecars/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/kubernetes/deleting-not-running-pods/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/kubernetes/kubectl-set-namespace/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/kubernetes/rbac-testing/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/kubernetes/workload-id-test/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/kubernetes/sleeper/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/home/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/home/cu/consumer-unit/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/home/oven/oven/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/home/washing/washing-machine/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://breadnet.co.uk/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://bradley.breadnet.co.uk/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://kubernetes.breadnet.co.uk/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/mkdocs.org/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/assets/pipeline.png \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cdn-cgi/l/email-protection#bccbd9ded1ddcfc8d9cefcdeced9ddd8d2d9c892dfd392c9d7","title":"Broken link checker"},{"location":"kb/linux-cli/blc/#broken-link-checker","text":"","title":"Broken link checker"},{"location":"kb/linux-cli/blc/#what","text":"Broken links on a site cause issues. During the migration of bookstack to mkdocs , there were a LOAD of links that broke","title":"What"},{"location":"kb/linux-cli/blc/#how","text":"We can use a tool like blc to programmatically check for broken links","title":"How"},{"location":"kb/linux-cli/blc/#installing","text":"npm install broken-link-checker -g For more information, see stevenvachon/broken-link-checker","title":"Installing"},{"location":"kb/linux-cli/blc/#using","text":"blc https://bookstack.breadnet.co.uk","title":"Using"},{"location":"kb/linux-cli/blc/#example-output","text":"\u279c blc https://bookstack.breadnet.co.uk Getting links from: https://bookstack.breadnet.co.uk/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://squidfunk.github.io/mkdocs-material/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://breadnet.co.uk/favicon.png \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://git-scm.com/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/automation/ansible/basics/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/automation/ansible/python-install/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/automation/airflow/airflow-basics/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/terraform/building-infrastructure/ \u251c\u2500BROKEN\u2500 https://uk.linkedin.com/in/bradley-stannard (HTTP_999) \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/terraform/openstack/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/terraform/terraform-plugin-cannot-locate-module-locally-unknown-reason/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/terraform/google-iap/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://github.com/userbradley/documentation.breadnet.co.uk \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/terraform/tfupdate/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/terraform/recursive-delete-of-terraform/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/terraform/remote-data/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/gcp/curl-to-iap/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/gcp/export-to-terraform-using-gcloud-cli/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/gcp/grafeas/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/aqua/aqua-page-1/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://github.com/userbradley/documentation.breadnet.co.uk/edit/dev/docs/index.md \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/aqua/aqua-page-2/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cloud/aqua/partnership-info/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-architect/gcp-architect-1/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-architect/gcp-architect-2/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-architect/gcp-architect-3/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-architect/kubectl-commands/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-architect/gcp-load-balancer/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-security/1-the-exam-topic/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-security/gcp-security-1/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://github.com/userbradley \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/workspace-admin/user-administration/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/workspace-admin/managing-gsuit/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/workspace-admin/mdm/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-associate/regions-and-zones/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-associate/gcp/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-associate/gce-google-compute-engine/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-associate/networking-and-lb/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-associate/vm-bootstrapping-templates-and-images/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-associate/costs/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/google-certs/gcp-associate/compute-engine/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/certifications/cka/cka-1/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/disk-management/expanding-a-filesystem/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/disk-management/formatting-drive-automount/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/disk-management/mount-a-new-drive/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/disk-management/gpt-pmbr-size-mismatch-will-be-corrected-by-write/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/minio/old/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/minio/minio-over-s3fs/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/minio/s3-policies/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/php/install-php/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/php/wordpress-permissions/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/docker/installing-docker/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/docker/basics-of-docker/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/docker/installing-jellyfin/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/docker/docker-intro-and-notes/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/docker/exporting-and-importing/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/docker/docker-architecture/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/docker/bulk-retag/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/google-cloud/projects-resources-iam-users-roles-permissions-apis-and-cloud-shell/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/google-cloud/serverless-vpc-access-for-cloudrun-across-projects/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/google-cloud/api-get-project/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/google-cloud/api-get-sa/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/uninstall-netplan/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/dns-on-ubuntu/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/bringing-up-interfaces/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/netplan-2-interfaces/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/centos-iptables/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/null-routing/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/listen-on-a-port/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/list-of-unique-ips/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/connections-on-a-port/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-networking/rvc-ip-range/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/ssh/ssh-keys/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/ssh/ssh-port-redirection/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/ssh/ssh-client-setup-using-keys/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/ssh/importing-ssh-keys-from-github/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/ssh/weird-bash/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/ssh/sshuttle/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/cachet/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/certbot/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/reverse-web-proxy/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/bookstack/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/jellyfin-s3/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/custom-headers/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/remove-server-headers/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/wildcard-certificates/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/send-test-email-on-passbolt/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/check-passbolt-is-healthy/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/installed-applications-reverse/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/public-web-facing/nginxservice-failed-because-the-control-process-exited/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/other/downloading-the-breadnet-site-and-serving-a-stale-copy/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/wasabi/policies/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/wasabi/transport-endpoint-is-not-connected/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/authentication/fingerprint-on-linux-mint/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/aws/cloud-init-sg1/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/aws/aws-cli/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/google-workspace/setup-routing/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/linux-cli/uuidgen-lowercase/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/networking/vpn-network-routing-mikrotik/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/networking/update-tough-switch/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/cloud/cloud-init/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/markdown/github-repos-and-stuff/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/markdown/custom-badges/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/markdown/built-by-badge/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/kubernetes/connect-to-container-that-has-sidecars/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/kubernetes/deleting-not-running-pods/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/kubernetes/kubectl-set-namespace/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/kubernetes/rbac-testing/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/kubernetes/workload-id-test/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/kb/kubernetes/sleeper/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/home/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/home/cu/consumer-unit/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/home/oven/oven/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/home/washing/washing-machine/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://breadnet.co.uk/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://bradley.breadnet.co.uk/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://kubernetes.breadnet.co.uk/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/mkdocs.org/ \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/assets/pipeline.png \u251c\u2500\u2500\u2500OK\u2500\u2500\u2500 https://documentation.breadnet.co.uk/cdn-cgi/l/email-protection#bccbd9ded1ddcfc8d9cefcdeced9ddd8d2d9c892dfd392c9d7","title":"Example output"},{"location":"kb/linux-cli/fingerprint-for-sudo-mac/","text":"Use fingerprint as sudo on mac Why Using sudo on any linux device can raise the risk level substantially. Using your fingerprint already enrolled on the mac prevents someone from being able to type your password in when you're not around How Edit the below file in your favourite editor Nano Vi/Vim sudo nano /etc/pam.d/sudo sudo vi /etc/pam.d/sud The file /etc/pam.d/sudo should look something like below # sudo: auth account password session auth sufficient pam_smartcard.so auth required pam_opendirectory.so account required pam_permit.so password required pam_deny.so session required pam_permit.so Add the below line just under # sudo: auth account password session auth sufficient pam_tid.so Your file should now look like the below # sudo: auth account password session auth sufficient pam_tid.so auth sufficient pam_smartcard.so auth required pam_opendirectory.so account required pam_permit.so password required pam_deny.so session required pam_permit.so Save the file using the below commands Nano Vi/Vim Control x + y + enter Escape + :wq! + enter","title":"Passwordless sudo using fingerprint on mac"},{"location":"kb/linux-cli/fingerprint-for-sudo-mac/#use-fingerprint-as-sudo-on-mac","text":"","title":"Use fingerprint as sudo on mac"},{"location":"kb/linux-cli/fingerprint-for-sudo-mac/#why","text":"Using sudo on any linux device can raise the risk level substantially. Using your fingerprint already enrolled on the mac prevents someone from being able to type your password in when you're not around","title":"Why"},{"location":"kb/linux-cli/fingerprint-for-sudo-mac/#how","text":"Edit the below file in your favourite editor Nano Vi/Vim sudo nano /etc/pam.d/sudo sudo vi /etc/pam.d/sud The file /etc/pam.d/sudo should look something like below # sudo: auth account password session auth sufficient pam_smartcard.so auth required pam_opendirectory.so account required pam_permit.so password required pam_deny.so session required pam_permit.so Add the below line just under # sudo: auth account password session auth sufficient pam_tid.so Your file should now look like the below # sudo: auth account password session auth sufficient pam_tid.so auth sufficient pam_smartcard.so auth required pam_opendirectory.so account required pam_permit.so password required pam_deny.so session required pam_permit.so Save the file using the below commands Nano Vi/Vim Control x + y + enter Escape + :wq! + enter","title":"How"},{"location":"kb/linux-cli/get-current-folder/","text":"Get current folder Sometimes, we need to get the current folder, for things like Creating helm charts at work The problem Usually we are able to do something like pwd but this returns the entire working directory Solution Use basename basename $( pwd ) Outcome documentation.breadnet.co.uk","title":"Get current Folder"},{"location":"kb/linux-cli/get-current-folder/#get-current-folder","text":"Sometimes, we need to get the current folder, for things like Creating helm charts at work","title":"Get current folder"},{"location":"kb/linux-cli/get-current-folder/#the-problem","text":"Usually we are able to do something like pwd but this returns the entire working directory","title":"The problem"},{"location":"kb/linux-cli/get-current-folder/#solution","text":"Use basename basename $( pwd ) Outcome documentation.breadnet.co.uk","title":"Solution"},{"location":"kb/linux-cli/the-following-signatures-couldnt-be-verified-because-the-public-key-is-not-available/","text":"The following signatures couldn't be verified because the public key is not available I got this error when installing Unifi root@unifi-01:~# sudo apt-get update Err:5 https://dl.ubnt.com/unifi/debian stable InRelease The following signatures couldn't be verified because the public key is not available : NO_PUBKEY 06E85760C0A52C50 # (1) Reading package lists... Done W : GPG error : https://dl.ubnt.com/unifi/debian stable InRelease : The following signatures couldn't be verified because the public key is not available : NO_PUBKEY 06E85760C0A52C50 E : The repository 'https://www.ui.com/downloads/unifi/debian stable InRelease' is not signed. N : Updating from such a repository can't be done securely, and is therefore disabled by default. N : See apt-secure(8) manpage for repository creation and user configuration details. This is the line we need to take note of Specifically the line The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 06E85760C0A52C50 What's the issue? The Key was requested, but could not be found on the system. Solution Make a note of the keyID we need - This is the section after NO_PUBKEY NO_PUBKEY 06E85760C0A52C50 Run the below to pull the keys from keyserver.ubuntu.com apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 06E85760C0A52C50 Replace the key with the one that has errored for you","title":"The following signatures couldn't be verified because the public key is not available"},{"location":"kb/linux-cli/the-following-signatures-couldnt-be-verified-because-the-public-key-is-not-available/#the-following-signatures-couldnt-be-verified-because-the-public-key-is-not-available","text":"I got this error when installing Unifi root@unifi-01:~# sudo apt-get update Err:5 https://dl.ubnt.com/unifi/debian stable InRelease The following signatures couldn't be verified because the public key is not available : NO_PUBKEY 06E85760C0A52C50 # (1) Reading package lists... Done W : GPG error : https://dl.ubnt.com/unifi/debian stable InRelease : The following signatures couldn't be verified because the public key is not available : NO_PUBKEY 06E85760C0A52C50 E : The repository 'https://www.ui.com/downloads/unifi/debian stable InRelease' is not signed. N : Updating from such a repository can't be done securely, and is therefore disabled by default. N : See apt-secure(8) manpage for repository creation and user configuration details. This is the line we need to take note of Specifically the line The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 06E85760C0A52C50","title":"The following signatures couldn't be verified because the public key is not available"},{"location":"kb/linux-cli/the-following-signatures-couldnt-be-verified-because-the-public-key-is-not-available/#whats-the-issue","text":"The Key was requested, but could not be found on the system.","title":"What's the issue?"},{"location":"kb/linux-cli/the-following-signatures-couldnt-be-verified-because-the-public-key-is-not-available/#solution","text":"Make a note of the keyID we need - This is the section after NO_PUBKEY NO_PUBKEY 06E85760C0A52C50 Run the below to pull the keys from keyserver.ubuntu.com apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 06E85760C0A52C50 Replace the key with the one that has errored for you","title":"Solution"},{"location":"kb/linux-cli/uuidgen-lowercase/","text":"uuidgen Lowercase Why we need this If you're instaling Submariner you need to create a UUID for the pods getting deployed as part of one of their commands in the Quick start guide alias uuidgen = 'uuidgen | tr \"[:upper:]\" \"[:lower:]\"'","title":"uuidgen Lower case"},{"location":"kb/linux-cli/uuidgen-lowercase/#uuidgen-lowercase","text":"Why we need this If you're instaling Submariner you need to create a UUID for the pods getting deployed as part of one of their commands in the Quick start guide alias uuidgen = 'uuidgen | tr \"[:upper:]\" \"[:lower:]\"'","title":"uuidgen Lowercase"},{"location":"kb/linux-cli/xcrun-error-invalid-active-developer-path/","text":"xcrun: error: invalid active developer path What this is This is an issue after an update on a mac from one version to next Major version. I my case it was from Monterey 12.6.1 to Ventura Version 13.0 How to resolve Install xcode xcode-select --install If you can't click install Sometimes the TOS box does not display, so pan out (3 finger swipte up) and you will see it. Annoyingly the installing box does not fade to the background, and stays in focus the entire time Still not working Failing that you can also try the below sudo xcode-select --reset","title":"xcrun: error: invalid active developer path"},{"location":"kb/linux-cli/xcrun-error-invalid-active-developer-path/#xcrun-error-invalid-active-developer-path","text":"","title":"xcrun: error: invalid active developer path"},{"location":"kb/linux-cli/xcrun-error-invalid-active-developer-path/#what-this-is","text":"This is an issue after an update on a mac from one version to next Major version. I my case it was from Monterey 12.6.1 to Ventura Version 13.0","title":"What this is"},{"location":"kb/linux-cli/xcrun-error-invalid-active-developer-path/#how-to-resolve","text":"","title":"How to resolve"},{"location":"kb/linux-cli/xcrun-error-invalid-active-developer-path/#install-xcode","text":"xcode-select --install If you can't click install Sometimes the TOS box does not display, so pan out (3 finger swipte up) and you will see it. Annoyingly the installing box does not fade to the background, and stays in focus the entire time","title":"Install xcode"},{"location":"kb/linux-cli/xcrun-error-invalid-active-developer-path/#still-not-working","text":"Failing that you can also try the below sudo xcode-select --reset","title":"Still not working"},{"location":"kb/linux-cli/your-xcode-is-too-outdated/","text":"Your Xcode is too outdated. Why This is an error that comes up after a major upgrade You will see the below error Error: Your Xcode (14.0.1) is too outdated. Please update to Xcode 14.1 (or delete it). Xcode can be updated from the App Store. Error: Your Command Line Tools (CLT) does not support macOS 13. It is either outdated or was modified. Please update your Command Line Tools (CLT) or delete it if no updates are available. Update them from Software Update in System Preferences. If that doesn't show you any updates, run: sudo rm -rf /Library/Developer/CommandLineTools sudo xcode-select --install Alternatively, manually download them from: https://developer.apple.com/download/all/. You should download the Command Line Tools for Xcode 14.1. How to fix System Preferences What to do if there's no updates Continue to Command line fix Open System preferences on the Mac Go to Software update Check for Updates Install Updates Command Line fix sudo rm -rf /Library/Developer/CommandLineTools sudo xcode-select --install A pop-up should display like the below Click install then 3 finger swipe up and click it again. It should display the below now Still getting the error Open xcode and check the version: Open xcode Click Xcode on the top bar Click about Xcode If the version is not the latest version ( 14.1 at time of writing) - You will need to do the below Update Command line tools for xcode Open terminal softwareupdate --list This should hoprefully show the below \u279c softwareupdate --list Software Update Tool Finding available software Software Update found the following new or updated software: * Label: Command Line Tools for Xcode-14.0 Title: Command Line Tools for Xcode, Version: 14.0, Size: 687109KiB, Recommended: YES, Upgrade it with: softwareupdate --install --all Still not working If it's still not working, move xcode to trash, reboot and re-install it Possible side effects XCRUN XCRUN may become unavailable during the installation Do not worry, it will come back once it's installed What to read next xcrun error: invalid active path","title":"Your Xcode is too outdated."},{"location":"kb/linux-cli/your-xcode-is-too-outdated/#your-xcode-is-too-outdated","text":"","title":"Your Xcode is too outdated."},{"location":"kb/linux-cli/your-xcode-is-too-outdated/#why","text":"This is an error that comes up after a major upgrade You will see the below error Error: Your Xcode (14.0.1) is too outdated. Please update to Xcode 14.1 (or delete it). Xcode can be updated from the App Store. Error: Your Command Line Tools (CLT) does not support macOS 13. It is either outdated or was modified. Please update your Command Line Tools (CLT) or delete it if no updates are available. Update them from Software Update in System Preferences. If that doesn't show you any updates, run: sudo rm -rf /Library/Developer/CommandLineTools sudo xcode-select --install Alternatively, manually download them from: https://developer.apple.com/download/all/. You should download the Command Line Tools for Xcode 14.1.","title":"Why"},{"location":"kb/linux-cli/your-xcode-is-too-outdated/#how-to-fix","text":"","title":"How to fix"},{"location":"kb/linux-cli/your-xcode-is-too-outdated/#system-preferences","text":"What to do if there's no updates Continue to Command line fix Open System preferences on the Mac Go to Software update Check for Updates Install Updates","title":"System Preferences"},{"location":"kb/linux-cli/your-xcode-is-too-outdated/#command-line-fix","text":"sudo rm -rf /Library/Developer/CommandLineTools sudo xcode-select --install A pop-up should display like the below Click install then 3 finger swipe up and click it again. It should display the below now","title":"Command Line fix"},{"location":"kb/linux-cli/your-xcode-is-too-outdated/#still-getting-the-error","text":"Open xcode and check the version: Open xcode Click Xcode on the top bar Click about Xcode If the version is not the latest version ( 14.1 at time of writing) - You will need to do the below","title":"Still getting the error"},{"location":"kb/linux-cli/your-xcode-is-too-outdated/#update-command-line-tools-for-xcode","text":"Open terminal softwareupdate --list This should hoprefully show the below \u279c softwareupdate --list Software Update Tool Finding available software Software Update found the following new or updated software: * Label: Command Line Tools for Xcode-14.0 Title: Command Line Tools for Xcode, Version: 14.0, Size: 687109KiB, Recommended: YES, Upgrade it with: softwareupdate --install --all","title":"Update Command line tools for xcode"},{"location":"kb/linux-cli/your-xcode-is-too-outdated/#still-not-working","text":"If it's still not working, move xcode to trash, reboot and re-install it","title":"Still not working"},{"location":"kb/linux-cli/your-xcode-is-too-outdated/#possible-side-effects","text":"","title":"Possible side effects"},{"location":"kb/linux-cli/your-xcode-is-too-outdated/#xcrun","text":"XCRUN may become unavailable during the installation Do not worry, it will come back once it's installed","title":"XCRUN"},{"location":"kb/linux-cli/your-xcode-is-too-outdated/#what-to-read-next","text":"xcrun error: invalid active path","title":"What to read next"},{"location":"kb/linux-networking/bringing-up-interfaces/","text":"Bringing up Interfaces Bringing interfaces up/down using ip Usage: ip link set dev <interface> up ip link set dev <interface> down Example: ip link set dev eth0 up ip link set dev eth0 down","title":"Bringing up Interfaces"},{"location":"kb/linux-networking/bringing-up-interfaces/#bringing-up-interfaces","text":"","title":"Bringing up Interfaces"},{"location":"kb/linux-networking/bringing-up-interfaces/#bringing-interfaces-updown","text":"","title":"Bringing interfaces up/down"},{"location":"kb/linux-networking/bringing-up-interfaces/#using-ip","text":"Usage: ip link set dev <interface> up ip link set dev <interface> down Example: ip link set dev eth0 up ip link set dev eth0 down","title":"using ip"},{"location":"kb/linux-networking/centos-iptables/","text":"Centos Iptables systemctl stop firewalld systemctl disable firewalld yum -y install iptables-services nano /etc/sysconfig/iptables systemctl restart iptables systemctl enable iptables","title":"Centos Iptables"},{"location":"kb/linux-networking/centos-iptables/#centos-iptables","text":"systemctl stop firewalld systemctl disable firewalld yum -y install iptables-services nano /etc/sysconfig/iptables systemctl restart iptables systemctl enable iptables","title":"Centos Iptables"},{"location":"kb/linux-networking/connections-on-a-port/","text":"Connections on a port netstat -tn 2 >/dev/null | grep :80 | awk '{print $5}' | cut -d: -f1 | sort | uniq -c | sort -nr | head netstat -anpe | grep \"80\" | grep \"LISTEN\"","title":"Connections on a port"},{"location":"kb/linux-networking/connections-on-a-port/#connections-on-a-port","text":"netstat -tn 2 >/dev/null | grep :80 | awk '{print $5}' | cut -d: -f1 | sort | uniq -c | sort -nr | head netstat -anpe | grep \"80\" | grep \"LISTEN\"","title":"Connections on a port"},{"location":"kb/linux-networking/dns-on-ubuntu/","text":"DNS on Ubuntu Install the resolvconf package. sudo apt install resolvconf Edit /etc/resolvconf/resolv.conf.d/head and add the following: # Make edits to /etc/resolvconf/resolv.conf.d/head. nameserver 8 .8.4.4 nameserver 8 .8.8.8 Restart the resolvconf service. sudo service resolvconf restart Fix should be permanent.","title":"DNS on Ubuntu"},{"location":"kb/linux-networking/dns-on-ubuntu/#dns-on-ubuntu","text":"Install the resolvconf package. sudo apt install resolvconf Edit /etc/resolvconf/resolv.conf.d/head and add the following: # Make edits to /etc/resolvconf/resolv.conf.d/head. nameserver 8 .8.4.4 nameserver 8 .8.8.8 Restart the resolvconf service. sudo service resolvconf restart Fix should be permanent.","title":"DNS on Ubuntu"},{"location":"kb/linux-networking/list-of-unique-ips/","text":"List of unique IP's awk '{print $1}' access.log | sort | uniq -c | sort -nr Funny story I had an AWS role interview, and this was what they were after. I actually wrote this page 8 months ago (2021)","title":"List of unique IP's"},{"location":"kb/linux-networking/list-of-unique-ips/#list-of-unique-ips","text":"awk '{print $1}' access.log | sort | uniq -c | sort -nr Funny story I had an AWS role interview, and this was what they were after. I actually wrote this page 8 months ago (2021)","title":"List of unique IP's"},{"location":"kb/linux-networking/listen-on-a-port/","text":"Listen on a port We can listen to connections coming in on a port using nc nc -lnvk 8080","title":"Listen on a port"},{"location":"kb/linux-networking/listen-on-a-port/#listen-on-a-port","text":"We can listen to connections coming in on a port using nc nc -lnvk 8080","title":"Listen on a port"},{"location":"kb/linux-networking/netplan-2-interfaces/","text":"Netplan 2 interfaces Edit the file in /etc/netplan/ and add the below: Make sure to back up the file! network : ethernets : eth0 : dhcp4 : true eth1 : dhcp4 : true version : 2 Worth looking here for additional info","title":"Netplan 2 interfaces"},{"location":"kb/linux-networking/netplan-2-interfaces/#netplan-2-interfaces","text":"Edit the file in /etc/netplan/ and add the below: Make sure to back up the file! network : ethernets : eth0 : dhcp4 : true eth1 : dhcp4 : true version : 2 Worth looking here for additional info","title":"Netplan 2 interfaces"},{"location":"kb/linux-networking/nmap-scans/","text":"Nmap scanning commands Cool things to note You can press space to view the status Generic scan nmap -Pn <ip> Scans top 1000 port, but will not enumerate past synack Service Enumeration Top 1000 ports Specific Port All Ports nmap -sC -sV <ip> nmap -sC -sV -p 80 <ip> nmap -p- <ip>","title":"Nmap scanning"},{"location":"kb/linux-networking/nmap-scans/#nmap-scanning-commands","text":"","title":"Nmap scanning commands"},{"location":"kb/linux-networking/nmap-scans/#cool-things-to-note","text":"You can press space to view the status","title":"Cool things to note"},{"location":"kb/linux-networking/nmap-scans/#generic-scan","text":"nmap -Pn <ip> Scans top 1000 port, but will not enumerate past synack","title":"Generic scan"},{"location":"kb/linux-networking/nmap-scans/#service-enumeration","text":"Top 1000 ports Specific Port All Ports nmap -sC -sV <ip> nmap -sC -sV -p 80 <ip> nmap -p- <ip>","title":"Service Enumeration"},{"location":"kb/linux-networking/null-routing/","text":"Null routing route add -net <subnet> gw 127 .0.0.1 lo Note Just remove -net when only doing one IP","title":"Null routing"},{"location":"kb/linux-networking/null-routing/#null-routing","text":"route add -net <subnet> gw 127 .0.0.1 lo Note Just remove -net when only doing one IP","title":"Null routing"},{"location":"kb/linux-networking/rvc-ip-range/","text":"RVC IP range RVC Master IP range: 195.194.217.0/25 Outbound IP for Eduroam : 195.195.217.140/32 Host 0.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 1.217.195.195.in-addr.arpa domain name pointer dis.rvc.ac.uk. 2.217.195.195.in-addr.arpa domain name pointer www.vetschools.ac.uk. 2.217.195.195.in-addr.arpa domain name pointer supervets.rvc.ac.uk. 2.217.195.195.in-addr.arpa domain name pointer www.supervets.rvc.ac.uk. Host 3.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 4.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 5.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 6.217.195.195.in-addr.arpa domain name pointer www.ectp.eu.com. 7.217.195.195.in-addr.arpa domain name pointer ns1.rvc.ac.uk. 8.217.195.195.in-addr.arpa domain name pointer rotasafevaccine.rvc.ac.uk. 9.217.195.195.in-addr.arpa domain name pointer www.nobarriers2uni.org. 10.217.195.195.in-addr.arpa domain name pointer act.rvc.ac.uk. Host 11.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 12.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 13.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 14.217.195.195.in-addr.arpa domain name pointer www.careersinhealth.org.uk. 14.217.195.195.in-addr.arpa domain name pointer researchpubs.rvc.ac.uk. 14.217.195.195.in-addr.arpa domain name pointer evedna.rvc.ac.uk. 14.217.195.195.in-addr.arpa domain name pointer ferrets.rvc.ac.uk. 15.217.195.195.in-addr.arpa domain name pointer ahchemots.rvc.ac.uk. 15.217.195.195.in-addr.arpa domain name pointer londonahots.rvc.ac.uk. 15.217.195.195.in-addr.arpa domain name pointer library.rvc.ac.uk. 15.217.195.195.in-addr.arpa domain name pointer ahsportsots.rvc.ac.uk. 15.217.195.195.in-addr.arpa domain name pointer hkvn.rvc.ac.uk. 15.217.195.195.in-addr.arpa domain name pointer ahartsots.rvc.ac.uk. Host 16.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 17.217.195.195.in-addr.arpa domain name pointer www.asvin.ac.uk. 17.217.195.195.in-addr.arpa domain name pointer oliver.live.ac.uk. 18.217.195.195.in-addr.arpa domain name pointer www.mypad.rvc.ac.uk. 18.217.195.195.in-addr.arpa domain name pointer pdp.rvc.ac.uk. 19.217.195.195.in-addr.arpa domain name pointer nurses.vetschools.ac.uk. Host 20.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 21.217.195.195.in-addr.arpa domain name pointer www.eclw.ac.uk. Host 22.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 23.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 24.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 25.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 26.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 27.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 28.217.195.195.in-addr.arpa domain name pointer reviews.rvc.ac.uk. Host 29.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 30.217.195.195.in-addr.arpa domain name pointer cmrdweb.rvc.ac.uk. 30.217.195.195.in-addr.arpa domain name pointer cmrdweb01.rvc.ac.uk. 30.217.195.195.in-addr.arpa domain name pointer rdweb.rvc.ac.uk. 30.217.195.195.in-addr.arpa domain name pointer cmgw.rvc.ac.uk. Host 31.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 32.217.195.195.in-addr.arpa domain name pointer fr.wikivet.net. 32.217.195.195.in-addr.arpa domain name pointer mailing.wikivet.net. 32.217.195.195.in-addr.arpa domain name pointer survey.wikivet.net. 32.217.195.195.in-addr.arpa domain name pointer en.wikivet.net. 32.217.195.195.in-addr.arpa domain name pointer es.wikivet.net. 32.217.195.195.in-addr.arpa domain name pointer wikivet.net. 32.217.195.195.in-addr.arpa domain name pointer www.onlineveterinaryanatomy.net. 32.217.195.195.in-addr.arpa domain name pointer newsletter.wikivet.net. 32.217.195.195.in-addr.arpa domain name pointer stream2.rvc.ac.uk. 32.217.195.195.in-addr.arpa domain name pointer dev.wikivet.net. 32.217.195.195.in-addr.arpa domain name pointer commons.wikivet.net. Host 33.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 34.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 35.217.195.195.in-addr.arpa domain name pointer blackboard2.rvc.ac.uk. 35.217.195.195.in-addr.arpa domain name pointer blackboard.rvc.ac.uk. Host 36.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 37.217.195.195.in-addr.arpa domain name pointer auth1.rvc.ac.uk. Host 38.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 39.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 40.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 41.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 42.217.195.195.in-addr.arpa domain name pointer owaop.rvc.ac.uk. 42.217.195.195.in-addr.arpa domain name pointer live.ac.uk. 42.217.195.195.in-addr.arpa domain name pointer cmex01.rvc.ac.uk. 43.217.195.195.in-addr.arpa domain name pointer cmex02.rvc.ac.uk. 43.217.195.195.in-addr.arpa domain name pointer owaop.rvc.ac.uk. Host 44.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 45.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 46.217.195.195.in-addr.arpa domain name pointer ispring.rvc.ac.uk. 47.217.195.195.in-addr.arpa domain name pointer www.asfnetwork.org. Host 48.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 49.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 50.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 51.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 52.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 53.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 54.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 55.217.195.195.in-addr.arpa domain name pointer myaccount.rvc.ac.uk. Host 56.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 57.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 58.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 59.217.195.195.in-addr.arpa domain name pointer mobilett.rvc.ac.uk. Host 60.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 61.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 62.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 63.217.195.195.in-addr.arpa domain name pointer cmcise.rvc.ac.uk. Host 64.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 65.217.195.195.in-addr.arpa domain name pointer mdp.rvc.ac.uk. 66.217.195.195.in-addr.arpa domain name pointer cmisepsn1.rvc.ac.uk. 67.217.195.195.in-addr.arpa domain name pointer cmisepsn2.rvc.ac.uk. 68.217.195.195.in-addr.arpa domain name pointer vetcompass-listy.api-test.rvc.ac.uk. 68.217.195.195.in-addr.arpa domain name pointer pwm.rvc.ac.uk. 68.217.195.195.in-addr.arpa domain name pointer student-data.api.rvc.ac.uk. 68.217.195.195.in-addr.arpa domain name pointer vetcompass-listy.api.rvc.ac.uk. 68.217.195.195.in-addr.arpa domain name pointer oss.rvc.ac.uk. 68.217.195.195.in-addr.arpa domain name pointer maintenance.rvc.ac.uk. 68.217.195.195.in-addr.arpa domain name pointer agrl.rvc.ac.uk. 68.217.195.195.in-addr.arpa domain name pointer photos-proxy.api.rvc.ac.uk. 68.217.195.195.in-addr.arpa domain name pointer gradebook.rvc.ac.uk. Host 69.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 70.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 71.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 72.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 73.217.195.195.in-addr.arpa domain name pointer rvchpc1.rvc.ac.uk. 74.217.195.195.in-addr.arpa domain name pointer rvchpcservice1.rvc.ac.uk. Host 75.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 76.217.195.195.in-addr.arpa domain name pointer jobs.rvc.ac.uk. 76.217.195.195.in-addr.arpa domain name pointer jobstest.rvc.ac.uk. 77.217.195.195.in-addr.arpa domain name pointer library.rvc.ac.uk. 78.217.195.195.in-addr.arpa domain name pointer digitalslides.rvc.ac.uk. Host 79.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 80.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 81.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 82.217.195.195.in-addr.arpa domain name pointer rx.api.rvc.ac.uk. 83.217.195.195.in-addr.arpa domain name pointer idp.rvc.ac.uk. Host 84.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 85.217.195.195.in-addr.arpa domain name pointer vct-cpd.rvc.ac.uk. Host 86.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 87.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 88.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 89.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 90.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 91.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 92.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 93.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 94.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 95.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 96.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 97.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 98.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 99.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 100.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 101.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 102.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 103.217.195.195.in-addr.arpa domain name pointer cm-mskms-xx01.rvc.ac.uk. 104.217.195.195.in-addr.arpa domain name pointer redcap.rvc.ac.uk. Host 105.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 106.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 107.217.195.195.in-addr.arpa domain name pointer dynamicslive.rvc.ac.uk. 108.217.195.195.in-addr.arpa domain name pointer dynamicsdev.rvc.ac.uk. 109.217.195.195.in-addr.arpa domain name pointer imagebank.rvc.ac.uk. 109.217.195.195.in-addr.arpa domain name pointer assetbank.rvc.ac.uk. 109.217.195.195.in-addr.arpa domain name pointer images.rvc.ac.uk. Host 110.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 111.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 112.217.195.195.in-addr.arpa domain name pointer registry.rvc.ac.uk. 113.217.195.195.in-addr.arpa domain name pointer clobba.rvc.ac.uk. Host 114.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 115.217.195.195.in-addr.arpa domain name pointer cmsbc01.rvc.ac.uk. Host 116.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 117.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 118.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 119.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 120.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 121.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 122.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 123.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 124.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 125.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 126.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 127.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 128.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 129.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 130.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 131.217.195.195.in-addr.arpa domain name pointer hhw2k3lib01.rvc.ac.uk. 132.217.195.195.in-addr.arpa domain name pointer www.vetcompass.com. 132.217.195.195.in-addr.arpa domain name pointer www.vetcompass.org. 132.217.195.195.in-addr.arpa domain name pointer vetcompass.org.nz. 132.217.195.195.in-addr.arpa domain name pointer identity.vetcompass.org. 132.217.195.195.in-addr.arpa domain name pointer cris.api.rvc.ac.uk. 132.217.195.195.in-addr.arpa domain name pointer vetcompass.rvc.ac.uk. 132.217.195.195.in-addr.arpa domain name pointer eunity.api.rvc.ac.uk. 132.217.195.195.in-addr.arpa domain name pointer vetcompass.org. 132.217.195.195.in-addr.arpa domain name pointer www.vetcompass.org.nz. 132.217.195.195.in-addr.arpa domain name pointer vetcompass.com. 132.217.195.195.in-addr.arpa domain name pointer cris.rvc.ac.uk. 133.217.195.195.in-addr.arpa domain name pointer test.vetcompass.org. 133.217.195.195.in-addr.arpa domain name pointer identitytest.vetcompass.org. Host 134.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 135.217.195.195.in-addr.arpa domain name pointer thankq.rvc.ac.uk. 136.217.195.195.in-addr.arpa domain name pointer exr.rvc.ac.uk. Host 137.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 138.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 139.217.195.195.in-addr.arpa domain name pointer eunity.rvc.ac.uk. 140.217.195.195.in-addr.arpa domain name pointer vpn.rvc.ac.uk. Host 141.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 142.217.195.195.in-addr.arpa domain name pointer topdesk.rvc.ac.uk. 142.217.195.195.in-addr.arpa domain name pointer animalaspirations.org. 142.217.195.195.in-addr.arpa domain name pointer library.rvc.ac.uk. 142.217.195.195.in-addr.arpa domain name pointer worktribe.rvc.ac.uk. 142.217.195.195.in-addr.arpa domain name pointer *.animalaspirations.org. Host 143.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 144.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 145.217.195.195.in-addr.arpa domain name pointer hhsbc01.rvc.ac.uk.rvc.ac.uk. 145.217.195.195.in-addr.arpa domain name pointer hhsbc01.rvc.ac.uk. Host 146.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 147.217.195.195.in-addr.arpa domain name pointer hhagrwebtest01.rvc.ac.uk. Host 148.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 149.217.195.195.in-addr.arpa domain name pointer dmztest.rvc.ac.uk. Host 150.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 151.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 152.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 153.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 154.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 155.217.195.195.in-addr.arpa domain name pointer biochip.rvc.ac.uk. 155.217.195.195.in-addr.arpa domain name pointer biochip-base.rvc.ac.uk. 155.217.195.195.in-addr.arpa domain name pointer lrrfinder.rvc.ac.uk. 155.217.195.195.in-addr.arpa domain name pointer pgemad.rvc.ac.uk. 155.217.195.195.in-addr.arpa domain name pointer redmite.rvc.ac.uk. 155.217.195.195.in-addr.arpa domain name pointer asked.rvc.ac.uk. 155.217.195.195.in-addr.arpa domain name pointer hhlin5bio01.rvc.ac.uk. Host 156.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 157.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 158.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 159.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 160.217.195.195.in-addr.arpa domain name pointer dss.rvc.ac.uk. 161.217.195.195.in-addr.arpa domain name pointer cmnt4008.rvc.ac.uk. 161.217.195.195.in-addr.arpa domain name pointer hhisepsn1.rvc.ac.uk. 162.217.195.195.in-addr.arpa domain name pointer hhisepsn2.rvc.ac.uk. Host 163.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 164.217.195.195.in-addr.arpa domain name pointer auth2.rvc.ac.uk. 165.217.195.195.in-addr.arpa domain name pointer transfer2.rvc.ac.uk. 166.217.195.195.in-addr.arpa domain name pointer hhsrvacs02.rvc.ac.uk. 167.217.195.195.in-addr.arpa domain name pointer dsa.rvc.ac.uk. 168.217.195.195.in-addr.arpa domain name pointer dsb.rvc.ac.uk. Host 169.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 170.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 171.217.195.195.in-addr.arpa domain name pointer cpd.rvc.ac.uk. Host 172.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 173.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 174.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 175.217.195.195.in-addr.arpa domain name pointer www.atp-ilhp.org. 175.217.195.195.in-addr.arpa domain name pointer www.vetethics.com. 175.217.195.195.in-addr.arpa domain name pointer csc.rvc.ac.uk. 175.217.195.195.in-addr.arpa domain name pointer www.live.ac.uk. 175.217.195.195.in-addr.arpa domain name pointer www.rumirad.com. Host 176.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 177.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 178.217.195.195.in-addr.arpa domain name pointer equinehik1.rvc.ac.uk. 179.217.195.195.in-addr.arpa domain name pointer equinehik2.rvc.ac.uk. Host 180.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 181.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 182.217.195.195.in-addr.arpa domain name pointer svn.rvc.ac.uk. 182.217.195.195.in-addr.arpa domain name pointer trac.rvc.ac.uk. 182.217.195.195.in-addr.arpa domain name pointer smlresearch.rvc.ac.uk. Host 183.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 184.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 185.217.195.195.in-addr.arpa domain name pointer edt.rvc.ac.uk. Host 186.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 187.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 188.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 189.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 190.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 191.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 192.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 193.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 194.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 195.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 196.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 197.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 198.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 199.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 200.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 201.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 202.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 203.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 204.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 205.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 206.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 207.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 208.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 209.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 210.217.195.195.in-addr.arpa domain name pointer www.rvcequine.co.uk. 210.217.195.195.in-addr.arpa domain name pointer www.equinereferralhospital.co.uk. 210.217.195.195.in-addr.arpa domain name pointer www.rvcequine.com. 210.217.195.195.in-addr.arpa domain name pointer www.beaumontsainsbury.co.uk. 210.217.195.195.in-addr.arpa domain name pointer www.qmha.co.uk. 210.217.195.195.in-addr.arpa domain name pointer www.equinepractice.co.uk. 210.217.195.195.in-addr.arpa domain name pointer ram.oiecollaboratingcentre.org. Host 211.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 212.217.195.195.in-addr.arpa domain name pointer sftp.rvc.ac.uk. Host 213.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 214.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 215.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 216.217.195.195.in-addr.arpa domain name pointer registry-dev.rvc.ac.uk. Host 217.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 218.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 219.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 220.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 221.217.195.195.in-addr.arpa domain name pointer bletech.rvc.ac.uk. 222.217.195.195.in-addr.arpa domain name pointer hhrdweb.rvc.ac.uk. 222.217.195.195.in-addr.arpa domain name pointer hhsrvts01.rvc.ac.uk. 222.217.195.195.in-addr.arpa domain name pointer hhsrvts03.rvc.ac.uk. 222.217.195.195.in-addr.arpa domain name pointer hhgw.rvc.ac.uk. 222.217.195.195.in-addr.arpa domain name pointer hhrdweb01.rvc.ac.uk. Host 223.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 224.217.195.195.in-addr.arpa domain name pointer hhsrvlyncedge01.rvc.ac.uk.rvc.ac.uk. 225.217.195.195.in-addr.arpa domain name pointer hhcise.rvc.ac.uk. Host 226.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 227.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 228.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 229.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 230.217.195.195.in-addr.arpa domain name pointer myprint.rvc.ac.uk. Host 231.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 232.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 233.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 234.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 235.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 236.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 237.217.195.195.in-addr.arpa domain name pointer adfs.rvc.ac.uk. Host 238.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 239.217.195.195.in-addr.arpa domain name pointer timetable.rvc.ac.uk. Host 240.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 241.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 242.217.195.195.in-addr.arpa domain name pointer owaop.rvc.ac.uk. 242.217.195.195.in-addr.arpa domain name pointer hhex01.rvc.ac.uk. 242.217.195.195.in-addr.arpa domain name pointer legacy.rvc.ac.uk. 243.217.195.195.in-addr.arpa domain name pointer hhex02.rvc.ac.uk. 243.217.195.195.in-addr.arpa domain name pointer owaop.rvc.ac.uk. 244.217.195.195.in-addr.arpa domain name pointer webscheduler.rvc.ac.uk. Host 245.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 246.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 247.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 248.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 249.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 250.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 251.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 252.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 253.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 254.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 255.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN)","title":"RVC IP Range"},{"location":"kb/linux-networking/rvc-ip-range/#rvc-ip-range","text":"RVC Master IP range: 195.194.217.0/25 Outbound IP for Eduroam : 195.195.217.140/32 Host 0.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 1.217.195.195.in-addr.arpa domain name pointer dis.rvc.ac.uk. 2.217.195.195.in-addr.arpa domain name pointer www.vetschools.ac.uk. 2.217.195.195.in-addr.arpa domain name pointer supervets.rvc.ac.uk. 2.217.195.195.in-addr.arpa domain name pointer www.supervets.rvc.ac.uk. Host 3.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 4.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 5.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 6.217.195.195.in-addr.arpa domain name pointer www.ectp.eu.com. 7.217.195.195.in-addr.arpa domain name pointer ns1.rvc.ac.uk. 8.217.195.195.in-addr.arpa domain name pointer rotasafevaccine.rvc.ac.uk. 9.217.195.195.in-addr.arpa domain name pointer www.nobarriers2uni.org. 10.217.195.195.in-addr.arpa domain name pointer act.rvc.ac.uk. Host 11.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 12.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 13.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 14.217.195.195.in-addr.arpa domain name pointer www.careersinhealth.org.uk. 14.217.195.195.in-addr.arpa domain name pointer researchpubs.rvc.ac.uk. 14.217.195.195.in-addr.arpa domain name pointer evedna.rvc.ac.uk. 14.217.195.195.in-addr.arpa domain name pointer ferrets.rvc.ac.uk. 15.217.195.195.in-addr.arpa domain name pointer ahchemots.rvc.ac.uk. 15.217.195.195.in-addr.arpa domain name pointer londonahots.rvc.ac.uk. 15.217.195.195.in-addr.arpa domain name pointer library.rvc.ac.uk. 15.217.195.195.in-addr.arpa domain name pointer ahsportsots.rvc.ac.uk. 15.217.195.195.in-addr.arpa domain name pointer hkvn.rvc.ac.uk. 15.217.195.195.in-addr.arpa domain name pointer ahartsots.rvc.ac.uk. Host 16.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 17.217.195.195.in-addr.arpa domain name pointer www.asvin.ac.uk. 17.217.195.195.in-addr.arpa domain name pointer oliver.live.ac.uk. 18.217.195.195.in-addr.arpa domain name pointer www.mypad.rvc.ac.uk. 18.217.195.195.in-addr.arpa domain name pointer pdp.rvc.ac.uk. 19.217.195.195.in-addr.arpa domain name pointer nurses.vetschools.ac.uk. Host 20.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 21.217.195.195.in-addr.arpa domain name pointer www.eclw.ac.uk. Host 22.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 23.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 24.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 25.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 26.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 27.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 28.217.195.195.in-addr.arpa domain name pointer reviews.rvc.ac.uk. Host 29.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 30.217.195.195.in-addr.arpa domain name pointer cmrdweb.rvc.ac.uk. 30.217.195.195.in-addr.arpa domain name pointer cmrdweb01.rvc.ac.uk. 30.217.195.195.in-addr.arpa domain name pointer rdweb.rvc.ac.uk. 30.217.195.195.in-addr.arpa domain name pointer cmgw.rvc.ac.uk. Host 31.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 32.217.195.195.in-addr.arpa domain name pointer fr.wikivet.net. 32.217.195.195.in-addr.arpa domain name pointer mailing.wikivet.net. 32.217.195.195.in-addr.arpa domain name pointer survey.wikivet.net. 32.217.195.195.in-addr.arpa domain name pointer en.wikivet.net. 32.217.195.195.in-addr.arpa domain name pointer es.wikivet.net. 32.217.195.195.in-addr.arpa domain name pointer wikivet.net. 32.217.195.195.in-addr.arpa domain name pointer www.onlineveterinaryanatomy.net. 32.217.195.195.in-addr.arpa domain name pointer newsletter.wikivet.net. 32.217.195.195.in-addr.arpa domain name pointer stream2.rvc.ac.uk. 32.217.195.195.in-addr.arpa domain name pointer dev.wikivet.net. 32.217.195.195.in-addr.arpa domain name pointer commons.wikivet.net. Host 33.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 34.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 35.217.195.195.in-addr.arpa domain name pointer blackboard2.rvc.ac.uk. 35.217.195.195.in-addr.arpa domain name pointer blackboard.rvc.ac.uk. Host 36.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 37.217.195.195.in-addr.arpa domain name pointer auth1.rvc.ac.uk. Host 38.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 39.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 40.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 41.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 42.217.195.195.in-addr.arpa domain name pointer owaop.rvc.ac.uk. 42.217.195.195.in-addr.arpa domain name pointer live.ac.uk. 42.217.195.195.in-addr.arpa domain name pointer cmex01.rvc.ac.uk. 43.217.195.195.in-addr.arpa domain name pointer cmex02.rvc.ac.uk. 43.217.195.195.in-addr.arpa domain name pointer owaop.rvc.ac.uk. Host 44.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 45.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 46.217.195.195.in-addr.arpa domain name pointer ispring.rvc.ac.uk. 47.217.195.195.in-addr.arpa domain name pointer www.asfnetwork.org. Host 48.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 49.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 50.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 51.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 52.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 53.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 54.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 55.217.195.195.in-addr.arpa domain name pointer myaccount.rvc.ac.uk. Host 56.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 57.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 58.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 59.217.195.195.in-addr.arpa domain name pointer mobilett.rvc.ac.uk. Host 60.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 61.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 62.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 63.217.195.195.in-addr.arpa domain name pointer cmcise.rvc.ac.uk. Host 64.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 65.217.195.195.in-addr.arpa domain name pointer mdp.rvc.ac.uk. 66.217.195.195.in-addr.arpa domain name pointer cmisepsn1.rvc.ac.uk. 67.217.195.195.in-addr.arpa domain name pointer cmisepsn2.rvc.ac.uk. 68.217.195.195.in-addr.arpa domain name pointer vetcompass-listy.api-test.rvc.ac.uk. 68.217.195.195.in-addr.arpa domain name pointer pwm.rvc.ac.uk. 68.217.195.195.in-addr.arpa domain name pointer student-data.api.rvc.ac.uk. 68.217.195.195.in-addr.arpa domain name pointer vetcompass-listy.api.rvc.ac.uk. 68.217.195.195.in-addr.arpa domain name pointer oss.rvc.ac.uk. 68.217.195.195.in-addr.arpa domain name pointer maintenance.rvc.ac.uk. 68.217.195.195.in-addr.arpa domain name pointer agrl.rvc.ac.uk. 68.217.195.195.in-addr.arpa domain name pointer photos-proxy.api.rvc.ac.uk. 68.217.195.195.in-addr.arpa domain name pointer gradebook.rvc.ac.uk. Host 69.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 70.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 71.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 72.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 73.217.195.195.in-addr.arpa domain name pointer rvchpc1.rvc.ac.uk. 74.217.195.195.in-addr.arpa domain name pointer rvchpcservice1.rvc.ac.uk. Host 75.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 76.217.195.195.in-addr.arpa domain name pointer jobs.rvc.ac.uk. 76.217.195.195.in-addr.arpa domain name pointer jobstest.rvc.ac.uk. 77.217.195.195.in-addr.arpa domain name pointer library.rvc.ac.uk. 78.217.195.195.in-addr.arpa domain name pointer digitalslides.rvc.ac.uk. Host 79.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 80.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 81.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 82.217.195.195.in-addr.arpa domain name pointer rx.api.rvc.ac.uk. 83.217.195.195.in-addr.arpa domain name pointer idp.rvc.ac.uk. Host 84.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 85.217.195.195.in-addr.arpa domain name pointer vct-cpd.rvc.ac.uk. Host 86.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 87.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 88.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 89.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 90.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 91.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 92.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 93.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 94.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 95.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 96.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 97.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 98.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 99.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 100.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 101.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 102.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 103.217.195.195.in-addr.arpa domain name pointer cm-mskms-xx01.rvc.ac.uk. 104.217.195.195.in-addr.arpa domain name pointer redcap.rvc.ac.uk. Host 105.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 106.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 107.217.195.195.in-addr.arpa domain name pointer dynamicslive.rvc.ac.uk. 108.217.195.195.in-addr.arpa domain name pointer dynamicsdev.rvc.ac.uk. 109.217.195.195.in-addr.arpa domain name pointer imagebank.rvc.ac.uk. 109.217.195.195.in-addr.arpa domain name pointer assetbank.rvc.ac.uk. 109.217.195.195.in-addr.arpa domain name pointer images.rvc.ac.uk. Host 110.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 111.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 112.217.195.195.in-addr.arpa domain name pointer registry.rvc.ac.uk. 113.217.195.195.in-addr.arpa domain name pointer clobba.rvc.ac.uk. Host 114.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 115.217.195.195.in-addr.arpa domain name pointer cmsbc01.rvc.ac.uk. Host 116.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 117.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 118.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 119.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 120.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 121.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 122.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 123.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 124.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 125.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 126.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 127.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 128.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 129.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 130.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 131.217.195.195.in-addr.arpa domain name pointer hhw2k3lib01.rvc.ac.uk. 132.217.195.195.in-addr.arpa domain name pointer www.vetcompass.com. 132.217.195.195.in-addr.arpa domain name pointer www.vetcompass.org. 132.217.195.195.in-addr.arpa domain name pointer vetcompass.org.nz. 132.217.195.195.in-addr.arpa domain name pointer identity.vetcompass.org. 132.217.195.195.in-addr.arpa domain name pointer cris.api.rvc.ac.uk. 132.217.195.195.in-addr.arpa domain name pointer vetcompass.rvc.ac.uk. 132.217.195.195.in-addr.arpa domain name pointer eunity.api.rvc.ac.uk. 132.217.195.195.in-addr.arpa domain name pointer vetcompass.org. 132.217.195.195.in-addr.arpa domain name pointer www.vetcompass.org.nz. 132.217.195.195.in-addr.arpa domain name pointer vetcompass.com. 132.217.195.195.in-addr.arpa domain name pointer cris.rvc.ac.uk. 133.217.195.195.in-addr.arpa domain name pointer test.vetcompass.org. 133.217.195.195.in-addr.arpa domain name pointer identitytest.vetcompass.org. Host 134.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 135.217.195.195.in-addr.arpa domain name pointer thankq.rvc.ac.uk. 136.217.195.195.in-addr.arpa domain name pointer exr.rvc.ac.uk. Host 137.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 138.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 139.217.195.195.in-addr.arpa domain name pointer eunity.rvc.ac.uk. 140.217.195.195.in-addr.arpa domain name pointer vpn.rvc.ac.uk. Host 141.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 142.217.195.195.in-addr.arpa domain name pointer topdesk.rvc.ac.uk. 142.217.195.195.in-addr.arpa domain name pointer animalaspirations.org. 142.217.195.195.in-addr.arpa domain name pointer library.rvc.ac.uk. 142.217.195.195.in-addr.arpa domain name pointer worktribe.rvc.ac.uk. 142.217.195.195.in-addr.arpa domain name pointer *.animalaspirations.org. Host 143.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 144.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 145.217.195.195.in-addr.arpa domain name pointer hhsbc01.rvc.ac.uk.rvc.ac.uk. 145.217.195.195.in-addr.arpa domain name pointer hhsbc01.rvc.ac.uk. Host 146.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 147.217.195.195.in-addr.arpa domain name pointer hhagrwebtest01.rvc.ac.uk. Host 148.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 149.217.195.195.in-addr.arpa domain name pointer dmztest.rvc.ac.uk. Host 150.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 151.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 152.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 153.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 154.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 155.217.195.195.in-addr.arpa domain name pointer biochip.rvc.ac.uk. 155.217.195.195.in-addr.arpa domain name pointer biochip-base.rvc.ac.uk. 155.217.195.195.in-addr.arpa domain name pointer lrrfinder.rvc.ac.uk. 155.217.195.195.in-addr.arpa domain name pointer pgemad.rvc.ac.uk. 155.217.195.195.in-addr.arpa domain name pointer redmite.rvc.ac.uk. 155.217.195.195.in-addr.arpa domain name pointer asked.rvc.ac.uk. 155.217.195.195.in-addr.arpa domain name pointer hhlin5bio01.rvc.ac.uk. Host 156.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 157.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 158.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 159.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 160.217.195.195.in-addr.arpa domain name pointer dss.rvc.ac.uk. 161.217.195.195.in-addr.arpa domain name pointer cmnt4008.rvc.ac.uk. 161.217.195.195.in-addr.arpa domain name pointer hhisepsn1.rvc.ac.uk. 162.217.195.195.in-addr.arpa domain name pointer hhisepsn2.rvc.ac.uk. Host 163.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 164.217.195.195.in-addr.arpa domain name pointer auth2.rvc.ac.uk. 165.217.195.195.in-addr.arpa domain name pointer transfer2.rvc.ac.uk. 166.217.195.195.in-addr.arpa domain name pointer hhsrvacs02.rvc.ac.uk. 167.217.195.195.in-addr.arpa domain name pointer dsa.rvc.ac.uk. 168.217.195.195.in-addr.arpa domain name pointer dsb.rvc.ac.uk. Host 169.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 170.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 171.217.195.195.in-addr.arpa domain name pointer cpd.rvc.ac.uk. Host 172.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 173.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 174.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 175.217.195.195.in-addr.arpa domain name pointer www.atp-ilhp.org. 175.217.195.195.in-addr.arpa domain name pointer www.vetethics.com. 175.217.195.195.in-addr.arpa domain name pointer csc.rvc.ac.uk. 175.217.195.195.in-addr.arpa domain name pointer www.live.ac.uk. 175.217.195.195.in-addr.arpa domain name pointer www.rumirad.com. Host 176.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 177.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 178.217.195.195.in-addr.arpa domain name pointer equinehik1.rvc.ac.uk. 179.217.195.195.in-addr.arpa domain name pointer equinehik2.rvc.ac.uk. Host 180.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 181.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 182.217.195.195.in-addr.arpa domain name pointer svn.rvc.ac.uk. 182.217.195.195.in-addr.arpa domain name pointer trac.rvc.ac.uk. 182.217.195.195.in-addr.arpa domain name pointer smlresearch.rvc.ac.uk. Host 183.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 184.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 185.217.195.195.in-addr.arpa domain name pointer edt.rvc.ac.uk. Host 186.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 187.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 188.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 189.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 190.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 191.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 192.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 193.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 194.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 195.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 196.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 197.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 198.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 199.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 200.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 201.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 202.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 203.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 204.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 205.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 206.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 207.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 208.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 209.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 210.217.195.195.in-addr.arpa domain name pointer www.rvcequine.co.uk. 210.217.195.195.in-addr.arpa domain name pointer www.equinereferralhospital.co.uk. 210.217.195.195.in-addr.arpa domain name pointer www.rvcequine.com. 210.217.195.195.in-addr.arpa domain name pointer www.beaumontsainsbury.co.uk. 210.217.195.195.in-addr.arpa domain name pointer www.qmha.co.uk. 210.217.195.195.in-addr.arpa domain name pointer www.equinepractice.co.uk. 210.217.195.195.in-addr.arpa domain name pointer ram.oiecollaboratingcentre.org. Host 211.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 212.217.195.195.in-addr.arpa domain name pointer sftp.rvc.ac.uk. Host 213.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 214.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 215.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 216.217.195.195.in-addr.arpa domain name pointer registry-dev.rvc.ac.uk. Host 217.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 218.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 219.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 220.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 221.217.195.195.in-addr.arpa domain name pointer bletech.rvc.ac.uk. 222.217.195.195.in-addr.arpa domain name pointer hhrdweb.rvc.ac.uk. 222.217.195.195.in-addr.arpa domain name pointer hhsrvts01.rvc.ac.uk. 222.217.195.195.in-addr.arpa domain name pointer hhsrvts03.rvc.ac.uk. 222.217.195.195.in-addr.arpa domain name pointer hhgw.rvc.ac.uk. 222.217.195.195.in-addr.arpa domain name pointer hhrdweb01.rvc.ac.uk. Host 223.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 224.217.195.195.in-addr.arpa domain name pointer hhsrvlyncedge01.rvc.ac.uk.rvc.ac.uk. 225.217.195.195.in-addr.arpa domain name pointer hhcise.rvc.ac.uk. Host 226.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 227.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 228.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 229.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 230.217.195.195.in-addr.arpa domain name pointer myprint.rvc.ac.uk. Host 231.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 232.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 233.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 234.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 235.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 236.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 237.217.195.195.in-addr.arpa domain name pointer adfs.rvc.ac.uk. Host 238.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 239.217.195.195.in-addr.arpa domain name pointer timetable.rvc.ac.uk. Host 240.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 241.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) 242.217.195.195.in-addr.arpa domain name pointer owaop.rvc.ac.uk. 242.217.195.195.in-addr.arpa domain name pointer hhex01.rvc.ac.uk. 242.217.195.195.in-addr.arpa domain name pointer legacy.rvc.ac.uk. 243.217.195.195.in-addr.arpa domain name pointer hhex02.rvc.ac.uk. 243.217.195.195.in-addr.arpa domain name pointer owaop.rvc.ac.uk. 244.217.195.195.in-addr.arpa domain name pointer webscheduler.rvc.ac.uk. Host 245.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 246.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 247.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 248.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 249.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 250.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 251.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 252.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 253.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 254.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN) Host 255.217.195.195.in-addr.arpa. not found: 3(NXDOMAIN)","title":"RVC IP range"},{"location":"kb/linux-networking/uninstall-netplan/","text":"Uninstalling netplan These directions have been tested also to Ubuntu 18.04.1 and will very likely work also for any future release using netplan and systemd . There's no need at all to fiddle with GRUB nor any manual file removal. The configuration set up in /etc/networking files and directories will survive reboots . These are the verified steps: Check the actual interface names you are interested in with ip l for the links (aka interfaces) and with ip a for addresses. Install ifupdown with sudo apt -y install ifupdown . Purge netplan with sudo apt -y purge netplan.io . Configure /etc/network/interfaces and/or /etc/network/interfaces.d accordingly to your needs ( man 5 interfaces can be of some help with examples). Restart the networking service with sudo systemctl restart networking; systemctl status networking or sudo /etc/init.d/networking restart; /etc/init.d/networking status . The output of the status command should mention active as its status. The command ip a will show whether the expected network configuration has been applied. Optionally, manually purge the remants of the netplan configuration files with sudo rm -vfr /usr/share/netplan /etc/netplan . No reboot is needed in order to \"refresh\" the IP configuration: it will be active as of step no.5 . In case of troubles, double check the interface names. A typical IPv4 DHCP configuration will resemble this one: auto enp0s3 iface enp0s3 inet dhcp while a static IPv4 address can be configured like this: auto enp0s3 iface enp0s3 inet static address 192.168.255.42/24 gateway 192.168.255.254 #dns-nameservers 8.8.8.8 208.67.222.222","title":"Uninstall Netplan"},{"location":"kb/linux-networking/uninstall-netplan/#uninstalling-netplan","text":"These directions have been tested also to Ubuntu 18.04.1 and will very likely work also for any future release using netplan and systemd . There's no need at all to fiddle with GRUB nor any manual file removal. The configuration set up in /etc/networking files and directories will survive reboots . These are the verified steps: Check the actual interface names you are interested in with ip l for the links (aka interfaces) and with ip a for addresses. Install ifupdown with sudo apt -y install ifupdown . Purge netplan with sudo apt -y purge netplan.io . Configure /etc/network/interfaces and/or /etc/network/interfaces.d accordingly to your needs ( man 5 interfaces can be of some help with examples). Restart the networking service with sudo systemctl restart networking; systemctl status networking or sudo /etc/init.d/networking restart; /etc/init.d/networking status . The output of the status command should mention active as its status. The command ip a will show whether the expected network configuration has been applied. Optionally, manually purge the remants of the netplan configuration files with sudo rm -vfr /usr/share/netplan /etc/netplan . No reboot is needed in order to \"refresh\" the IP configuration: it will be active as of step no.5 . In case of troubles, double check the interface names. A typical IPv4 DHCP configuration will resemble this one: auto enp0s3 iface enp0s3 inet dhcp while a static IPv4 address can be configured like this: auto enp0s3 iface enp0s3 inet static address 192.168.255.42/24 gateway 192.168.255.254 #dns-nameservers 8.8.8.8 208.67.222.222","title":"Uninstalling netplan"},{"location":"kb/markdown/built-by-badge/","text":"Built by badge ![ Maintainer ]( https://img.shields.io/badge/Built%20By-Bradley-brightgreen?style=for-the-badge&logo=terraform )","title":"Built By badge"},{"location":"kb/markdown/built-by-badge/#built-by-badge","text":"![ Maintainer ]( https://img.shields.io/badge/Built%20By-Bradley-brightgreen?style=for-the-badge&logo=terraform )","title":"Built by badge"},{"location":"kb/markdown/custom-badges/","text":"Custom markdown badges To make a badge like the below: Find the Logo Navigate to Simple icons Make note of the Logo name. If you are struggling to find the name, go to their git repo and search for it Encode the color On the left hand side of a simpleicon is the color code Paste the copied value in to the below site and encode it https://meyerweb.com/eric/tools/dencoder/ Assemble the badge ![ <brand> ]( https://img.shields.io/badge/<brand>-<encoded value>.svg?style=for-the-badge&logo=<brand>&logoColor=<hex> ) Replace with the brands icon name. Brand names with spaces Where the brand has a space in the name, like Pets at Home you will need to use the url encoded value of a space: %20 It will look like Pets%20at%20Home Example ![ deutschebank ]( https://img.shields.io/badge/deutschebank-%230018A8.svg?style=for-the-badge&logo=deutschebank&logoColor=#0018A8 ) Examples Pets at Home American Express Deutsche Bank About the Color The background color is back as the brands image is the same as the background, thus causing it to look bad About the Color The background color is back as the brands image is the same as the background, thus causing it to look bad","title":"Custom markdown badges"},{"location":"kb/markdown/custom-badges/#custom-markdown-badges","text":"To make a badge like the below:","title":"Custom markdown badges"},{"location":"kb/markdown/custom-badges/#find-the-logo","text":"Navigate to Simple icons Make note of the Logo name. If you are struggling to find the name, go to their git repo and search for it","title":"Find the Logo"},{"location":"kb/markdown/custom-badges/#encode-the-color","text":"On the left hand side of a simpleicon is the color code Paste the copied value in to the below site and encode it https://meyerweb.com/eric/tools/dencoder/","title":"Encode the color"},{"location":"kb/markdown/custom-badges/#assemble-the-badge","text":"![ <brand> ]( https://img.shields.io/badge/<brand>-<encoded value>.svg?style=for-the-badge&logo=<brand>&logoColor=<hex> ) Replace with the brands icon name. Brand names with spaces Where the brand has a space in the name, like Pets at Home you will need to use the url encoded value of a space: %20 It will look like Pets%20at%20Home Example ![ deutschebank ]( https://img.shields.io/badge/deutschebank-%230018A8.svg?style=for-the-badge&logo=deutschebank&logoColor=#0018A8 )","title":"Assemble the badge"},{"location":"kb/markdown/custom-badges/#examples","text":"Pets at Home American Express Deutsche Bank About the Color The background color is back as the brands image is the same as the background, thus causing it to look bad About the Color The background color is back as the brands image is the same as the background, thus causing it to look bad","title":"Examples"},{"location":"kb/markdown/github-repos-and-stuff/","text":"How to use git To clone a repo, go to the webpage and click the clone or download button and click 'Use SSH' SSH or https Seeing as you have 2fa enabled on your account, you should be using ssh as it doesnt require you to login each action you make Once you have the string on your clipboard open a terminal window and browse to a location where your code will now live. I use ~\\github Run: git clone git@github.com:<repo url> Once cloned, go to its folder and edit the code to your hearts content Once you're ready to push your code git add <file ' s that changed> git commit -m ' Added files/ refactored prd git push","title":"How to use git"},{"location":"kb/markdown/github-repos-and-stuff/#how-to-use-git","text":"To clone a repo, go to the webpage and click the clone or download button and click 'Use SSH' SSH or https Seeing as you have 2fa enabled on your account, you should be using ssh as it doesnt require you to login each action you make Once you have the string on your clipboard open a terminal window and browse to a location where your code will now live. I use ~\\github Run: git clone git@github.com:<repo url> Once cloned, go to its folder and edit the code to your hearts content Once you're ready to push your code git add <file ' s that changed> git commit -m ' Added files/ refactored prd git push","title":"How to use git"},{"location":"kb/minio/minio-over-s3fs/","text":"Minio over s3fs Create the password file sudo echo private:sectet > /etc/passwd-s3fs Assign permissions chmod 600 /etc/passwd-s3fs Create a mount point for the bucket sudo mkdir /mnt/s3 Connect to the bucket s3fs <bucket> /mnt/s3 -o passwd_file = /etc/passwd-s3fs,use_path_request_style,url = https://s3.breadnet.co.uk:9000","title":"Connecting to minio over s3fs"},{"location":"kb/minio/minio-over-s3fs/#minio-over-s3fs","text":"","title":"Minio over s3fs"},{"location":"kb/minio/minio-over-s3fs/#create-the-password-file","text":"sudo echo private:sectet > /etc/passwd-s3fs","title":"Create the password file"},{"location":"kb/minio/minio-over-s3fs/#assign-permissions","text":"chmod 600 /etc/passwd-s3fs","title":"Assign permissions"},{"location":"kb/minio/minio-over-s3fs/#create-a-mount-point-for-the-bucket","text":"sudo mkdir /mnt/s3","title":"Create a mount point for the bucket"},{"location":"kb/minio/minio-over-s3fs/#connect-to-the-bucket","text":"s3fs <bucket> /mnt/s3 -o passwd_file = /etc/passwd-s3fs,use_path_request_style,url = https://s3.breadnet.co.uk:9000","title":"Connect to the bucket"},{"location":"kb/minio/old/","text":"OLD Adding s3fs sudo echo private:sectet > /etc/passwd-s3fs chmod 600 /etc/passwd-s3fs s3fs <bucket> /s3 -o passwd_file = /etc/passwd-s3fs,use_path_request_style,url = https://s3.breadnet.co.uk:9000 connecting to server backup bucket s3fs serverbackup /mnt/s3 -o passwd_file = /etc/passwd-s3fs,use_path_request_style,url = https://s3.breadnet.co.uk:9000 create mc user ./mc admin user add <name><password> Add them to the readwrite group ./mc admin policy set myminio readwrite user= backup script by date mkdir /mnt/s3/<servername>/$(date +\"%m-%d-%y\") rsync -ravzX </path/to/file/you/want/to/backup/> /mnt/s3/<hostname>/$(date +\"%m-%d-%y\")/ normal backup rsync -ravzX <path to backup folder> /mnt/s3/<hostname> backup that removes files removed on local system rsync -ravzX --delete<path to backup folder> /mnt/s3/<hostname>","title":"OLD"},{"location":"kb/minio/old/#old","text":"","title":"OLD"},{"location":"kb/minio/old/#adding-s3fs","text":"sudo echo private:sectet > /etc/passwd-s3fs chmod 600 /etc/passwd-s3fs s3fs <bucket> /s3 -o passwd_file = /etc/passwd-s3fs,use_path_request_style,url = https://s3.breadnet.co.uk:9000","title":"Adding s3fs"},{"location":"kb/minio/old/#connecting-to-server-backup-bucket","text":"s3fs serverbackup /mnt/s3 -o passwd_file = /etc/passwd-s3fs,use_path_request_style,url = https://s3.breadnet.co.uk:9000","title":"connecting to server backup bucket"},{"location":"kb/minio/old/#create-mc-user","text":"./mc admin user add <name><password> Add them to the readwrite group ./mc admin policy set myminio readwrite user=","title":"create mc user"},{"location":"kb/minio/old/#backup-script-by-date","text":"mkdir /mnt/s3/<servername>/$(date +\"%m-%d-%y\") rsync -ravzX </path/to/file/you/want/to/backup/> /mnt/s3/<hostname>/$(date +\"%m-%d-%y\")/","title":"backup script by date"},{"location":"kb/minio/old/#normal-backup","text":"rsync -ravzX <path to backup folder> /mnt/s3/<hostname>","title":"normal backup"},{"location":"kb/minio/old/#backup-that-removes-files-removed-on-local-system","text":"rsync -ravzX --delete<path to backup folder> /mnt/s3/<hostname>","title":"backup that removes files removed on local system"},{"location":"kb/minio/s3-policies/","text":"title: minio: Creating users and assigning policies outdated: true minio: Creating users and assigning policies Connect to the minio server mc config host add myminio https://s3.breadnet.co.uk:9000 BWFEUE9RZUOQWAFP1DCQ LYLBt+qW8TnqlD7Kq1y29IFLDUBL5Qvx+JMimcR7 Create the policy, using the below as a guide. Save it to reflect what it does Create the policy on minio mc admin policy add myminio <policy name> <policy file>.json Create a user to assign it to mc admin user add myminio <username> <password> Assign the policy to the user mc admin policy set myminio <policyname> user=<username> Policy to allow access to a full bucket { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"*\" ] }, \"Action\" : [ \"s3:GetBucketLocation\" , \"s3:ListBucket\" , \"s3:ListBucketMultipartUploads\" ], \"Resource\" : [ \"arn:aws:s3:::testbucket-1\" ] }, { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"*\" ] }, \"Action\" : [ \"s3:AbortMultipartUpload\" , \"s3:DeleteObject\" , \"s3:GetObject\" , \"s3:ListMultipartUploadParts\" , \"s3:PutObject\" ], \"Resource\" : [ \"arn:aws:s3:::testbucket-1/*\" ] } ] } Policy to allow access to a folder in a bucket { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowStatement1\" , \"Action\" : [ \"s3:ListAllMyBuckets\" , \"s3:GetBucketLocation\" ], \"Effect\" : \"Allow\" , \"Resource\" : [ \"arn:aws:s3:::*\" ] }, { \"Sid\" : \"AllowStatement2A\" , \"Action\" : [ \"s3:ListBucket\" ], \"Effect\" : \"Allow\" , \"Resource\" : [ \"arn:aws:s3:::serverbackup\" ], \"Condition\" :{ \"StringEquals\" :{ \"s3:prefix\" :[ \"\" , \"dbserver\" ]}} }, { \"Sid\" : \"AllowStatement3\" , \"Action\" : [ \"s3:ListBucket\" ], \"Effect\" : \"Allow\" , \"Resource\" : [ \"arn:aws:s3:::serverbackup\" ], \"Condition\" :{ \"StringLike\" :{ \"s3:prefix\" :[ \"dbserver/*\" ]}} }, { \"Sid\" : \"AllowStatement4A\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:AbortMultipartUpload\" , \"s3:DeleteObject\" , \"s3:GetObject\" , \"s3:ListMultipartUploadParts\" , \"s3:PutObject\" ], \"Resource\" : [ \"arn:aws:s3:::serverbackup/dbserver/*\" ] } ] }","title":"creating users and assigning policies"},{"location":"kb/minio/s3-policies/#minio-creating-users-and-assigning-policies","text":"","title":"minio: Creating users and assigning policies"},{"location":"kb/minio/s3-policies/#connect-to-the-minio-server","text":"mc config host add myminio https://s3.breadnet.co.uk:9000 BWFEUE9RZUOQWAFP1DCQ LYLBt+qW8TnqlD7Kq1y29IFLDUBL5Qvx+JMimcR7 Create the policy, using the below as a guide. Save it to reflect what it does Create the policy on minio mc admin policy add myminio <policy name> <policy file>.json Create a user to assign it to mc admin user add myminio <username> <password>","title":"Connect to the minio server"},{"location":"kb/minio/s3-policies/#assign-the-policy-to-the-user","text":"mc admin policy set myminio <policyname> user=<username>","title":"Assign the policy to the user"},{"location":"kb/minio/s3-policies/#policy-to-allow-access-to-a-full-bucket","text":"{ \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"*\" ] }, \"Action\" : [ \"s3:GetBucketLocation\" , \"s3:ListBucket\" , \"s3:ListBucketMultipartUploads\" ], \"Resource\" : [ \"arn:aws:s3:::testbucket-1\" ] }, { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"*\" ] }, \"Action\" : [ \"s3:AbortMultipartUpload\" , \"s3:DeleteObject\" , \"s3:GetObject\" , \"s3:ListMultipartUploadParts\" , \"s3:PutObject\" ], \"Resource\" : [ \"arn:aws:s3:::testbucket-1/*\" ] } ] }","title":"Policy to allow access to a full bucket"},{"location":"kb/minio/s3-policies/#policy-to-allow-access-to-a-folder-in-a-bucket","text":"{ \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowStatement1\" , \"Action\" : [ \"s3:ListAllMyBuckets\" , \"s3:GetBucketLocation\" ], \"Effect\" : \"Allow\" , \"Resource\" : [ \"arn:aws:s3:::*\" ] }, { \"Sid\" : \"AllowStatement2A\" , \"Action\" : [ \"s3:ListBucket\" ], \"Effect\" : \"Allow\" , \"Resource\" : [ \"arn:aws:s3:::serverbackup\" ], \"Condition\" :{ \"StringEquals\" :{ \"s3:prefix\" :[ \"\" , \"dbserver\" ]}} }, { \"Sid\" : \"AllowStatement3\" , \"Action\" : [ \"s3:ListBucket\" ], \"Effect\" : \"Allow\" , \"Resource\" : [ \"arn:aws:s3:::serverbackup\" ], \"Condition\" :{ \"StringLike\" :{ \"s3:prefix\" :[ \"dbserver/*\" ]}} }, { \"Sid\" : \"AllowStatement4A\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:AbortMultipartUpload\" , \"s3:DeleteObject\" , \"s3:GetObject\" , \"s3:ListMultipartUploadParts\" , \"s3:PutObject\" ], \"Resource\" : [ \"arn:aws:s3:::serverbackup/dbserver/*\" ] } ] }","title":"Policy to allow access to a folder in a bucket"},{"location":"kb/networking/update-tough-switch/","text":"Update Tough switch Ubiquiti have renamed their tough switch line in the web UI, to Edge MAx Reset the switch It's honestly easier to reset the switch (Holding down the reset button) and then connecting to it directly. You will need to set your network interface to the below computer IP: 192.168.1.22 subnet: 255.255.255.0 Gateway: NUll Download the firmware edgeswitch-xp/es-8xp/edgeswitch-xp-firmware-v210 Edit your SSH config file This is becuase it uses the diffie hellman group key Algorithms Host 192 .168.1.20 # (1)! KexAlgorithms +diffie-hellman-group1-sha1 Ciphers aes128-cbc Replace the IP address with that of the switch you're connecting to. Copy the files to the switch scp SW.v2.1.0.142.210208.1325.bin ubnt@192.168.1.20:/tmp/fwupdate.bin SSH to the switch ssh ubnt@192.168.1.20 The password will be ubnt Start the upgrade cd /bin/ fwupdate -m Do not panic It will take around 5-10 minutes after running the command. DO NOT power off the switch, however you are able to unplug your computer.","title":"Updating Ubiquiti Tough switch"},{"location":"kb/networking/update-tough-switch/#update-tough-switch","text":"Ubiquiti have renamed their tough switch line in the web UI, to Edge MAx","title":"Update Tough switch"},{"location":"kb/networking/update-tough-switch/#reset-the-switch","text":"It's honestly easier to reset the switch (Holding down the reset button) and then connecting to it directly. You will need to set your network interface to the below computer IP: 192.168.1.22 subnet: 255.255.255.0 Gateway: NUll","title":"Reset the switch"},{"location":"kb/networking/update-tough-switch/#download-the-firmware","text":"edgeswitch-xp/es-8xp/edgeswitch-xp-firmware-v210","title":"Download the firmware"},{"location":"kb/networking/update-tough-switch/#edit-your-ssh-config-file","text":"This is becuase it uses the diffie hellman group key Algorithms Host 192 .168.1.20 # (1)! KexAlgorithms +diffie-hellman-group1-sha1 Ciphers aes128-cbc Replace the IP address with that of the switch you're connecting to.","title":"Edit your SSH config file"},{"location":"kb/networking/update-tough-switch/#copy-the-files-to-the-switch","text":"scp SW.v2.1.0.142.210208.1325.bin ubnt@192.168.1.20:/tmp/fwupdate.bin","title":"Copy the files to the switch"},{"location":"kb/networking/update-tough-switch/#ssh-to-the-switch","text":"ssh ubnt@192.168.1.20 The password will be ubnt","title":"SSH to the switch"},{"location":"kb/networking/update-tough-switch/#start-the-upgrade","text":"cd /bin/ fwupdate -m Do not panic It will take around 5-10 minutes after running the command. DO NOT power off the switch, however you are able to unplug your computer.","title":"Start the upgrade"},{"location":"kb/networking/vpn-network-routing-mikrotik/","text":"VPN network routing Mikrotik This is a bit rough and ready and may contain mistakes but should point you in the right direction. This is also not the only way it could be done and may not be the best Add IP addresses you want to route via the VPN to an address list /ip firewall address-list add address = 192 .168.88.254 list = OutVpn create a rule to mark packets from your address list for routing /ip firewall mangle chain = prerouting action = mark-routing new-routing-mark = VpnRoute passthrough = yes \\ src-address-list = OutVpn log = no log-prefix = \"\" dst-address = 0 .0.0.0/0 NAT the traffic so that traffic tunnelling through the VPN appears to come from 192.168.42.10 (the Hap IP) /ip firewall nat add action = src-nat chain = srcnat disabled = yes src-address-list = OutVpn to-addresses = \\ 192 .168.42.10 Add a Route sending all marked packets/traffic via the VPN /ip route add check-gateway = ping distance = 1 gateway = YourVpn routing-mark = VpnRoute","title":"VPN Network routing Mikrotik"},{"location":"kb/networking/vpn-network-routing-mikrotik/#vpn-network-routing-mikrotik","text":"This is a bit rough and ready and may contain mistakes but should point you in the right direction. This is also not the only way it could be done and may not be the best Add IP addresses you want to route via the VPN to an address list /ip firewall address-list add address = 192 .168.88.254 list = OutVpn create a rule to mark packets from your address list for routing /ip firewall mangle chain = prerouting action = mark-routing new-routing-mark = VpnRoute passthrough = yes \\ src-address-list = OutVpn log = no log-prefix = \"\" dst-address = 0 .0.0.0/0 NAT the traffic so that traffic tunnelling through the VPN appears to come from 192.168.42.10 (the Hap IP) /ip firewall nat add action = src-nat chain = srcnat disabled = yes src-address-list = OutVpn to-addresses = \\ 192 .168.42.10 Add a Route sending all marked packets/traffic via the VPN /ip route add check-gateway = ping distance = 1 gateway = YourVpn routing-mark = VpnRoute","title":"VPN network routing Mikrotik"},{"location":"kb/nginx/custom-headers/","text":"Custom Headers Install the below: apt-get install nginx-extras You can now add this anywhere: more_set_headers \"Bradley: is cool\" ; Additional Resources","title":"Custom Headers"},{"location":"kb/nginx/custom-headers/#custom-headers","text":"Install the below: apt-get install nginx-extras You can now add this anywhere: more_set_headers \"Bradley: is cool\" ; Additional Resources","title":"Custom Headers"},{"location":"kb/nginx/nginx-redirects-to-the-first-alphabetical-site-when-not-found-in-config/","text":"nginx redirects to the first alphabetical site when not found in config This is expected This is expected behaviour of nginx, and when you think about it, it makes sense. If you point a DNS entry towards the server that it does not recognise, it will send it to the first page it has, and when there is no site sepcified as default, it falls back to alphabetical. This is a weird thing I saw when I was migrating sites. Move DNS from old server to new server Forget to create site Browse to URL Sends traffic to anal.breadnet.co.uk (filename anal.breadnet.co.uk ) When I changed another site to be aa-<> it sent it to that site. What seems to be happening is that a site is elected as default, based on alphabetical order in the sites enabled directory. How to fix Create a site called default server { listen 80 default_server ; listen [::]:80 default_server ; listen 443 default_server ssl ; listen [::]:443 default_server ssl ; return 444 ; # silently drop the connection # or you can define some landing page here } This will return a 444 What is http:444 Enable the site by creating a symbolic link ( ln -s ) Additional Resources Nginx doesnt listen on port 80 twice Block access with https ipaddress on nginx Nginx redirects to the first alphabetical site when not found in config","title":"nginx redirects to the first alphabetical site when not found in config"},{"location":"kb/nginx/nginx-redirects-to-the-first-alphabetical-site-when-not-found-in-config/#nginx-redirects-to-the-first-alphabetical-site-when-not-found-in-config","text":"This is expected This is expected behaviour of nginx, and when you think about it, it makes sense. If you point a DNS entry towards the server that it does not recognise, it will send it to the first page it has, and when there is no site sepcified as default, it falls back to alphabetical. This is a weird thing I saw when I was migrating sites. Move DNS from old server to new server Forget to create site Browse to URL Sends traffic to anal.breadnet.co.uk (filename anal.breadnet.co.uk ) When I changed another site to be aa-<> it sent it to that site. What seems to be happening is that a site is elected as default, based on alphabetical order in the sites enabled directory.","title":"nginx redirects to the first alphabetical site when not found in config"},{"location":"kb/nginx/nginx-redirects-to-the-first-alphabetical-site-when-not-found-in-config/#how-to-fix","text":"Create a site called default server { listen 80 default_server ; listen [::]:80 default_server ; listen 443 default_server ssl ; listen [::]:443 default_server ssl ; return 444 ; # silently drop the connection # or you can define some landing page here } This will return a 444 What is http:444 Enable the site by creating a symbolic link ( ln -s )","title":"How to fix"},{"location":"kb/nginx/nginx-redirects-to-the-first-alphabetical-site-when-not-found-in-config/#additional-resources","text":"Nginx doesnt listen on port 80 twice Block access with https ipaddress on nginx Nginx redirects to the first alphabetical site when not found in config","title":"Additional Resources"},{"location":"kb/nginx/nginxservice-failed-because-the-control-process-exited/","text":"nginx Failed because the Control process exited Recently got this after getting a cert for bookstack.breadnet.co.uk Run: sudo fuser -k 80 /tcp sudo fuser -k 443 /tcp Failing that sudo netstat -tulpn","title":"nginx.service failed because the control process exited"},{"location":"kb/nginx/nginxservice-failed-because-the-control-process-exited/#nginx-failed-because-the-control-process-exited","text":"Recently got this after getting a cert for bookstack.breadnet.co.uk Run: sudo fuser -k 80 /tcp sudo fuser -k 443 /tcp Failing that sudo netstat -tulpn","title":"nginx Failed because the Control process exited"},{"location":"kb/nginx/remove-server-headers/","text":"Remove server Headers Install nginx extras Add below server_tokens off ; more_set_headers \"Server: gws\" ; # (1)! Set's the server header to Google web server","title":"Remove server headers"},{"location":"kb/nginx/remove-server-headers/#remove-server-headers","text":"Install nginx extras Add below server_tokens off ; more_set_headers \"Server: gws\" ; # (1)! Set's the server header to Google web server","title":"Remove server Headers"},{"location":"kb/nginx/reverse-web-proxy/","text":"Reverse web proxy go to /etc/nginx/sites-available copy the 'default' file to the new reverse name, so status.breadnet.co.uk Edit the file to reflect the name and address server { server_name <your domain> ; add_header Strict-Transport-Security \"max-age=15552000 ; includeSubDomains\" always ; location / { proxy_pass http://<ip address or hostname of server you want accessable> ; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 ; proxy_set_header Host $host ; proxy_set_header X-Real-IP $remote_addr ; proxy_set_header X-Forward-For $proxy_add_x_forwarded_for ; proxy_set_header X-Forwarded-Proto https ; proxy_redirect off ; proxy_read_timeout 5m ; } client_max_body_size 10M ; listen [::]:443 ssl ; # managed by Certbot listen 443 ssl ; # managed by Certbot ssl_certificate /etc/letsencrypt/live/<domain>/fullchain.pem ; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/<domain>/privkey.pem ; # managed by Certbot include /etc/letsencrypt/options-ssl-nginx.conf ; # managed by Certbot ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem ; # managed by Certbot } server { if ( $host = <domain>) { return 301 https:// $host$request_uri ; } # managed by Certbot listen 80 ; listen [::]:80 ; server_name <domain> ; return 404 ; # managed by Certbot } save it create the symbolic link sudo ln -s /etc/nginx/sites-available/name.breadnet.co.uk /etc/nginx/sites-enabled/name.breadnet.co.uk Adding a cert sudo certbot --nginx -d name.breadnet.co.uk","title":"Reverse web proxy"},{"location":"kb/nginx/reverse-web-proxy/#reverse-web-proxy","text":"go to /etc/nginx/sites-available copy the 'default' file to the new reverse name, so status.breadnet.co.uk Edit the file to reflect the name and address server { server_name <your domain> ; add_header Strict-Transport-Security \"max-age=15552000 ; includeSubDomains\" always ; location / { proxy_pass http://<ip address or hostname of server you want accessable> ; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 ; proxy_set_header Host $host ; proxy_set_header X-Real-IP $remote_addr ; proxy_set_header X-Forward-For $proxy_add_x_forwarded_for ; proxy_set_header X-Forwarded-Proto https ; proxy_redirect off ; proxy_read_timeout 5m ; } client_max_body_size 10M ; listen [::]:443 ssl ; # managed by Certbot listen 443 ssl ; # managed by Certbot ssl_certificate /etc/letsencrypt/live/<domain>/fullchain.pem ; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/<domain>/privkey.pem ; # managed by Certbot include /etc/letsencrypt/options-ssl-nginx.conf ; # managed by Certbot ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem ; # managed by Certbot } server { if ( $host = <domain>) { return 301 https:// $host$request_uri ; } # managed by Certbot listen 80 ; listen [::]:80 ; server_name <domain> ; return 404 ; # managed by Certbot } save it create the symbolic link sudo ln -s /etc/nginx/sites-available/name.breadnet.co.uk /etc/nginx/sites-enabled/name.breadnet.co.uk Adding a cert sudo certbot --nginx -d name.breadnet.co.uk","title":"Reverse web proxy"},{"location":"kb/other/downloading-the-breadnet-site-and-serving-a-stale-copy/","text":"Downloading the breadNET site and serving a stale copy This is part of a project to serve the breadnet site when the server goes down for maintenance. Download the site wget \\ --recursive \\ --no-clobber \\ --page-requisites \\ --html-extension \\ --convert-links \\ --no-parent \\ breadnet.co.uk Replace broken links grep -rli 'jpgg' * | LC_ALL = C xargs sed -i '' 's/jpgg/jpg/g' grep -rli 'JPGG' * | LC_ALL = C xargs sed -i '' 's/JPGG/jpg/g' grep -rli 'jpgpg' * | LC_ALL = C xargs sed -i '' 's/jpgpg/jpg/g' grep -rli 'JPGPG' * | LC_ALL = C xargs sed -i '' 's/JPGPG/jpg/g' grep -rli 'jpgjpg' * | LC_ALL = C xargs sed -i '' 's/jpgjpg/jpg/g' grep -rli 'JPGJPG' * | LC_ALL = C xargs sed -i '' 's/JPGJPG/jpg/g' grep -rli 'pngg' * | xargs sed -i '' 's/pngg/png/g' grep -rli 'pngng' * | xargs sed -i '' 's/pngng/png/g' grep -rli 'pngpng' * | xargs sed -i '' 's/pngpng/png/g' grep -rli 'svgg' * | xargs sed -i '' 's/svgg/svg/g' grep -rli 'svgvg' * | xargs sed -i '' 's/svgvg/svg/g' grep -rli 'svgsvg' * | xargs sed -i '' 's/svgsg/svg/g' Serve the site docker run -p 8080 :80 -v $( pwd ) :/usr/share/nginx/html/ -it nginx","title":"Serving breadNET when server is offline"},{"location":"kb/other/downloading-the-breadnet-site-and-serving-a-stale-copy/#downloading-the-breadnet-site-and-serving-a-stale-copy","text":"This is part of a project to serve the breadnet site when the server goes down for maintenance.","title":"Downloading the breadNET site and serving a stale copy"},{"location":"kb/other/downloading-the-breadnet-site-and-serving-a-stale-copy/#download-the-site","text":"wget \\ --recursive \\ --no-clobber \\ --page-requisites \\ --html-extension \\ --convert-links \\ --no-parent \\ breadnet.co.uk","title":"Download the site"},{"location":"kb/other/downloading-the-breadnet-site-and-serving-a-stale-copy/#replace-broken-links","text":"grep -rli 'jpgg' * | LC_ALL = C xargs sed -i '' 's/jpgg/jpg/g' grep -rli 'JPGG' * | LC_ALL = C xargs sed -i '' 's/JPGG/jpg/g' grep -rli 'jpgpg' * | LC_ALL = C xargs sed -i '' 's/jpgpg/jpg/g' grep -rli 'JPGPG' * | LC_ALL = C xargs sed -i '' 's/JPGPG/jpg/g' grep -rli 'jpgjpg' * | LC_ALL = C xargs sed -i '' 's/jpgjpg/jpg/g' grep -rli 'JPGJPG' * | LC_ALL = C xargs sed -i '' 's/JPGJPG/jpg/g' grep -rli 'pngg' * | xargs sed -i '' 's/pngg/png/g' grep -rli 'pngng' * | xargs sed -i '' 's/pngng/png/g' grep -rli 'pngpng' * | xargs sed -i '' 's/pngpng/png/g' grep -rli 'svgg' * | xargs sed -i '' 's/svgg/svg/g' grep -rli 'svgvg' * | xargs sed -i '' 's/svgvg/svg/g' grep -rli 'svgsvg' * | xargs sed -i '' 's/svgsg/svg/g'","title":"Replace broken links"},{"location":"kb/other/downloading-the-breadnet-site-and-serving-a-stale-copy/#serve-the-site","text":"docker run -p 8080 :80 -v $( pwd ) :/usr/share/nginx/html/ -it nginx","title":"Serve the site"},{"location":"kb/php/install-php/","text":"Install PHP sudo apt-get install software-properties-common sudo add-apt-repository ppa:ondrej/php sudo apt install php7.2","title":"Install PHP"},{"location":"kb/php/install-php/#install-php","text":"sudo apt-get install software-properties-common sudo add-apt-repository ppa:ondrej/php sudo apt install php7.2","title":"Install PHP"},{"location":"kb/php/wordpress-permissions/","text":"WordPress permissions chown www-data:www-data -R * # Let Apache be owner find . -type d -exec chmod 755 {} \\; # Change directory permissions rwxr-xr-x find . -type f -exec chmod 644 {} \\; # Change file permissions rw-r--r--","title":"Wordpress permissions"},{"location":"kb/php/wordpress-permissions/#wordpress-permissions","text":"chown www-data:www-data -R * # Let Apache be owner find . -type d -exec chmod 755 {} \\; # Change directory permissions rwxr-xr-x find . -type f -exec chmod 644 {} \\; # Change file permissions rw-r--r--","title":"WordPress permissions"},{"location":"kb/public-web-facing/bookstack/","text":"Bookstack How to enable Dark Mode on Bookstack Note This was used before bookstack had a dark theme. Do not use! < link rel = \"stylesheet\" href = \"https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.29.0/theme/blackboard.min.css\" /> < script > window . codeTheme = 'blackboard' ;</ script > < link rel = \"stylesheet\" href = \"https://bookstack.breadnet.co.uk/css/dark-theme.css\" > < style > code { color : #50fa7b ; background-color : #282b36 ; white-space : pre-line ;} . page-content del { background : #FF5555 !important ;} . card . entity-list-item : not ( . no-hover ) : hover { background-color : #44475a !important ;} . page-content ins { background : #50fa7b !important ; color : #282a36 !important ;} . suggestion-box { background-color : #282a36 !important ; border : 1 px solid #BBB ; box-shadow : none !important ; border-radius : 0 !important ;} . suggestion-box li : last-child { border-bottom : 0 ; } . suggestion-box li . active { background-color : #44475a !important ;} . card . entity-list-item : not ( . no-hover ) : hover { background-color : #44475a !important ;} . card-title { text-transform : uppercase ; color : #f8f8f2 ; fill : #f8f8f2 ; font-weight : 400 ;} . markdown-editor-display { background-color : #282a36 ;} . markdown-editor-display body { background-color : #282a36 ;} . breadcrumb-listing-entity-list { background-color : #282a36 !important ;} . breadcrumb-listing . breadcrumb-listing-toggle : hover { fill : #f8f8f2 ; border-color : transparent ;} . entity-meta { fill : #f8f8f2 !important ;} . book-tree . sidebar-page-list . entity-list-item . selected { background-color : #44475a ;} table . table tr : hover { background-color : #44475a ;} . tri-layout-mobile-tab : first-child { border-right : 0 ; background : #282a36 ;} . tri-layout-mobile-tab { text-align : center ; border-bottom : 0 ; cursor : pointer ; background : #282a36 ;} . tri-layout-mobile-tab . active { border-bottom-color : none ; background : #44475a ;} . tri-layout-mobile-tabs { border : 0 ;} . entity-selector . entity-list-item { background-color : #282a36 ;} . entity-selector { border : 0 } blockquote { border-left : 4 px solid #8be9fd ; background-color : transparent !important ;} header . header-links { background-color : transparent ;} header . links a : hover , header . dropdown-container ul li a : hover { background-color : #44475a ;} @ media screen and ( max-width : 1000px ) { header . header-links { background-color : #282a36 ;}} </ style >","title":"Bookstack"},{"location":"kb/public-web-facing/bookstack/#bookstack","text":"How to enable Dark Mode on Bookstack Note This was used before bookstack had a dark theme. Do not use! < link rel = \"stylesheet\" href = \"https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.29.0/theme/blackboard.min.css\" /> < script > window . codeTheme = 'blackboard' ;</ script > < link rel = \"stylesheet\" href = \"https://bookstack.breadnet.co.uk/css/dark-theme.css\" > < style > code { color : #50fa7b ; background-color : #282b36 ; white-space : pre-line ;} . page-content del { background : #FF5555 !important ;} . card . entity-list-item : not ( . no-hover ) : hover { background-color : #44475a !important ;} . page-content ins { background : #50fa7b !important ; color : #282a36 !important ;} . suggestion-box { background-color : #282a36 !important ; border : 1 px solid #BBB ; box-shadow : none !important ; border-radius : 0 !important ;} . suggestion-box li : last-child { border-bottom : 0 ; } . suggestion-box li . active { background-color : #44475a !important ;} . card . entity-list-item : not ( . no-hover ) : hover { background-color : #44475a !important ;} . card-title { text-transform : uppercase ; color : #f8f8f2 ; fill : #f8f8f2 ; font-weight : 400 ;} . markdown-editor-display { background-color : #282a36 ;} . markdown-editor-display body { background-color : #282a36 ;} . breadcrumb-listing-entity-list { background-color : #282a36 !important ;} . breadcrumb-listing . breadcrumb-listing-toggle : hover { fill : #f8f8f2 ; border-color : transparent ;} . entity-meta { fill : #f8f8f2 !important ;} . book-tree . sidebar-page-list . entity-list-item . selected { background-color : #44475a ;} table . table tr : hover { background-color : #44475a ;} . tri-layout-mobile-tab : first-child { border-right : 0 ; background : #282a36 ;} . tri-layout-mobile-tab { text-align : center ; border-bottom : 0 ; cursor : pointer ; background : #282a36 ;} . tri-layout-mobile-tab . active { border-bottom-color : none ; background : #44475a ;} . tri-layout-mobile-tabs { border : 0 ;} . entity-selector . entity-list-item { background-color : #282a36 ;} . entity-selector { border : 0 } blockquote { border-left : 4 px solid #8be9fd ; background-color : transparent !important ;} header . header-links { background-color : transparent ;} header . links a : hover , header . dropdown-container ul li a : hover { background-color : #44475a ;} @ media screen and ( max-width : 1000px ) { header . header-links { background-color : #282a36 ;}} </ style >","title":"Bookstack"},{"location":"kb/public-web-facing/cachet/","text":"Cachet When you change anything with Cachet, ensure that you clear the cache! run the below in the cachet directory rm -rf bootstrap/cache/* then restart apache2 then: php artisan cache:clear","title":"Cachet"},{"location":"kb/public-web-facing/cachet/#cachet","text":"When you change anything with Cachet, ensure that you clear the cache! run the below in the cachet directory rm -rf bootstrap/cache/* then restart apache2 then: php artisan cache:clear","title":"Cachet"},{"location":"kb/public-web-facing/certbot/","text":"certbot Installing certbot sudo apt-get install certbot -y Getting a cert (no apache / ngix config) certbot -d <domain> --manual --preferred-challenges dns certonly --staple-ocsp -m legal@breadnet.co.uk --agree-tos To get a certificate for nginx sudo certbot --nginx -d example.com -d www.example.com","title":"Certbot"},{"location":"kb/public-web-facing/certbot/#certbot","text":"Installing certbot sudo apt-get install certbot -y Getting a cert (no apache / ngix config) certbot -d <domain> --manual --preferred-challenges dns certonly --staple-ocsp -m legal@breadnet.co.uk --agree-tos To get a certificate for nginx sudo certbot --nginx -d example.com -d www.example.com","title":"certbot"},{"location":"kb/public-web-facing/check-passbolt-is-healthy/","text":"Check Passbolt is Healthy Exec in to the container Run the below in bin/ ./cake passbolt healthcheck","title":"Check passbolt is Healthy"},{"location":"kb/public-web-facing/check-passbolt-is-healthy/#check-passbolt-is-healthy","text":"Exec in to the container Run the below in bin/ ./cake passbolt healthcheck","title":"Check Passbolt is Healthy"},{"location":"kb/public-web-facing/jellyfin-s3/","text":"Jellyfin using S3 and Docker DO NOT USE THIS IS ONLY MIGRATED TO KEEP LINKS ALIVE, AND SHOULD NEVER BE USED. IT'S INSECURE AND DOES NOT FOLLOW BEST PRACTICES. Prereqs: I am not a security expert, I am a sysadmin. The security of your server is up to you. Do basic things like enable ssh keys and lock down ingress ports with ufw or iptables. What ever angles your dangle. I am not a docker expert. I am a sysadmin which means I can use google (I think? bleh) so if you see anything whack in this, feel free to email me (webmaster[at]breadnet[dot]co[dot]uk) Comment: 3rd may 2020: After running this for a few days it's come to my attention that for this to be viable and be able to transcode you will need a slightly more powerful host. Whilst this does feel like a kick in the nuts, its not. There are lots of technologies that still work here. You will need: S3 compatible bucket - I suggest wasabi but you can also use Min.io if you're in to FOSS (wasabi has a 30 day free 1tb trial) Cloud vps somewhere with reasonable connection and sub 20ms ping to the bucket location. Not needed but is nice. Internet should be around 30mbps at min Ability to read and copy and paste. The first step First we will spin up a digital ocean droplet but you can use what ever VPS provider you want. Once the VPs is up and you're logged in run the below to update the sources and crap. should not take too long. sudo apt-get update && sudo apt-get upgrade Once that's done, install rclone sudo apt-get install rclone Now we need to configure the s3 bucket. I am using wasabi like I said to begin with, but some parts are universal to all s3 compatible storage options. Login to wasabi console at https://console.wasabisys.com Once logged in, go to buckets and create bucket - name it what you want. call it Jeff for all I care :) Now that your bucket is created, we need to create a policy that will later be applied to a user. Go to policies on the left hand nav bar and create new policy (top right) Name your policy something like jeff-RO (where the appending characters reference the permissions, here it's Read Only) The below policy is full access to that bucket. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": [ \"s3:GetBucketLocation\", \"s3:ListBucket\", \"s3:ListBucketMultipartUploads\" ], \"Resource\": \"arn:aws:s3:::jeff\" }, { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::jeff/*\" } ] } Below is full rw access { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"s3:ListAllMyBuckets\", \"Resource\": \"arn:aws:s3:::*\" }, { \"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::jeff\", \"arn:aws:s3:::jeff/*\" ] } ] } Once your policy is created you can now move on to creating the user. Go to users and then click 'create user' and follow though the prompts. Click API access: Now follow though the prompts till you get to the end. Don't click anything as it will then pop up with the keys. Click download. Now we can go to the vps we created (or pre-existed) and start with the docker setup and eventually rclone. Install dooker with: curl -fsSL https://get.docker.com -o get-docker.sh sh get-docker.sh sudo apt-get install docker-compose Once that is installed pull the jellyfin docker image docker pull jellyfin/jellyfin Create 3 folders in a jellyfin folder. I like mine to be like the below (dont worry about that file, that's next) . \u2514\u2500\u2500 docker \u2514\u2500\u2500 jellyfin \u251c\u2500\u2500 docker-compose.yml \u2514\u2500\u2500 files \u251c\u2500\u2500 cache \u251c\u2500\u2500 config \u2514\u2500\u2500 media 6 directories, 1 file Create a file called docker-compose.yml and paste the below in to it: (anything encased in <> should be changed to map to your folders) version: \"3\" services: jellyfin: image: jellyfin/jellyfin user: 1000:1000 network_mode: \"host\" volumes: - </path/to/config>:/config - </path/to/cache>:/cache - </path/to/media>:/media Now we need to sort the rclone business out. For this I suggest using screen. It can be opened by typing screen in a shell session (over ssh to the server) If it opens, close it with Ctrl + a + d (the a key followed by the d key) If it gives you shit about not having it installed run sudo apt-get install screen -y Now we need to setup rclone cd ~/.config mkdir rclone cd rclone nano rclone.conf [media] type = s3 env_auth = access_key_id = <first key in .csv file from user creation> secret_access_key = <second key from user creation> region = endpoint = https://s3.eu-central-1.wasabisys.com #if using a europe bucket, else; See below for specifics location_constraint = acl = server_side_encryption = storage_class = ####You dont need to copy this shit in to the file, it's in reference to line 7### #Wasabi US East 1 (N. Virginia): s3.wasabisys.com or s3.us-east-1.wasabisys.com #Wasabi US East 2 (N. Virginia): s3.us-east-2.wasabisys.com #Wasabi US West 1 (Oregon): s3.us-west-1.wasabisys.com #Wasabi EU Central 1 (Amsterdam): s3.eu-central-1.wasabisys.com Test this by going to the wasabi interface and uploading something to the bucket. then do rclone ls media:<bucket name, probably jeff> Should return the file name. If you have issues, contact me. You can email me at the webmaster email address and I'll be happy to help where I can. Next we need to map the bucket to a folder. Open screen by typing screen Change directory to the media folder you created erlier and run: rclone mount media:<bucket name> media --allow-others It may throw back an error about using --allow-others, but just do what the command says and edit the file, remove the # in from of the allow_others or what's closest in the file. It should return nothing if it worked. Press ctrl + a + d try ls on the media folder. We should see the file we uploaded from earlier. Now we can build the jellyfin docker image what's it. In the folder where the docker-compose.yml file exists, run: docker-compose up If all went well it should spin up and then you can visit the IP address for the vm you're working on with the jellyfin port number and then add the media like you would usually: I suggest you create the folders though wasabi webui and upload the files there if you're not 100% with rclone. That should be it. Let it whir away and it should be all fine and dandy. This is the VPS status for usage and disk usage: Like I said, if there's any issues or you're confused to me, reach out to me on Reddit or email!","title":"Jellyfin S3"},{"location":"kb/public-web-facing/jellyfin-s3/#jellyfin-using-s3-and-docker","text":"DO NOT USE THIS IS ONLY MIGRATED TO KEEP LINKS ALIVE, AND SHOULD NEVER BE USED. IT'S INSECURE AND DOES NOT FOLLOW BEST PRACTICES.","title":"Jellyfin using S3 and Docker"},{"location":"kb/public-web-facing/send-test-email-on-passbolt/","text":"Send test email on Passbolt Exec in to the container docker exec -it passbolt /bin/sh move to bin ./cake passbolt send_test_email test@breadnet.co.uk Check Passbolt is healthy","title":"Send test email on passbolt"},{"location":"kb/public-web-facing/send-test-email-on-passbolt/#send-test-email-on-passbolt","text":"Exec in to the container docker exec -it passbolt /bin/sh move to bin ./cake passbolt send_test_email test@breadnet.co.uk Check Passbolt is healthy","title":"Send test email on Passbolt"},{"location":"kb/public-web-facing/wildcard-certificates/","text":"Wildcard Certificates certbot certonly \\ --manual \\ --preferred-challenges = dns \\ --email legal@breadnet.co.uk \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --manual-public-ip-logging-ok \\ -d *.example.com","title":"Wildcart Certificates"},{"location":"kb/public-web-facing/wildcard-certificates/#wildcard-certificates","text":"certbot certonly \\ --manual \\ --preferred-challenges = dns \\ --email legal@breadnet.co.uk \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --manual-public-ip-logging-ok \\ -d *.example.com","title":"Wildcard Certificates"},{"location":"kb/ssh/importing-ssh-keys-from-github/","text":"Importing SSH keys from GitHub wget -O - https://github.com/userbradley.keys >> ~/.ssh/authorized_keys","title":"Importing SSH keys from GitHUb"},{"location":"kb/ssh/importing-ssh-keys-from-github/#importing-ssh-keys-from-github","text":"wget -O - https://github.com/userbradley.keys >> ~/.ssh/authorized_keys","title":"Importing SSH keys from GitHub"},{"location":"kb/ssh/ssh-client-setup-using-keys/","text":"SSH client setup using keys The following steps will setup your ssh keypairs on your local machine, copy the public key to your server, and configure your ssh client to use a specific private key with a server alias. First, generate your keypair, I generally name the keys with my username-service, username-hostname, or username-device, sometimes even a combination of the three. To generate keys for a new server at securedomain.com ssh-keygen -t rsa -b 4096 -C 'email@securedomain.com' Things to keep in mind: The email does not have to be the same domain as the service/server you are connecting to The -t rsa -b 4096 options are safe and will work on most servers, you can also use -t ed25519 if you wish. Once you're in the ssh-keygen prompt it will ask you to provide a name for your keys (you can also do this in the command itself using the -f <filename> ) Generating public/private rsa key pair. Enter file in which to save the key (/home/user/.ssh/id_rsa): <new_key_name> The ssh-keygen application will then, prompt you for a passphrase to secure your key. (You don't have to, but it is recommended). If you don't wish to use a passphrase just press ENTER twice. Now, we need to copy our keys to the server, we do so by entering the following ssh-copy-id -i .ssh/yourkeyfile username@hostname This will effectively copy your keyfile over to the new server in a secure fashion. More reading on this here. Next, you will want to add the key to your .ssh/config to be used automatically with the specified host it was created for; this saves the leg work of having to remember which key goes with which host, and also from having to type -i /path/to/key options with your ssh command. To do this, first we need to edit our config, so open up .ssh/config in your preferred editor and enter the following: # EXAMPLE # This will setup the use of example as an alias for the FQDN of the server you want to connect to Host example Hostname example.securedomain.com AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/securedomain_username In this config the following are true: Host is an alias to the server we wish to connect to, it does not have to match the domain name. Hostname is the actual FQDN of the server we wish to connect to AddKeysToAgent tells ssh to add the specified keyfiles to our existing s sh-agent UseKeychain tells ssh to utilize the keychain , which either starts the ssh-agent, or connects to an already running instance saving the trouble of typing the passphrase for a given key if you're logging in and out of a server frequently. IdentityFile is your keyfile you wish to use for the host you are configuring; probably the key you just generated. Using this config and example; we can now use the following command ssh user@example Which using this config, in the background is expanded out to the following command ssh -i ~/.ssh/securedomain_username user@example.securedomain.com","title":"SSH client setup using keys"},{"location":"kb/ssh/ssh-client-setup-using-keys/#ssh-client-setup-using-keys","text":"","title":"SSH client setup using keys"},{"location":"kb/ssh/ssh-keys/","text":"SSH keys generate SSH keys and copy them to host On the host you are logged in to, but want to ssh to another host with no password run the below ssh-keygen -t ed25519 Copy them to server then run this but put the server name where it asks for remote host. Don't worry about using a username as they are usually the one you're logged in as. cat ~/.ssh/id_rsa.pub | ssh username@remote_host \"mkdir -p ~/.ssh && cat >> ~/.ssh/authorized_keys\"","title":"SSH Keys"},{"location":"kb/ssh/ssh-keys/#ssh-keys","text":"","title":"SSH keys"},{"location":"kb/ssh/ssh-keys/#generate-ssh-keys-and-copy-them-to-host","text":"On the host you are logged in to, but want to ssh to another host with no password run the below ssh-keygen -t ed25519","title":"generate SSH keys and copy them to host"},{"location":"kb/ssh/ssh-keys/#copy-them-to-server","text":"then run this but put the server name where it asks for remote host. Don't worry about using a username as they are usually the one you're logged in as. cat ~/.ssh/id_rsa.pub | ssh username@remote_host \"mkdir -p ~/.ssh && cat >> ~/.ssh/authorized_keys\"","title":"Copy them to server"},{"location":"kb/ssh/ssh-port-redirection/","text":"SSH port redirection when you want to access a remote port on a client, but cant be asked to port forward, but can access over ssh say a service is running on server1 on port 8090 and we want to access it from our device over port 2020 ssh -L2020:localhost:8090 server1","title":"SSH Port redirection"},{"location":"kb/ssh/ssh-port-redirection/#ssh-port-redirection","text":"when you want to access a remote port on a client, but cant be asked to port forward, but can access over ssh say a service is running on server1 on port 8090 and we want to access it from our device over port 2020 ssh -L2020:localhost:8090 server1","title":"SSH port redirection"},{"location":"kb/ssh/sshuttle/","text":"sshuttle To connect as a poor man's vpn over ssh sshuttle -r user@server 0 .0.0.0/0 More can be found on the documentation site https://sshuttle.readthedocs.io/en/stable/","title":"sshuttle"},{"location":"kb/ssh/sshuttle/#sshuttle","text":"To connect as a poor man's vpn over ssh sshuttle -r user@server 0 .0.0.0/0 More can be found on the documentation site https://sshuttle.readthedocs.io/en/stable/","title":"sshuttle"},{"location":"kb/ssh/weird-bash/","text":"Weird bash If you log on to a server and there's no auto complete etc, run chsh set to /bin/bash logout and back in. problem solved","title":"Weird Bash"},{"location":"kb/ssh/weird-bash/#weird-bash","text":"If you log on to a server and there's no auto complete etc, run chsh set to /bin/bash logout and back in. problem solved","title":"Weird bash"},{"location":"kb/wasabi/policies/","text":"Wasabi: Policies Policies you should use { \"Version\" : \"2012-10-17\" , \"Statement\" :[ { \"Effect\" : \"Allow\" , \"Action\" :[ \"s3:ListAllMyBuckets\" ], \"Resource\" : \"arn:aws:s3:::*\" }, { \"Effect\" : \"Allow\" , \"Action\" :[ \"s3:ListBucket\" , \"s3:GetBucketLocation\" ], \"Resource\" : \"arn:aws:s3:::<bucket>\" }, { \"Effect\" : \"Allow\" , \"Action\" :[ \"s3:PutObject\" , \"s3:PutObjectAcl\" , \"s3:GetObject\" , \"s3:GetObjectAcl\" , \"s3:DeleteObject\" ], \"Resource\" : \"arn:aws:s3:::<bucket>/*\" } ] } Allow access to bucket policy { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"s3:ListAllMyBuckets\" , \"Resource\" : \"arn:aws:s3:::*\" }, { \"Effect\" : \"Allow\" , \"Action\" : \"s3:*\" , \"Resource\" : [ \"arn:aws:s3:::<bucket>\" , \"arn:aws:s3:::<bucket>/*\" ] } ] } { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"*\" }, \"Action\" : [ \"s3:GetBucketLocation\" , \"s3:ListBucket\" , \"s3:ListBucketMultipartUploads\" ], \"Resource\" : \"arn:aws:s3:::<bucekt>\" }, { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"*\" }, \"Action\" : \"s3:GetObject\" , \"Resource\" : \"arn:aws:s3:::<bucekt>/*\" } ] }","title":"Policies"},{"location":"kb/wasabi/policies/#wasabi-policies","text":"","title":"Wasabi: Policies"},{"location":"kb/wasabi/policies/#policies-you-should-use","text":"{ \"Version\" : \"2012-10-17\" , \"Statement\" :[ { \"Effect\" : \"Allow\" , \"Action\" :[ \"s3:ListAllMyBuckets\" ], \"Resource\" : \"arn:aws:s3:::*\" }, { \"Effect\" : \"Allow\" , \"Action\" :[ \"s3:ListBucket\" , \"s3:GetBucketLocation\" ], \"Resource\" : \"arn:aws:s3:::<bucket>\" }, { \"Effect\" : \"Allow\" , \"Action\" :[ \"s3:PutObject\" , \"s3:PutObjectAcl\" , \"s3:GetObject\" , \"s3:GetObjectAcl\" , \"s3:DeleteObject\" ], \"Resource\" : \"arn:aws:s3:::<bucket>/*\" } ] }","title":"Policies you should use"},{"location":"kb/wasabi/policies/#allow-access-to-bucket-policy","text":"{ \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"s3:ListAllMyBuckets\" , \"Resource\" : \"arn:aws:s3:::*\" }, { \"Effect\" : \"Allow\" , \"Action\" : \"s3:*\" , \"Resource\" : [ \"arn:aws:s3:::<bucket>\" , \"arn:aws:s3:::<bucket>/*\" ] } ] } { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"*\" }, \"Action\" : [ \"s3:GetBucketLocation\" , \"s3:ListBucket\" , \"s3:ListBucketMultipartUploads\" ], \"Resource\" : \"arn:aws:s3:::<bucekt>\" }, { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"*\" }, \"Action\" : \"s3:GetObject\" , \"Resource\" : \"arn:aws:s3:::<bucekt>/*\" } ] }","title":"Allow access to bucket policy"},{"location":"kb/wasabi/transport-endpoint-is-not-connected/","text":"Transport endpoint is not connected This sometimes comes about when you connect to an S3 bucket via rclone then kill it, user@host:/mnt# ls ls: cannot access 's3' : Transport endpoint is not connected ls: cannot access 'mkdir' : Transport endpoint is not connected ls: cannot access 's4' : Transport endpoint is not connected mkdir s3 s4 To solve this: fusermount -uz <name>","title":"Transport endpoint is not connected"},{"location":"kb/wasabi/transport-endpoint-is-not-connected/#transport-endpoint-is-not-connected","text":"This sometimes comes about when you connect to an S3 bucket via rclone then kill it, user@host:/mnt# ls ls: cannot access 's3' : Transport endpoint is not connected ls: cannot access 'mkdir' : Transport endpoint is not connected ls: cannot access 's4' : Transport endpoint is not connected mkdir s3 s4","title":"Transport endpoint is not connected"},{"location":"kb/wasabi/transport-endpoint-is-not-connected/#to-solve-this","text":"fusermount -uz <name>","title":"To solve this:"},{"location":"kubernetes/","text":"Kubernetes Folders KB GKE k3s Kubernetes has been awarded its own section as it encompasses the below: Cloud Hosted Kubernetes Managing Kubernetes Clusters On premise Cloud Commands kubectl Terraforming Clusters","title":"Kubernetes"},{"location":"kubernetes/#kubernetes","text":"","title":"Kubernetes"},{"location":"kubernetes/#folders","text":"KB GKE k3s Kubernetes has been awarded its own section as it encompasses the below: Cloud Hosted Kubernetes Managing Kubernetes Clusters On premise Cloud Commands kubectl Terraforming Clusters","title":"Folders"},{"location":"kubernetes/gke/","text":"GKE Specific Reading Kubernetes KB GKE K3S","title":"GKE"},{"location":"kubernetes/gke/#gke","text":"","title":"GKE"},{"location":"kubernetes/gke/#specific-reading","text":"Kubernetes KB GKE K3S","title":"Specific Reading"},{"location":"kubernetes/gke/service-account-with-workload-id/","text":"Kubernetes Service account with Workload ID apiVersion : v1 kind : ServiceAccount metadata : name : bradley annotations : iam.gke.io/gcp-service-account : <gservice account email> You will need to grant the GCP SA the below roles PROJECT_ID.svc.id.goog [ NAMESPACE/KSA ] As well as: roles/iam.workloadIdentityUser These need to be granted to the GCP SA in GCP See Workload Identity (Page coming soon)","title":"Workload ID KSA"},{"location":"kubernetes/gke/service-account-with-workload-id/#kubernetes-service-account-with-workload-id","text":"apiVersion : v1 kind : ServiceAccount metadata : name : bradley annotations : iam.gke.io/gcp-service-account : <gservice account email> You will need to grant the GCP SA the below roles PROJECT_ID.svc.id.goog [ NAMESPACE/KSA ] As well as: roles/iam.workloadIdentityUser These need to be granted to the GCP SA in GCP See Workload Identity (Page coming soon)","title":"Kubernetes Service account with Workload ID"},{"location":"kubernetes/gke/workload-id-test/","text":"Workload Identity test pod Why Sometimes you need to run a simple pod on the cluster to check things, call a service or run commands Things to note You will need to have the compute metadata api service running in the cluster How KSA GKE Workload nodes apiVersion : v1 kind : Pod metadata : name : workload-identity-test spec : containers : - image : google/cloud-sdk:slim name : workload-identity-test command : [ \"sleep\" , \"infinity\" ] serviceAccountName : <> Note Sometimes your may have nodes in a pool that dont have the metadata server enabled. apiVersion : v1 kind : Pod metadata : name : workload-identity-test spec : containers : - image : google/cloud-sdk:slim name : workload-identity-test command : [ \"sleep\" , \"infinity\" ] serviceAccountName : <> nodeSelector : iam.gke.io/gke-metadata-server-enabled : \"true\" What to read next Get project using compute engine API Get GCP Service account using compute engine API","title":"Workload ID test pod"},{"location":"kubernetes/gke/workload-id-test/#workload-identity-test-pod","text":"","title":"Workload Identity test pod"},{"location":"kubernetes/gke/workload-id-test/#why","text":"Sometimes you need to run a simple pod on the cluster to check things, call a service or run commands","title":"Why"},{"location":"kubernetes/gke/workload-id-test/#things-to-note","text":"You will need to have the compute metadata api service running in the cluster","title":"Things to note"},{"location":"kubernetes/gke/workload-id-test/#how","text":"KSA GKE Workload nodes apiVersion : v1 kind : Pod metadata : name : workload-identity-test spec : containers : - image : google/cloud-sdk:slim name : workload-identity-test command : [ \"sleep\" , \"infinity\" ] serviceAccountName : <> Note Sometimes your may have nodes in a pool that dont have the metadata server enabled. apiVersion : v1 kind : Pod metadata : name : workload-identity-test spec : containers : - image : google/cloud-sdk:slim name : workload-identity-test command : [ \"sleep\" , \"infinity\" ] serviceAccountName : <> nodeSelector : iam.gke.io/gke-metadata-server-enabled : \"true\"","title":"How"},{"location":"kubernetes/gke/workload-id-test/#what-to-read-next","text":"Get project using compute engine API Get GCP Service account using compute engine API","title":"What to read next"},{"location":"kubernetes/k3s/","text":"K3S Specific Reading Kubernetes KB GKE K3S","title":"K3S"},{"location":"kubernetes/k3s/#k3s","text":"","title":"K3S"},{"location":"kubernetes/k3s/#specific-reading","text":"Kubernetes KB GKE K3S","title":"Specific Reading"},{"location":"kubernetes/k3s/lenovo-sff-ubuntu/","text":"This is a post to save other people some time and pain. What follows may apply to other systems using a similar Lenovo BIOS. Symptom If a user selects Optimal or Default BIOS options, wipes the pre-installed existing operating systems and partitions, and then installs a Linux distribution in UEFI mode on an SCU device, the ThinkStation S30 may return the following error and refuse to boot after successful installation. Error 1962: No operating system found. Description It appears that, prior to to following the BootOrder specified in the UEFI Boot Manager, the Lenovo BIOS first checks the UEFI Boot Manager configuration for an entry labelled \"Windows Boot Manager\".[1] If this entry is not found, an error is presented to the user and the boot sequence terminates. The Lenovo BIOS does not require that the entry labelled \"Windows Boot Manager\" be used, only that it be present. This may be related to the behaviour observed here. It would be good if someone from Lenovo could confirm the observations about the behaviour of the Lenovo UEFI BIOS. Solution Ensure that the SCU device is the set up as the first boot device in the BIOS Setup Primary Boot Sequence. Lenovo has documented this issue here. Ensure that there is an entry in the UEFI Boot Manager labeled \"Windows Boot Manager\" Add an Entry to the UEFI Boot Manager labeled \"Windows Boot Manager\" The following can be performed after installation of the operating system. The instructions assume that the user has access to \"Live\" installation media, is able to boot from that media and issue commands at the shell. It is also assumed that the user has a basic understanding of Linux devices references. The operating system must have been installed in UEFI mode and GRUB2 (or another bootloader) must have created at least one valid EFI boot file on the EFI system partition. Insert the Live installation media, shutdown and power off the system. Power on the system. Interrupt the boot sequence with Enter or F12 and select the live installation media for boot. Open a shell. Establish whether or not the Live installation provides the efibootmgr tool, if it does not, install the package that provides efibootmgr. As root, inspect the current configuration of the UEFI Boot Manager: sudo efibootmgr -v If there is already an entry labelled \"Windows Boot Manager\", the remainder of these instructions do not apply. If there is no entry, add one. For example: sudo efibootmgr -c -d /dev/sda -p 1 -l '\\EFI\\ubuntu\\grubx64.efi' -L \"Windows Boot Manager\" Please note that the option provided to the -l option can be any valid EFI boot loader. Verify that the entry was added to the UEFI Boot Manager: sudo efibootmgr -v Set the desired boot order of the UEFI Boot Manager, for example if the boot loader for Ubuntu is 0000 and, Windows Boot Loader dummy is 0001, and remaining boot devices are 000A-000C: sudo efibootmgr -o 0000,0001,000A,000B,000C Veryify that the BootOrder variable has been set correctly: sudo efibootmgr -v Optionally, adjust the value of the Timeout option to provide the user with more time to interrupt the boot process, for example, to extend the timeout to 10 seconds: sudo efibootmgr -t 10 Shutdown the system, remove the live installation media and reboot. After booting into the installed operating system, confirm that the BootCurrent variable has selected the correct loader and is behaving as expected. Please note, you may have to install efibootmgr in the installed operating system. sudo efiboogmgr -v If the above fails, consult the documentation of your boot loader and ensure that it is configured correctly. Also ensure that your EFI system partition is laid out correctly and formatted as FAT32. Notes: [1] It is possible that the BIOS checks for the presence of either \"Windows Boot Manager' or \"Red Hat Enterprise Linux\", however I have not tested this: \"Windows Boot Manager\" worked and that suits me. Others may prefer a different aesthetic. https://forums.lenovo.com/t5/ThinkStation/UEFI-Mode-installation-of-Linux-distributions-on-Thinkstation/td-p/1018555","title":"Lenovo sff ubuntu"},{"location":"kubernetes/kb/","text":"Kubernetes Knowledge Base This subpage holds kubernetes Knowledge base articles Specific Reading Kubernetes KB GKE K3S","title":"Kubernetes Knowledge Base"},{"location":"kubernetes/kb/#kubernetes-knowledge-base","text":"This subpage holds kubernetes Knowledge base articles","title":"Kubernetes Knowledge Base"},{"location":"kubernetes/kb/#specific-reading","text":"Kubernetes KB GKE K3S","title":"Specific Reading"},{"location":"kubernetes/kb/connect-to-container-that-has-sidecars/","text":"Connect to container that has sidecars admin exec -it -n <namespace> <pod name> -c <sidecar name> /bin/sh This makes some assumptions: You will need to know the namespace of the container first, use kubectl get namespaces and then kubectl get pods -n <namespace) This assumes there is /bin/sh installed. Some containers do not have a shell.","title":"Connect to container that has side car containers"},{"location":"kubernetes/kb/connect-to-container-that-has-sidecars/#connect-to-container-that-has-sidecars","text":"admin exec -it -n <namespace> <pod name> -c <sidecar name> /bin/sh This makes some assumptions: You will need to know the namespace of the container first, use kubectl get namespaces and then kubectl get pods -n <namespace) This assumes there is /bin/sh installed. Some containers do not have a shell.","title":"Connect to container that has sidecars"},{"location":"kubernetes/kb/deleting-not-running-pods/","text":"Deleting not running pods Why Sometimes we can have a lot of pods that fail. A good example is on a GKE cluster running on preemptible instances The AWS Alternative is Spot Instances How Default namespace Specify Namespace kubectl get pods --field-selector status.phase! = Running -o name | xargs kubectl delete kubectl get pods --field-selector status.phase! = Running -n <namespace> -o name | xargs kubectl delete -n <namespace> Note A better solution is to use something like kubectx + kubens","title":"Delete not running pods"},{"location":"kubernetes/kb/deleting-not-running-pods/#deleting-not-running-pods","text":"","title":"Deleting not running pods"},{"location":"kubernetes/kb/deleting-not-running-pods/#why","text":"Sometimes we can have a lot of pods that fail. A good example is on a GKE cluster running on preemptible instances The AWS Alternative is Spot Instances","title":"Why"},{"location":"kubernetes/kb/deleting-not-running-pods/#how","text":"Default namespace Specify Namespace kubectl get pods --field-selector status.phase! = Running -o name | xargs kubectl delete kubectl get pods --field-selector status.phase! = Running -n <namespace> -o name | xargs kubectl delete -n <namespace> Note A better solution is to use something like kubectx + kubens","title":"How"},{"location":"kubernetes/kb/downward-api/","text":"Using the downward API in Kubernetes Kubernetes has something called the Downward API that allows pods to get information of their own running state and then expose it as env variables (As the below example) What is exposed See Official Documentation for more fields Name Description metadata.name the pod's name metadata.namespace the pod's namespace metadata.uid the pod's unique ID metadata.annotations['<KEY>'] the value of the pod's annotation named (for example, metadata.annotations['myannotation'] ) metadata.labels['<KEY>'] the text value of the pod's label named (for example, metadata.labels['mylabel'] ) spec.serviceAccountName the name of the pod's service account spec.nodeName the name of the node where the Pod is executing status.hostIP the primary IP address of the node to which the Pod is assigned status.podIP the pod's primary IP address (usually, its IPv4 address) Example Manifest apiVersion : v1 kind : Pod metadata : name : my-pod spec : containers : - image : nginx:latest name : downward-test command : - my-app --pod_id=$(POD_ID) --pod_name=$(POD_NAME) --namespace_name=$(NAMESPACE_NAME) env : - name : POD_ID valueFrom : fieldRef : fieldPath : metadata.uid - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : NAMESPACE_NAME valueFrom : fieldRef : fieldPath : metadata.namespace","title":"Downward API"},{"location":"kubernetes/kb/downward-api/#using-the-downward-api-in-kubernetes","text":"Kubernetes has something called the Downward API that allows pods to get information of their own running state and then expose it as env variables (As the below example)","title":"Using the downward API in Kubernetes"},{"location":"kubernetes/kb/downward-api/#what-is-exposed","text":"See Official Documentation for more fields Name Description metadata.name the pod's name metadata.namespace the pod's namespace metadata.uid the pod's unique ID metadata.annotations['<KEY>'] the value of the pod's annotation named (for example, metadata.annotations['myannotation'] ) metadata.labels['<KEY>'] the text value of the pod's label named (for example, metadata.labels['mylabel'] ) spec.serviceAccountName the name of the pod's service account spec.nodeName the name of the node where the Pod is executing status.hostIP the primary IP address of the node to which the Pod is assigned status.podIP the pod's primary IP address (usually, its IPv4 address)","title":"What is exposed"},{"location":"kubernetes/kb/downward-api/#example-manifest","text":"apiVersion : v1 kind : Pod metadata : name : my-pod spec : containers : - image : nginx:latest name : downward-test command : - my-app --pod_id=$(POD_ID) --pod_name=$(POD_NAME) --namespace_name=$(NAMESPACE_NAME) env : - name : POD_ID valueFrom : fieldRef : fieldPath : metadata.uid - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : NAMESPACE_NAME valueFrom : fieldRef : fieldPath : metadata.namespace","title":"Example Manifest"},{"location":"kubernetes/kb/kubectl-set-namespace/","text":"Kubectl set namespace Note A better solution is to use something like kubectx + kubens Why When you're running several commands in the same namespace (That isnt default) it get's annoying having to always specify the name space Solution admin config set-context --current --namespace = <insert-namespace-name-here>","title":"Set default namespace kubectl"},{"location":"kubernetes/kb/kubectl-set-namespace/#kubectl-set-namespace","text":"Note A better solution is to use something like kubectx + kubens","title":"Kubectl set namespace"},{"location":"kubernetes/kb/kubectl-set-namespace/#why","text":"When you're running several commands in the same namespace (That isnt default) it get's annoying having to always specify the name space","title":"Why"},{"location":"kubernetes/kb/kubectl-set-namespace/#solution","text":"admin config set-context --current --namespace = <insert-namespace-name-here>","title":"Solution"},{"location":"kubernetes/kb/rbac-testing/","text":"Testing RBAC changes in Kubernetes Why Sometimes we need to give a user access to a resource in Kubernetes, and don't want to have to pester them about changes we make Command Reference admin auth can-i Help Check whether an action is allowed. VERB is a logical Kubernetes API verb like 'get' , 'list' , 'watch' , 'delete' , etc. TYPE is a Kubernetes resource. Shortcuts and groups will be resolved. NONRESOURCEURL is a partial URL that starts with \"/\" . NAME is the name of a particular Kubernetes resource. This command pairs nicely with impersonation. See --as global flag. Examples: # Check to see if I can create pods in any namespace kubectl auth can-i create pods --all-namespaces # Check to see if I can list deployments in my current namespace kubectl auth can-i list deployments.apps # Check to see if I can do everything in my current namespace (\"*\" means all) kubectl auth can-i '*' '*' # Check to see if I can get the job named \"bar\" in namespace \"foo\" kubectl auth can-i list jobs.batch/bar -n foo # Check to see if I can read pod logs kubectl auth can-i get pods --subresource = log # Check to see if I can access the URL /logs/ kubectl auth can-i get /logs/ # List all allowed actions in namespace \"foo\" kubectl auth can-i --list --namespace = foo Options: -A, --all-namespaces = false: If true, check the specified action in all namespaces. --list = false: If true, prints all allowed actions. --no-headers = false: If true, prints allowed actions without headers -q, --quiet = false: If true, suppress output and just return the exit code. --subresource = '' : SubResource such as pod/log or deployment/scale Usage: kubectl auth can-i VERB [ TYPE | TYPE/NAME | NONRESOURCEURL ] [ options ] Use \"kubectl options\" for a list of global command-line options ( applies to all commands ) . Current user Create deployment Note This assumes namespace is default admin auth can-i create deployment Create deployment in different namespace admin auth can-i create deployment --namespace bradley Other user Check if current user can create deployment admin auth can-i create deployment --namespace <namespace> -as <username> # (1)! <username> needs to appear as it does in the RoleBinding Get list of actions other user can do admin auth can-i --list --as <username> # (1)! <username> needs to appear as it does in the RoleBinding","title":"Testing RBAC"},{"location":"kubernetes/kb/rbac-testing/#testing-rbac-changes-in-kubernetes","text":"","title":"Testing RBAC changes in Kubernetes"},{"location":"kubernetes/kb/rbac-testing/#why","text":"Sometimes we need to give a user access to a resource in Kubernetes, and don't want to have to pester them about changes we make","title":"Why"},{"location":"kubernetes/kb/rbac-testing/#command-reference","text":"admin auth can-i Help Check whether an action is allowed. VERB is a logical Kubernetes API verb like 'get' , 'list' , 'watch' , 'delete' , etc. TYPE is a Kubernetes resource. Shortcuts and groups will be resolved. NONRESOURCEURL is a partial URL that starts with \"/\" . NAME is the name of a particular Kubernetes resource. This command pairs nicely with impersonation. See --as global flag. Examples: # Check to see if I can create pods in any namespace kubectl auth can-i create pods --all-namespaces # Check to see if I can list deployments in my current namespace kubectl auth can-i list deployments.apps # Check to see if I can do everything in my current namespace (\"*\" means all) kubectl auth can-i '*' '*' # Check to see if I can get the job named \"bar\" in namespace \"foo\" kubectl auth can-i list jobs.batch/bar -n foo # Check to see if I can read pod logs kubectl auth can-i get pods --subresource = log # Check to see if I can access the URL /logs/ kubectl auth can-i get /logs/ # List all allowed actions in namespace \"foo\" kubectl auth can-i --list --namespace = foo Options: -A, --all-namespaces = false: If true, check the specified action in all namespaces. --list = false: If true, prints all allowed actions. --no-headers = false: If true, prints allowed actions without headers -q, --quiet = false: If true, suppress output and just return the exit code. --subresource = '' : SubResource such as pod/log or deployment/scale Usage: kubectl auth can-i VERB [ TYPE | TYPE/NAME | NONRESOURCEURL ] [ options ] Use \"kubectl options\" for a list of global command-line options ( applies to all commands ) .","title":"Command Reference"},{"location":"kubernetes/kb/rbac-testing/#current-user","text":"","title":"Current user"},{"location":"kubernetes/kb/rbac-testing/#create-deployment","text":"Note This assumes namespace is default admin auth can-i create deployment","title":"Create deployment"},{"location":"kubernetes/kb/rbac-testing/#create-deployment-in-different-namespace","text":"admin auth can-i create deployment --namespace bradley","title":"Create deployment in different namespace"},{"location":"kubernetes/kb/rbac-testing/#other-user","text":"","title":"Other user"},{"location":"kubernetes/kb/rbac-testing/#check-if-current-user-can-create-deployment","text":"admin auth can-i create deployment --namespace <namespace> -as <username> # (1)! <username> needs to appear as it does in the RoleBinding","title":"Check if current user can create deployment"},{"location":"kubernetes/kb/rbac-testing/#get-list-of-actions-other-user-can-do","text":"admin auth can-i --list --as <username> # (1)! <username> needs to appear as it does in the RoleBinding","title":"Get list of actions other user can do"},{"location":"kubernetes/kb/sleeper/","text":"Sleeper pod Why Sometimes we need to run a pod on the cluster to exec in to and test things, like web requests etc How Default Service account Environments from config map apiVersion : v1 kind : Pod metadata : name : sleeper spec : containers : - name : alpine image : alpine:latest # Just spin & wait forever command : [ \"/bin/sh\" , \"-c\" , \"--\" ] args : [ \"while true; do sleep 30; done;\" ] apiVersion : v1 kind : Pod metadata : name : sleeper spec : serviceAccountName : <> containers : - name : alpine image : alpine:latest # Just spin & wait forever command : [ \"/bin/sh\" , \"-c\" , \"--\" ] args : [ \"while true; do sleep 30; done;\" ] apiVersion : v1 kind : Pod metadata : name : sleeper spec : containers : - name : alpine image : alpine:latest # Just spin & wait forever command : [ \"/bin/sh\" , \"-c\" , \"--\" ] args : [ \"while true; do sleep 30; done;\" ] envFrom : - configMapRef : name : <config map name> Exec in to the pod admin exec -it pod/sleeper /bin/sh","title":"Sleeper pod"},{"location":"kubernetes/kb/sleeper/#sleeper-pod","text":"","title":"Sleeper pod"},{"location":"kubernetes/kb/sleeper/#why","text":"Sometimes we need to run a pod on the cluster to exec in to and test things, like web requests etc","title":"Why"},{"location":"kubernetes/kb/sleeper/#how","text":"Default Service account Environments from config map apiVersion : v1 kind : Pod metadata : name : sleeper spec : containers : - name : alpine image : alpine:latest # Just spin & wait forever command : [ \"/bin/sh\" , \"-c\" , \"--\" ] args : [ \"while true; do sleep 30; done;\" ] apiVersion : v1 kind : Pod metadata : name : sleeper spec : serviceAccountName : <> containers : - name : alpine image : alpine:latest # Just spin & wait forever command : [ \"/bin/sh\" , \"-c\" , \"--\" ] args : [ \"while true; do sleep 30; done;\" ] apiVersion : v1 kind : Pod metadata : name : sleeper spec : containers : - name : alpine image : alpine:latest # Just spin & wait forever command : [ \"/bin/sh\" , \"-c\" , \"--\" ] args : [ \"while true; do sleep 30; done;\" ] envFrom : - configMapRef : name : <config map name>","title":"How"},{"location":"kubernetes/kb/sleeper/#exec-in-to-the-pod","text":"admin exec -it pod/sleeper /bin/sh","title":"Exec in to the pod"}]}